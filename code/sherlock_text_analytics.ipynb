{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "signature": "sha256:a8c3f3e0e4fe4cb0840f3af7f2f2944eac3078b993c73a7b48b9d6b2111d3294"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Elementary, My Dear Watson!\n",
      "# Text Analytics Tutorial using Sherlock Holmes Stories"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, I will demonstrate several text analytics techniques that can be used to analyze various text corpora in order to extract various interesting insights from the text. \n",
      "Namely, throughout this notebook, I will use [The Sherlock Holmes Books Collection](https://sherlock-holm.es/) to show how to (a) calculate various textual statistics; (b) create the social network among entities that appear in the books; (c) use a topic model to discover abstract topics in the text; and (d) utilize Word2Vec to find connections among various section of the text, and do some predictions.\n",
      "\n",
      "To perform the text analytics presented in this notebook, we will use [NLTK](http://www.nltk.org/) Python package, [GraphLab Create's](https://dato.com/products/create/) [SFrame](https://dato.com/products/create/docs/generated/graphlab.SFrame.html) and [SGraph](https://dato.com/products/create/docs/generated/graphlab.SGraph.html) objects, as well as GraphLab's [text analytics toolkit](https://dato.com/products/create/docs/graphlab.toolkits.text_analytics.html), and\n",
      "[Word2Vec](https://code.google.com/p/word2vec/) deep learning inspired model implemented in the [Gensim](http://radimrehurek.com/gensim/models/word2vec.html) Python package.\n",
      "\n",
      "\n",
      "The notebook is divided to the following sections:\n",
      "- <a href=\"#setup\">Setup</a>\n",
      "- <a href=\"#prepare\">Preparing and validating the dataset</a>\n",
      "- <a href=\"#stat\">Calculating Various Statistics</a>\n",
      "- <a href=\"#sn\">Constructing Social Networks</a>\n",
      "- <a href=\"#topic\">Topic Models</a>\n",
      "\n",
      "Each section can be executed independently. So feel free to skip ahead, just remember  to import all the required packages, and define all the needed functions.\n",
      "\n",
      "Required Python Packages:\n",
      "- [GraphLab Create](https://dato.com/products/create/quick-start-guide.html) - for classification, data.\n",
      "- [NLTK](http://www.nltk.org/) (including downloading [punkt](http://www.nltk.org/data.html)).\n",
      "- [Stanford Named Entity Recognizer](http://nlp.stanford.edu/software/CRF-NER.shtml) - for named entity recognition. \n",
      "- [Gensim](https://radimrehurek.com/gensim/) - for Word2Vec deep learning.\n",
      " engineering, and evaluation.\n",
      "- [pyLDAvis](https://github.com/bmabey/pyLDAvis) - for topic model visualziation. \n",
      "\n",
      "Let's do some text analytics!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <a id=\"setup\"></a>0. Setup"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Before we begin, make sure you have installed all the required Python packages. (The instructions below use pip. You can use easy_install, too.) Also, consider using [virtualenv](https://virtualenv.pypa.io/en/latest/) for a cleaner installation experience instead of sudo. I also recommend to run the code via [IPython Notebook](http://ipython.org/notebook.html)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "$ sudo pip install --upgrade gensim\n",
      "$ sudo pip install --upgrade nltk\n",
      "$ sudo pip install --upgrade graphlab-create\n",
      "$ sudo pip install --upgrade pyldavis\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You will need [a product key for GraphLab Create](https://dato.com/products/create/quick-start-guide.html), and to make [Stanford Named Entity Recognizer work with Pyhton NLTK](http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages).\n",
      "\n",
      "After installing NLTK and from an interactive shell download the punkt model by importing nltk and running nltk.download(). From the resulting interactive window navigate to the model tab and select punkt and download to your system. \n",
      "\n",
      "To prepare the Stanford Named Entity Recognizer to work in your system make sure you read the following links : \n",
      "[1 - How to] (http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages)\n",
      "[2 - API] (http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford)\n",
      "[3 - Stanford Parser FAQ] (http://nlp.stanford.edu/software/parser-faq.shtml)\n",
      "[4 - download] (http://nlp.stanford.edu/software/CRF-NER.html)\n",
      "5 - In the extracted directory it seems to help to run the stanford-ner.jar file in some computer configurations. \n",
      "\n",
      "Take note of what directory you downloaded and extracted the files to as you will need the location to pass the classifier and Stanford NER as arguments later in this notebook.\n",
      "\n",
      "Also in case you haven\u2019t already, make sure you are running the latest [Java JDK] (http://www.oracle.com/technetwork/java/javase/downloads/index.html)\n",
      "\n",
      "For a quick test you can open an ipython shell and try the following:\n",
      "\n",
      "from nltk.tag import StanfordNERTagger\n",
      "st = StanfordNERTagger(\u2018[path_to_your_downloaded_package_classifiers_directory]/english.all.3class.distsim.crf.ser.gz', '[path_to_your_downloaded_package_root_directory]/stanford-ner.jar')\n",
      " st.tag('When we turned him over, the Boots recognized him at once as being the same gentleman who had engaged the room under the name of Joseph Stangerson.'.split())\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <a id=\"prepare\"></a>1. Preparing the Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 1.1 Constructing the Dataset"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "<i>\"Data! Data! Data!\" he cried impatiently. \"I can't make bricks without clay.\"</i>\n",
      "                                                -The Adventure of the Copper Beeches\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Throughout this notebook, we will be analyzing Sherlock Holmes's stories collection. \n",
      "So first, we will download the stories in ASCII format from the [sherlock-holm.es website](https://sherlock-holm.es/). \n",
      "sherlock-holm.es website contains over sixty downloadable stories, we will use the following code to download the stories and insert them into a SFrame object.\n",
      "\n",
      "<b> <u>Important Note:</u></b> in some countries, such as the U.S., few of Sherlock Holmes's books & stories are still under copyright restrictions. For more information, please advise the following [website](http://www.sherlockian.net/acd/copyright.html), and read the guidelines that appear in the end of [sherlock-holm.es ASCII](https://sherlock-holm.es/ascii/) download page. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import urllib2\n",
      "import graphlab as gl\n",
      "\n",
      "BASE_DIR = \"/Users/pablo/fromHomeMac/sherlock/data\" # NOTE: Update BASE_DIR to your own directory path\n",
      "\n",
      "books_url = \"http://sherlock-holm.es/ascii/\"\n",
      "re_books_links = re.compile(\"\\\"piwik_download\\\"\\s+href=\\\"(?P<link>.*?)\\\">(?P<title>.*?)</a>\", re.MULTILINE)\n",
      "html = urllib2.urlopen(books_url).read()\n",
      "books_list = [m.groupdict() for m in re_books_links.finditer(html)]\n",
      "print books_list"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[{'link': '/stories/plain-text/cano.txt', 'title': 'The Complete Canon'}, {'link': '/stories/plain-text/cnus.txt', 'title': 'The Canon \\xe2\\x80\\x94 U.S. edition'}, {'link': '/stories/plain-text/advs.txt', 'title': 'The Adventures of Sherlock Holmes'}, {'link': '/stories/plain-text/mems.txt', 'title': 'The Memoirs of Sherlock Holmes'}, {'link': '/stories/plain-text/retn.txt', 'title': 'The Return of Sherlock Holmes'}, {'link': '/stories/plain-text/lstb.txt', 'title': 'His Last Bow'}, {'link': '/stories/plain-text/case.txt', 'title': 'The Case-Book of Sherlock Holmes'}, {'link': '/stories/plain-text/stud.txt', 'title': 'A Study In Scarlet'}, {'link': '/stories/plain-text/sign.txt', 'title': 'The Sign of the Four'}, {'link': '/stories/plain-text/houn.txt', 'title': 'The Hound of the Baskervilles'}, {'link': '/stories/plain-text/vall.txt', 'title': 'The Valley of Fear'}, {'link': '/stories/plain-text/scan.txt', 'title': 'A Scandal in Bohemia'}, {'link': '/stories/plain-text/redh.txt', 'title': 'The Red-Headed League'}, {'link': '/stories/plain-text/iden.txt', 'title': 'A Case of Identity'}, {'link': '/stories/plain-text/bosc.txt', 'title': 'The Boscombe Valley Mystery'}, {'link': '/stories/plain-text/five.txt', 'title': 'The Five Orange Pips'}, {'link': '/stories/plain-text/twis.txt', 'title': 'The Man with the Twisted Lip'}, {'link': '/stories/plain-text/blue.txt', 'title': 'The Adventure of the Blue Carbuncle'}, {'link': '/stories/plain-text/spec.txt', 'title': 'The Adventure of the Speckled Band'}, {'link': '/stories/plain-text/engr.txt', 'title': \"The Adventure of the Engineer's Thumb\"}, {'link': '/stories/plain-text/nobl.txt', 'title': 'The Adventure of the Noble Bachelor'}, {'link': '/stories/plain-text/bery.txt', 'title': 'The Adventure of the Beryl Coronet'}, {'link': '/stories/plain-text/copp.txt', 'title': 'The Adventure of the Copper Beeches'}, {'link': '/stories/plain-text/silv.txt', 'title': 'Silver Blaze'}, {'link': '/stories/plain-text/yell.txt', 'title': 'Yellow Face'}, {'link': '/stories/plain-text/stoc.txt', 'title': \"The Stockbroker's Clerk\"}, {'link': '/stories/plain-text/glor.txt', 'title': 'The \\xe2\\x80\\x9cGloria Scott\\xe2\\x80\\x9d'}, {'link': '/stories/plain-text/musg.txt', 'title': 'The Musgrave Ritual'}, {'link': '/stories/plain-text/reig.txt', 'title': 'The Reigate Puzzle'}, {'link': '/stories/plain-text/croo.txt', 'title': 'The Crooked Man'}, {'link': '/stories/plain-text/resi.txt', 'title': 'The Resident Patient'}, {'link': '/stories/plain-text/gree.txt', 'title': 'The Greek Interpreter'}, {'link': '/stories/plain-text/nava.txt', 'title': 'The Naval Treaty'}, {'link': '/stories/plain-text/fina.txt', 'title': 'The Final Problem'}, {'link': '/stories/plain-text/empt.txt', 'title': 'The Empty House'}, {'link': '/stories/plain-text/norw.txt', 'title': 'The Norwood Builder'}, {'link': '/stories/plain-text/danc.txt', 'title': 'The Dancing Men'}, {'link': '/stories/plain-text/soli.txt', 'title': 'The Solitary Cyclist'}, {'link': '/stories/plain-text/prio.txt', 'title': 'The Priory School'}, {'link': '/stories/plain-text/blac.txt', 'title': 'Black Peter'}, {'link': '/stories/plain-text/chas.txt', 'title': 'Charles Augustus Milverton'}, {'link': '/stories/plain-text/sixn.txt', 'title': 'The Six Napoleons'}, {'link': '/stories/plain-text/3stu.txt', 'title': 'The Three Students'}, {'link': '/stories/plain-text/gold.txt', 'title': 'The Golden Pince-Nez'}, {'link': '/stories/plain-text/miss.txt', 'title': 'The Missing Three-Quarter'}, {'link': '/stories/plain-text/abbe.txt', 'title': 'The Abbey Grange'}, {'link': '/stories/plain-text/seco.txt', 'title': 'The Second Stain'}, {'link': '/stories/plain-text/wist.txt', 'title': 'Wisteria Lodge'}, {'link': '/stories/plain-text/card.txt', 'title': 'The Cardboard Box'}, {'link': '/stories/plain-text/redc.txt', 'title': 'The Red Circle'}, {'link': '/stories/plain-text/bruc.txt', 'title': 'The Bruce-Partington Plans'}, {'link': '/stories/plain-text/dyin.txt', 'title': 'The Dying Detective'}, {'link': '/stories/plain-text/lady.txt', 'title': 'Lady Frances Carfax'}, {'link': '/stories/plain-text/devi.txt', 'title': \"The Devil's Foot\"}, {'link': '/stories/plain-text/last.txt', 'title': 'His Last Bow'}, {'link': '/stories/plain-text/illu.txt', 'title': 'The Illustrious Client'}, {'link': '/stories/plain-text/blan.txt', 'title': 'The Blanched Soldier'}, {'link': '/stories/plain-text/maza.txt', 'title': 'The Mazarin Stone'}, {'link': '/stories/plain-text/3gab.txt', 'title': 'The Three Gables'}, {'link': '/stories/plain-text/suss.txt', 'title': 'The Sussex Vampire'}, {'link': '/stories/plain-text/3gar.txt', 'title': 'The Three Garridebs'}, {'link': '/stories/plain-text/thor.txt', 'title': 'Thor Bridge'}, {'link': '/stories/plain-text/cree.txt', 'title': 'The Creeping Man'}, {'link': '/stories/plain-text/lion.txt', 'title': \"The Lion's Mane\"}, {'link': '/stories/plain-text/veil.txt', 'title': 'The Veiled Lodger'}, {'link': '/stories/plain-text/shos.txt', 'title': 'Shoscombe Old Place'}, {'link': '/stories/plain-text/reti.txt', 'title': 'The Retired Colourman'}]\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We got the books' titles and links, now let's download the books' texts. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Filter books due to copyright issues. In this code, we filtered \"The Complete Canon\", \u201cCase-Book of Sherlock Holmes\u201d books, and\n",
      "# \"The Canon \u2014 U.S. edition\" book (For more information please read the note above).\n",
      "filtered_books = set([\"The Complete Canon\", \"The Case-Book of Sherlock Holmes\", \"The Canon \u2014 U.S. edition\" ])\n",
      "books_list = filter(lambda d: d['title'] not in filtered_books, books_list )\n",
      "\n",
      "#Download books' texts (to not overload the website we download the text in batch and not in parallel)\n",
      "for d in books_list:\n",
      "    d['text'] = urllib2.urlopen(\"http://sherlock-holm.es\" + d['link']).read().strip()"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's load the dict list into a SFrame object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf = gl.SFrame(books_list).unpack(\"X1\", column_name_prefix=\"\")\n",
      "sf.save(\"%s/books.sframe\" % BASE_DIR)\n",
      "sf.head(3)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[INFO] This non-commercial license of GraphLab Create is assigned to pablo@dato.com and will expire on January 02, 2020. For commercial licensing options, visit https://dato.com/buy/.\n",
        "\n",
        "[INFO] Start server at: ipc:///tmp/graphlab_server-1136 - Server binary: /Users/pablo/anaconda/envs/dato-env/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1453918014.log\n",
        "[INFO] GraphLab Server Version: 1.8\n"
       ]
      },
      {
       "data": {
        "text/html": [
         "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
         "    <tr>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">link</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">text</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">title</th>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">/stories/plain-<br>text/advs.txt ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">THE ADVENTURES OF<br>SHERLOCK HOLMES\\n\\n ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Adventures of<br>Sherlock Holmes ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">/stories/plain-<br>text/mems.txt ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">THE MEMOIRS OF SHERLOCK<br>HOLMES\\n\\n ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Memoirs of Sherlock<br>Holmes ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">/stories/plain-<br>text/retn.txt ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">THE RETURN OF SHERLOCK<br>HOLMES\\n\\n ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Return of Sherlock<br>Holmes ...</td>\n",
         "    </tr>\n",
         "</table>\n",
         "[3 rows x 3 columns]<br/>\n",
         "</div>"
        ],
        "text/plain": [
         "Columns:\n",
         "\tlink\tstr\n",
         "\ttext\tstr\n",
         "\ttitle\tstr\n",
         "\n",
         "Rows: 3\n",
         "\n",
         "Data:\n",
         "+------------------------------+-------------------------------+\n",
         "|             link             |              text             |\n",
         "+------------------------------+-------------------------------+\n",
         "| /stories/plain-text/advs.txt | THE ADVENTURES OF SHERLOCK... |\n",
         "| /stories/plain-text/mems.txt | THE MEMOIRS OF SHERLOCK HO... |\n",
         "| /stories/plain-text/retn.txt | THE RETURN OF SHERLOCK HOL... |\n",
         "+------------------------------+-------------------------------+\n",
         "+--------------------------------+\n",
         "|             title              |\n",
         "+--------------------------------+\n",
         "| The Adventures of Sherlock...  |\n",
         "| The Memoirs of Sherlock Holmes |\n",
         "| The Return of Sherlock Holmes  |\n",
         "+--------------------------------+\n",
         "[3 rows x 3 columns]"
        ]
       },
       "execution_count": 4,
       "metadata": {},
       "output_type": "execute_result"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <a id=\"stat\"></a>2. Calculating Various Statistics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section, I will demonstrate  how it is very straight-forward to utilize GraphLab Create SFrame object to calculate & visualize various statistics.\n",
      "\n",
      "In previous section, we created a SFrame object which consists of 64 texts. Let us first load the SFrame object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import graphlab as gl\n",
      "import re\n",
      "\n",
      "BASE_DIR = \"/Users/pablo/fromHomeMac/sherlock/data\" # NOTE: Update BASE_DIR to your own directory path\n",
      "gl.canvas.set_target('ipynb')\n",
      "sf = gl.load_sframe(\"%s\\\\books.sframe\" % BASE_DIR)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using Python, it is very easy to calculate the number of characters  in a text, we just need to use the built-in [len function](https://docs.python.org/2/library/functions.html#len). Let's calculate the number of characters  in each downloaded text using the the <i>len</i> function and SArray's [<i>apply</i> function](https://dato.com/products/create/docs/generated/graphlab.SArray.apply.html#graphlab.SArray.apply) (notice that each column in a [SFrame object](https://dato.com/products/create/docs/generated/graphlab.SFrame.html) is a [SArray object](https://dato.com/products/create/docs/generated/graphlab.SArray.html?highlight=sarray))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf['chars_num'] = sf['text'].apply(lambda t: len(t))\n",
      "sf.head(3)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "text/html": [
         "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
         "    <tr>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">link</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">text</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">title</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">chars_num</th>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">/stories/plain-<br>text/advs.txt ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">THE ADVENTURES OF<br>SHERLOCK HOLMES\\n\\n ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Adventures of<br>Sherlock Holmes ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">610886</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">/stories/plain-<br>text/mems.txt ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">THE MEMOIRS OF SHERLOCK<br>HOLMES\\n\\n ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Memoirs of Sherlock<br>Holmes ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">511747</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">/stories/plain-<br>text/retn.txt ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">THE RETURN OF SHERLOCK<br>HOLMES\\n\\n ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Return of Sherlock<br>Holmes ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">662242</td>\n",
         "    </tr>\n",
         "</table>\n",
         "[3 rows x 4 columns]<br/>\n",
         "</div>"
        ],
        "text/plain": [
         "Columns:\n",
         "\tlink\tstr\n",
         "\ttext\tstr\n",
         "\ttitle\tstr\n",
         "\tchars_num\tint\n",
         "\n",
         "Rows: 3\n",
         "\n",
         "Data:\n",
         "+------------------------------+-------------------------------+\n",
         "|             link             |              text             |\n",
         "+------------------------------+-------------------------------+\n",
         "| /stories/plain-text/advs.txt | THE ADVENTURES OF SHERLOCK... |\n",
         "| /stories/plain-text/mems.txt | THE MEMOIRS OF SHERLOCK HO... |\n",
         "| /stories/plain-text/retn.txt | THE RETURN OF SHERLOCK HOL... |\n",
         "+------------------------------+-------------------------------+\n",
         "+--------------------------------+-----------+\n",
         "|             title              | chars_num |\n",
         "+--------------------------------+-----------+\n",
         "| The Adventures of Sherlock...  |   610886  |\n",
         "| The Memoirs of Sherlock Holmes |   511747  |\n",
         "| The Return of Sherlock Holmes  |   662242  |\n",
         "+--------------------------------+-----------+\n",
         "[3 rows x 4 columns]"
        ]
       },
       "execution_count": 6,
       "metadata": {},
       "output_type": "execute_result"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's use the [show function](https://dato.com/products/create/docs/generated/graphlab.SArray.show.html) to visualize  the distribution  of text length in each one of our downloaded text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf['chars_num'].show()"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sarray','Numeric', {\"ipython\": true, \"sketch\": {\"std\": 134594.53590457857, \"complete\": true, \"min\": 26234.0, \"max\": 662242.0, \"quantile\": [26234.0, 26234.0, 33400.0, 33400.0, 34513.0, 34709.0, 34709.0, 35907.0, 35973.0, 35973.0, 36435.0, 36798.0, 36798.0, 36830.0, 36830.0, 38779.0, 40212.0, 40212.0, 40305.0, 40445.0, 40445.0, 41651.0, 42184.0, 42184.0, 42457.0, 42624.0, 42624.0, 42936.0, 42936.0, 42947.0, 43391.0, 43391.0, 43688.0, 43821.0, 43821.0, 44698.0, 45342.0, 45342.0, 45461.0, 45461.0, 46241.0, 46317.0, 46317.0, 46409.0, 46441.0, 46441.0, 48044.0, 48477.0, 48477.0, 48521.0, 48931.0, 48931.0, 49254.0, 49254.0, 50194.0, 51396.0, 51396.0, 51745.0, 53030.0, 53030.0, 53342.0, 53938.0, 53938.0, 54142.0, 54142.0, 55229.0, 55886.0, 55886.0, 56223.0, 56690.0, 56690.0, 57280.0, 57470.0, 57470.0, 57536.0, 58039.0, 58039.0, 58042.0, 58042.0, 58358.0, 60289.0, 60289.0, 64945.0, 68260.0, 68260.0, 68476.0, 74759.0, 74759.0, 251814.0, 251814.0, 260051.0, 340223.0, 340223.0, 346404.0, 402296.0, 402296.0, 511747.0, 610886.0, 610886.0, 662242.0, 662242.0], \"median\": 48931.0, \"numeric\": true, \"num_unique\": 64, \"num_undefined\": 0, \"var\": 18115689095.368893, \"progress\": 1.0, \"size\": 64, \"frequent_items\": {\"42624\": {\"frequency\": 1, \"value\": 42624}, \"60289\": {\"frequency\": 1, \"value\": 60289}, \"511747\": {\"frequency\": 1, \"value\": 511747}, \"57280\": {\"frequency\": 1, \"value\": 57280}, \"35973\": {\"frequency\": 1, \"value\": 35973}, \"74759\": {\"frequency\": 1, \"value\": 74759}, \"48521\": {\"frequency\": 1, \"value\": 48521}, \"34709\": {\"frequency\": 1, \"value\": 34709}, \"50194\": {\"frequency\": 1, \"value\": 50194}, \"42947\": {\"frequency\": 1, \"value\": 42947}, \"40212\": {\"frequency\": 1, \"value\": 40212}, \"45461\": {\"frequency\": 1, \"value\": 45461}, \"44698\": {\"frequency\": 1, \"value\": 44698}, \"45342\": {\"frequency\": 1, \"value\": 45342}, \"56223\": {\"frequency\": 1, \"value\": 56223}, \"51745\": {\"frequency\": 1, \"value\": 51745}, \"48931\": {\"frequency\": 1, \"value\": 48931}, \"68260\": {\"frequency\": 1, \"value\": 68260}, \"251814\": {\"frequency\": 1, \"value\": 251814}, \"43688\": {\"frequency\": 1, \"value\": 43688}, \"33400\": {\"frequency\": 1, \"value\": 33400}, \"48044\": {\"frequency\": 1, \"value\": 48044}, \"43821\": {\"frequency\": 1, \"value\": 43821}, \"64945\": {\"frequency\": 1, \"value\": 64945}, \"53938\": {\"frequency\": 1, \"value\": 53938}, \"41651\": {\"frequency\": 1, \"value\": 41651}, \"36830\": {\"frequency\": 1, \"value\": 36830}, \"58039\": {\"frequency\": 1, \"value\": 58039}, \"42936\": {\"frequency\": 1, \"value\": 42936}, \"58042\": {\"frequency\": 1, \"value\": 58042}, \"55229\": {\"frequency\": 1, \"value\": 55229}, \"36798\": {\"frequency\": 1, \"value\": 36798}, \"57536\": {\"frequency\": 1, \"value\": 57536}, \"35907\": {\"frequency\": 1, \"value\": 35907}, \"51396\": {\"frequency\": 1, \"value\": 51396}, \"58358\": {\"frequency\": 1, \"value\": 58358}, \"610886\": {\"frequency\": 1, \"value\": 610886}, \"46241\": {\"frequency\": 1, \"value\": 46241}, \"42184\": {\"frequency\": 1, \"value\": 42184}, \"46409\": {\"frequency\": 1, \"value\": 46409}, \"55886\": {\"frequency\": 1, \"value\": 55886}, \"34513\": {\"frequency\": 1, \"value\": 34513}, \"260051\": {\"frequency\": 1, \"value\": 260051}, \"42457\": {\"frequency\": 1, \"value\": 42457}, \"48477\": {\"frequency\": 1, \"value\": 48477}, \"53342\": {\"frequency\": 1, \"value\": 53342}, \"662242\": {\"frequency\": 1, \"value\": 662242}, \"38779\": {\"frequency\": 1, \"value\": 38779}, \"53030\": {\"frequency\": 1, \"value\": 53030}, \"49254\": {\"frequency\": 1, \"value\": 49254}, \"46441\": {\"frequency\": 1, \"value\": 46441}, \"46317\": {\"frequency\": 1, \"value\": 46317}, \"40305\": {\"frequency\": 1, \"value\": 40305}, \"56690\": {\"frequency\": 1, \"value\": 56690}, \"36435\": {\"frequency\": 1, \"value\": 36435}, \"57470\": {\"frequency\": 1, \"value\": 57470}, \"346404\": {\"frequency\": 1, \"value\": 346404}, \"402296\": {\"frequency\": 1, \"value\": 402296}, \"26234\": {\"frequency\": 1, \"value\": 26234}, \"340223\": {\"frequency\": 1, \"value\": 340223}, \"68476\": {\"frequency\": 1, \"value\": 68476}, \"40445\": {\"frequency\": 1, \"value\": 40445}, \"54142\": {\"frequency\": 1, \"value\": 54142}, \"43391\": {\"frequency\": 1, \"value\": 43391}}, \"mean\": 95020.42187499999}, \"selected_variable\": {\"name\": [\"<SArray>\"], \"dtype\": \"int\", \"view_component\": \"Numeric\", \"view_file\": \"sarray\", \"descriptives\": {\"rows\": 64}, \"type\": \"SArray\", \"view_components\": [\"Numeric\", \"Categorical\"]}, \"histogram\": {\"progress\": 1.0, \"histogram\": {\"max\": 667593.5079999999, \"bins\": [55, 1, 0, 0, 2, 1, 1, 1, 0, 1, 1, 1], \"min\": 20414.11600000001}, \"min\": 26234, \"complete\": 1, \"max\": 662242}}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that the mean characters  number in the download stories is 95020.42, and the maximal number of characters  in a story is 662,242 characters.\n",
      "Let's also calculate  the number of words in each text. Calculating the number of words in a text is little trickier and there are several methods to perform this task. Using one of the following methods:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "text = \"\"\"I think that you know me well enough, Watson, to understand that I am by no means a nervous man. At the same time,\n",
      "it is stupidity rather than courage to refuse to recognize danger when it is close upon you.\"\"\"\n",
      "\n",
      "#using the split function\n",
      "print text.split()"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['I', 'think', 'that', 'you', 'know', 'me', 'well', 'enough,', 'Watson,', 'to', 'understand', 'that', 'I', 'am', 'by', 'no', 'means', 'a', 'nervous', 'man.', 'At', 'the', 'same', 'time,', 'it', 'is', 'stupidity', 'rather', 'than', 'courage', 'to', 'refuse', 'to', 'recognize', 'danger', 'when', 'it', 'is', 'close', 'upon', 'you.']\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Using NLTK \n",
      "#Note: Remember to download the NLTK's punkt package by running nltk.download() from the Interactive Python Shell\n",
      "import nltk\n",
      "print nltk.word_tokenize(text)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['I', 'think', 'that', 'you', 'know', 'me', 'well', 'enough', ',', 'Watson', ',', 'to', 'understand', 'that', 'I', 'am', 'by', 'no', 'means', 'a', 'nervous', 'man', '.', 'At', 'the', 'same', 'time', ',', 'it', 'is', 'stupidity', 'rather', 'than', 'courage', 'to', 'refuse', 'to', 'recognize', 'danger', 'when', 'it', 'is', 'close', 'upon', 'you', '.']\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You can see that both using the split function, or using the regular expression work pretty-well. \n",
      "However, it is important to notice, that the first regular expression can mistakenly split words, \n",
      "such as \"S.H.\" into two words, while the split function doesn't remove punctuation. \n",
      "Therefore, if we want to be precise, we can use the NLTK's tokenize package and remove punctuation from the results.\n",
      "Nevertheless, for our case, it is good enough to use the regular expression method to count words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf['words_num'] = sf['text'].apply(lambda t: len(re_words_split.findall(t)))"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also use NLTK to count the number of sentences in each story."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "def txt2sentences(txt, remove_none_english_chars=True):\n",
      "    \"\"\"\n",
      "    Split the English text into sentences using NLTK\n",
      "    :param txt: input text.    \n",
      "    :param remove_none_english_chars: if True then remove none English chars from text\n",
      "    :return: string in which each line consists of single sentence from the original input text.\n",
      "    :rtype: str\n",
      "    \"\"\"        \n",
      "    # decode to utf8 to avoid encoding problems - if someone has better idea how to solve encoding \n",
      "    # problem I will love to learn about it.     \n",
      "    txt = txt.decode(\"utf8\") \n",
      "    # split text into sentences using NLTK package\n",
      "    for s in tokenizer.tokenize(txt):\n",
      "        if remove_none_english_chars:\n",
      "            #remove none English chars\n",
      "            s = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
      "        yield s\n",
      "\n",
      "sf['sentences_num'] = sf['text'].apply(lambda t: len(list(txt2sentences(t))))"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf[['chars_num','words_num','sentences_num']].show()"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sframe','Summary', {\"ipython\": true, \"sketch\": {\"sentences_num\": {\"std\": 1546.2633518490586, \"complete\": true, \"min\": 334.0, \"max\": 7807.0, \"quantile\": [334.0, 334.0, 401.0, 401.0, 407.0, 409.0, 409.0, 417.0, 435.0, 435.0, 438.0, 448.0, 448.0, 449.0, 449.0, 462.0, 472.0, 472.0, 472.0, 474.0, 474.0, 479.0, 499.0, 499.0, 506.0, 511.0, 511.0, 515.0, 515.0, 518.0, 523.0, 523.0, 525.0, 532.0, 532.0, 535.0, 540.0, 540.0, 541.0, 541.0, 543.0, 552.0, 552.0, 558.0, 561.0, 561.0, 564.0, 566.0, 566.0, 566.0, 572.0, 572.0, 578.0, 578.0, 592.0, 595.0, 595.0, 599.0, 611.0, 611.0, 612.0, 617.0, 617.0, 623.0, 623.0, 627.0, 629.0, 629.0, 633.0, 634.0, 634.0, 643.0, 662.0, 662.0, 668.0, 673.0, 673.0, 693.0, 693.0, 718.0, 760.0, 760.0, 798.0, 885.0, 885.0, 892.0, 923.0, 923.0, 2699.0, 2699.0, 2930.0, 3875.0, 3875.0, 4268.0, 5086.0, 5086.0, 5502.0, 6815.0, 6815.0, 7807.0, 7807.0], \"median\": 572.0, \"numeric\": true, \"num_unique\": 62, \"num_undefined\": 0, \"var\": 2390930.353271486, \"progress\": 1.0, \"size\": 64, \"frequent_items\": {\"643\": {\"frequency\": 1, \"value\": 643}, \"518\": {\"frequency\": 1, \"value\": 518}, \"2699\": {\"frequency\": 1, \"value\": 2699}, \"525\": {\"frequency\": 1, \"value\": 525}, \"401\": {\"frequency\": 1, \"value\": 401}, \"515\": {\"frequency\": 1, \"value\": 515}, \"532\": {\"frequency\": 1, \"value\": 532}, \"662\": {\"frequency\": 1, \"value\": 662}, \"407\": {\"frequency\": 1, \"value\": 407}, \"409\": {\"frequency\": 1, \"value\": 409}, \"923\": {\"frequency\": 1, \"value\": 923}, \"540\": {\"frequency\": 1, \"value\": 540}, \"541\": {\"frequency\": 1, \"value\": 541}, \"798\": {\"frequency\": 1, \"value\": 798}, \"6815\": {\"frequency\": 1, \"value\": 6815}, \"673\": {\"frequency\": 1, \"value\": 673}, \"3875\": {\"frequency\": 1, \"value\": 3875}, \"552\": {\"frequency\": 1, \"value\": 552}, \"668\": {\"frequency\": 1, \"value\": 668}, \"4268\": {\"frequency\": 1, \"value\": 4268}, \"558\": {\"frequency\": 1, \"value\": 558}, \"561\": {\"frequency\": 1, \"value\": 561}, \"435\": {\"frequency\": 1, \"value\": 435}, \"564\": {\"frequency\": 1, \"value\": 564}, \"693\": {\"frequency\": 1, \"value\": 693}, \"438\": {\"frequency\": 1, \"value\": 438}, \"462\": {\"frequency\": 1, \"value\": 462}, \"543\": {\"frequency\": 1, \"value\": 543}, \"572\": {\"frequency\": 1, \"value\": 572}, \"885\": {\"frequency\": 1, \"value\": 885}, \"448\": {\"frequency\": 1, \"value\": 448}, \"449\": {\"frequency\": 1, \"value\": 449}, \"578\": {\"frequency\": 1, \"value\": 578}, \"523\": {\"frequency\": 1, \"value\": 523}, \"566\": {\"frequency\": 2, \"value\": 566}, \"417\": {\"frequency\": 1, \"value\": 417}, \"535\": {\"frequency\": 1, \"value\": 535}, \"718\": {\"frequency\": 1, \"value\": 718}, \"592\": {\"frequency\": 1, \"value\": 592}, \"595\": {\"frequency\": 1, \"value\": 595}, \"334\": {\"frequency\": 1, \"value\": 334}, \"599\": {\"frequency\": 1, \"value\": 599}, \"472\": {\"frequency\": 2, \"value\": 472}, \"474\": {\"frequency\": 1, \"value\": 474}, \"506\": {\"frequency\": 1, \"value\": 506}, \"5086\": {\"frequency\": 1, \"value\": 5086}, \"479\": {\"frequency\": 1, \"value\": 479}, \"627\": {\"frequency\": 1, \"value\": 627}, \"611\": {\"frequency\": 1, \"value\": 611}, \"612\": {\"frequency\": 1, \"value\": 612}, \"617\": {\"frequency\": 1, \"value\": 617}, \"623\": {\"frequency\": 1, \"value\": 623}, \"2930\": {\"frequency\": 1, \"value\": 2930}, \"499\": {\"frequency\": 1, \"value\": 499}, \"629\": {\"frequency\": 1, \"value\": 629}, \"760\": {\"frequency\": 1, \"value\": 760}, \"633\": {\"frequency\": 1, \"value\": 633}, \"634\": {\"frequency\": 1, \"value\": 634}, \"511\": {\"frequency\": 1, \"value\": 511}, \"892\": {\"frequency\": 1, \"value\": 892}, \"5502\": {\"frequency\": 1, \"value\": 5502}, \"7807\": {\"frequency\": 1, \"value\": 7807}}, \"mean\": 1108.921875}, \"chars_num\": {\"std\": 134594.53590457857, \"complete\": true, \"min\": 26234.0, \"max\": 662242.0, \"quantile\": [26234.0, 26234.0, 33400.0, 33400.0, 34513.0, 34709.0, 34709.0, 35907.0, 35973.0, 35973.0, 36435.0, 36798.0, 36798.0, 36830.0, 36830.0, 38779.0, 40212.0, 40212.0, 40305.0, 40445.0, 40445.0, 41651.0, 42184.0, 42184.0, 42457.0, 42624.0, 42624.0, 42936.0, 42936.0, 42947.0, 43391.0, 43391.0, 43688.0, 43821.0, 43821.0, 44698.0, 45342.0, 45342.0, 45461.0, 45461.0, 46241.0, 46317.0, 46317.0, 46409.0, 46441.0, 46441.0, 48044.0, 48477.0, 48477.0, 48521.0, 48931.0, 48931.0, 49254.0, 49254.0, 50194.0, 51396.0, 51396.0, 51745.0, 53030.0, 53030.0, 53342.0, 53938.0, 53938.0, 54142.0, 54142.0, 55229.0, 55886.0, 55886.0, 56223.0, 56690.0, 56690.0, 57280.0, 57470.0, 57470.0, 57536.0, 58039.0, 58039.0, 58042.0, 58042.0, 58358.0, 60289.0, 60289.0, 64945.0, 68260.0, 68260.0, 68476.0, 74759.0, 74759.0, 251814.0, 251814.0, 260051.0, 340223.0, 340223.0, 346404.0, 402296.0, 402296.0, 511747.0, 610886.0, 610886.0, 662242.0, 662242.0], \"median\": 48931.0, \"numeric\": true, \"num_unique\": 64, \"num_undefined\": 0, \"var\": 18115689095.368893, \"progress\": 1.0, \"size\": 64, \"frequent_items\": {\"42624\": {\"frequency\": 1, \"value\": 42624}, \"60289\": {\"frequency\": 1, \"value\": 60289}, \"511747\": {\"frequency\": 1, \"value\": 511747}, \"57280\": {\"frequency\": 1, \"value\": 57280}, \"35973\": {\"frequency\": 1, \"value\": 35973}, \"74759\": {\"frequency\": 1, \"value\": 74759}, \"48521\": {\"frequency\": 1, \"value\": 48521}, \"34709\": {\"frequency\": 1, \"value\": 34709}, \"50194\": {\"frequency\": 1, \"value\": 50194}, \"42947\": {\"frequency\": 1, \"value\": 42947}, \"40212\": {\"frequency\": 1, \"value\": 40212}, \"45461\": {\"frequency\": 1, \"value\": 45461}, \"44698\": {\"frequency\": 1, \"value\": 44698}, \"45342\": {\"frequency\": 1, \"value\": 45342}, \"56223\": {\"frequency\": 1, \"value\": 56223}, \"51745\": {\"frequency\": 1, \"value\": 51745}, \"48931\": {\"frequency\": 1, \"value\": 48931}, \"68260\": {\"frequency\": 1, \"value\": 68260}, \"251814\": {\"frequency\": 1, \"value\": 251814}, \"43688\": {\"frequency\": 1, \"value\": 43688}, \"33400\": {\"frequency\": 1, \"value\": 33400}, \"48044\": {\"frequency\": 1, \"value\": 48044}, \"43821\": {\"frequency\": 1, \"value\": 43821}, \"64945\": {\"frequency\": 1, \"value\": 64945}, \"53938\": {\"frequency\": 1, \"value\": 53938}, \"41651\": {\"frequency\": 1, \"value\": 41651}, \"36830\": {\"frequency\": 1, \"value\": 36830}, \"58039\": {\"frequency\": 1, \"value\": 58039}, \"42936\": {\"frequency\": 1, \"value\": 42936}, \"58042\": {\"frequency\": 1, \"value\": 58042}, \"55229\": {\"frequency\": 1, \"value\": 55229}, \"36798\": {\"frequency\": 1, \"value\": 36798}, \"57536\": {\"frequency\": 1, \"value\": 57536}, \"35907\": {\"frequency\": 1, \"value\": 35907}, \"51396\": {\"frequency\": 1, \"value\": 51396}, \"58358\": {\"frequency\": 1, \"value\": 58358}, \"610886\": {\"frequency\": 1, \"value\": 610886}, \"46241\": {\"frequency\": 1, \"value\": 46241}, \"42184\": {\"frequency\": 1, \"value\": 42184}, \"46409\": {\"frequency\": 1, \"value\": 46409}, \"55886\": {\"frequency\": 1, \"value\": 55886}, \"34513\": {\"frequency\": 1, \"value\": 34513}, \"260051\": {\"frequency\": 1, \"value\": 260051}, \"42457\": {\"frequency\": 1, \"value\": 42457}, \"48477\": {\"frequency\": 1, \"value\": 48477}, \"53342\": {\"frequency\": 1, \"value\": 53342}, \"662242\": {\"frequency\": 1, \"value\": 662242}, \"38779\": {\"frequency\": 1, \"value\": 38779}, \"53030\": {\"frequency\": 1, \"value\": 53030}, \"49254\": {\"frequency\": 1, \"value\": 49254}, \"46441\": {\"frequency\": 1, \"value\": 46441}, \"46317\": {\"frequency\": 1, \"value\": 46317}, \"40305\": {\"frequency\": 1, \"value\": 40305}, \"56690\": {\"frequency\": 1, \"value\": 56690}, \"36435\": {\"frequency\": 1, \"value\": 36435}, \"57470\": {\"frequency\": 1, \"value\": 57470}, \"346404\": {\"frequency\": 1, \"value\": 346404}, \"402296\": {\"frequency\": 1, \"value\": 402296}, \"26234\": {\"frequency\": 1, \"value\": 26234}, \"340223\": {\"frequency\": 1, \"value\": 340223}, \"68476\": {\"frequency\": 1, \"value\": 68476}, \"40445\": {\"frequency\": 1, \"value\": 40445}, \"54142\": {\"frequency\": 1, \"value\": 54142}, \"43391\": {\"frequency\": 1, \"value\": 43391}}, \"mean\": 95020.42187499999}, \"words_num\": {\"std\": 23280.89778114439, \"complete\": true, \"min\": 4622.0, \"max\": 114461.0, \"quantile\": [4622.0, 4622.0, 5698.0, 5698.0, 5910.0, 5986.0, 5986.0, 6137.0, 6285.0, 6285.0, 6303.0, 6394.0, 6394.0, 6470.0, 6470.0, 6649.0, 6825.0, 6825.0, 6940.0, 6991.0, 6991.0, 7163.0, 7190.0, 7190.0, 7317.0, 7339.0, 7339.0, 7378.0, 7378.0, 7447.0, 7498.0, 7498.0, 7555.0, 7677.0, 7677.0, 7759.0, 7850.0, 7850.0, 7891.0, 7891.0, 7899.0, 8051.0, 8051.0, 8067.0, 8068.0, 8068.0, 8310.0, 8317.0, 8317.0, 8358.0, 8483.0, 8483.0, 8581.0, 8581.0, 8752.0, 8839.0, 8839.0, 8917.0, 9217.0, 9217.0, 9371.0, 9412.0, 9412.0, 9445.0, 9445.0, 9482.0, 9821.0, 9821.0, 9828.0, 9829.0, 9829.0, 9864.0, 9897.0, 9897.0, 9960.0, 10025.0, 10025.0, 10035.0, 10035.0, 10148.0, 10181.0, 10181.0, 10916.0, 11671.0, 11671.0, 11732.0, 12901.0, 12901.0, 43956.0, 43956.0, 44210.0, 59099.0, 59099.0, 60104.0, 68922.0, 68922.0, 88497.0, 106040.0, 106040.0, 114461.0, 114461.0], \"median\": 8483.0, \"numeric\": true, \"num_unique\": 64, \"num_undefined\": 0, \"var\": 542000201.4960939, \"progress\": 1.0, \"size\": 64, \"frequent_items\": {\"9217\": {\"frequency\": 1, \"value\": 9217}, \"8067\": {\"frequency\": 1, \"value\": 8067}, \"8068\": {\"frequency\": 1, \"value\": 8068}, \"8581\": {\"frequency\": 1, \"value\": 8581}, \"8839\": {\"frequency\": 1, \"value\": 8839}, \"9864\": {\"frequency\": 1, \"value\": 9864}, \"9482\": {\"frequency\": 1, \"value\": 9482}, \"11671\": {\"frequency\": 1, \"value\": 11671}, \"6285\": {\"frequency\": 1, \"value\": 6285}, \"4622\": {\"frequency\": 1, \"value\": 4622}, \"9829\": {\"frequency\": 1, \"value\": 9829}, \"7555\": {\"frequency\": 1, \"value\": 7555}, \"7317\": {\"frequency\": 1, \"value\": 7317}, \"7190\": {\"frequency\": 1, \"value\": 7190}, \"7447\": {\"frequency\": 1, \"value\": 7447}, \"9371\": {\"frequency\": 1, \"value\": 9371}, \"6940\": {\"frequency\": 1, \"value\": 6940}, \"114461\": {\"frequency\": 1, \"value\": 114461}, \"6303\": {\"frequency\": 1, \"value\": 6303}, \"8483\": {\"frequency\": 1, \"value\": 8483}, \"10148\": {\"frequency\": 1, \"value\": 10148}, \"6825\": {\"frequency\": 1, \"value\": 6825}, \"8358\": {\"frequency\": 1, \"value\": 8358}, \"9897\": {\"frequency\": 1, \"value\": 9897}, \"7850\": {\"frequency\": 1, \"value\": 7850}, \"7339\": {\"frequency\": 1, \"value\": 7339}, \"8317\": {\"frequency\": 1, \"value\": 8317}, \"8752\": {\"frequency\": 1, \"value\": 8752}, \"88497\": {\"frequency\": 1, \"value\": 88497}, \"44210\": {\"frequency\": 1, \"value\": 44210}, \"10035\": {\"frequency\": 1, \"value\": 10035}, \"43956\": {\"frequency\": 1, \"value\": 43956}, \"106040\": {\"frequency\": 1, \"value\": 106040}, \"68922\": {\"frequency\": 1, \"value\": 68922}, \"5698\": {\"frequency\": 1, \"value\": 5698}, \"9412\": {\"frequency\": 1, \"value\": 9412}, \"10181\": {\"frequency\": 1, \"value\": 10181}, \"6470\": {\"frequency\": 1, \"value\": 6470}, \"60104\": {\"frequency\": 1, \"value\": 60104}, \"7498\": {\"frequency\": 1, \"value\": 7498}, \"6991\": {\"frequency\": 1, \"value\": 6991}, \"5910\": {\"frequency\": 1, \"value\": 5910}, \"7891\": {\"frequency\": 1, \"value\": 7891}, \"11732\": {\"frequency\": 1, \"value\": 11732}, \"8917\": {\"frequency\": 1, \"value\": 8917}, \"6137\": {\"frequency\": 1, \"value\": 6137}, \"10916\": {\"frequency\": 1, \"value\": 10916}, \"7759\": {\"frequency\": 1, \"value\": 7759}, \"9821\": {\"frequency\": 1, \"value\": 9821}, \"7899\": {\"frequency\": 1, \"value\": 7899}, \"5986\": {\"frequency\": 1, \"value\": 5986}, \"9828\": {\"frequency\": 1, \"value\": 9828}, \"9445\": {\"frequency\": 1, \"value\": 9445}, \"59099\": {\"frequency\": 1, \"value\": 59099}, \"9960\": {\"frequency\": 1, \"value\": 9960}, \"7378\": {\"frequency\": 1, \"value\": 7378}, \"12901\": {\"frequency\": 1, \"value\": 12901}, \"8051\": {\"frequency\": 1, \"value\": 8051}, \"8310\": {\"frequency\": 1, \"value\": 8310}, \"10025\": {\"frequency\": 1, \"value\": 10025}, \"6649\": {\"frequency\": 1, \"value\": 6649}, \"6394\": {\"frequency\": 1, \"value\": 6394}, \"7163\": {\"frequency\": 1, \"value\": 7163}, \"7677\": {\"frequency\": 1, \"value\": 7677}}, \"mean\": 16420.9375}}, \"selected_variable\": {\"name\": [\"<SFrame>\"], \"descriptives\": {\"rows\": 64, \"columns\": 3}, \"view_component\": \"Summary\", \"view_file\": \"sframe\", \"view_params\": {\"y\": null, \"x\": null, \"columns\": [\"chars_num\", \"words_num\", \"sentences_num\"], \"view\": null}, \"view_components\": [\"Summary\", \"Table\", \"Bar Chart\", \"BoxWhisker Plot\", \"Line Chart\", \"Scatter Plot\", \"Heat Map\", \"Plots\"], \"type\": \"SFrame\", \"columns\": [{\"dtype\": \"int\", \"name\": \"chars_num\"}, {\"dtype\": \"int\", \"name\": \"words_num\"}, {\"dtype\": \"int\", \"name\": \"sentences_num\"}], \"column_identifiers\": [\"sentences_num\", \"chars_num\", \"words_num\"]}, \"columns\": [{\"dtype\": \"int\", \"name\": \"chars_num\"}, {\"dtype\": \"int\", \"name\": \"words_num\"}, {\"dtype\": \"int\", \"name\": \"sentences_num\"}]}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Until now I calculated very basic text statistics. Let's try to do something more complicated like count the number of time the words 'Sherlock',\n",
      "'Watson', and 'Elementary' appeared in each story. We will do it using GraphLab's [text_analytics.count_words](https://dato.com/products/create/docs/generated/graphlab.text_analytics.count_words.html#graphlab.text_analytics.count_words) toolkit.\n",
      "\n",
      "<u>Note</u>: To count the frequency a word appears in a text, one can also consider using the [collection.Counter](https://docs.python.org/2/library/collections.html) function."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf['words_count'] = gl.text_analytics.count_words(sf['text'], to_lower=True)\n",
      "sf['sherlock_count'] = sf['words_count'].apply(lambda d: d.get('sherlock',0))\n",
      "sf['watson_count'] = sf['words_count'].apply(lambda d: d.get('watson',0))\n",
      "sf['elementary_count'] = sf['words_count'].apply(lambda d: d.get('elementary',0))\n",
      "sf[['sherlock_count', 'watson_count', 'elementary_count']].show()"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sframe','Summary', {\"ipython\": true, \"sketch\": {\"sherlock_count\": {\"std\": 16.146417160143457, \"complete\": true, \"min\": 0.0, \"max\": 96.0, \"quantile\": [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 6.0, 6.0, 6.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 8.0, 8.0, 8.0, 9.0, 9.0, 9.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 12.0, 12.0, 27.0, 34.0, 34.0, 35.0, 45.0, 45.0, 52.0, 61.0, 61.0, 96.0, 96.0], \"median\": 5.0, \"numeric\": true, \"num_unique\": 19, \"num_undefined\": 0, \"var\": 260.70678710937506, \"progress\": 1.0, \"size\": 64, \"frequent_items\": {\"96\": {\"frequency\": 1, \"value\": 96}, \"1\": {\"frequency\": 6, \"value\": 1}, \"34\": {\"frequency\": 1, \"value\": 34}, \"35\": {\"frequency\": 1, \"value\": 35}, \"4\": {\"frequency\": 3, \"value\": 4}, \"5\": {\"frequency\": 5, \"value\": 5}, \"6\": {\"frequency\": 2, \"value\": 6}, \"7\": {\"frequency\": 6, \"value\": 7}, \"8\": {\"frequency\": 2, \"value\": 8}, \"9\": {\"frequency\": 2, \"value\": 9}, \"10\": {\"frequency\": 7, \"value\": 10}, \"12\": {\"frequency\": 1, \"value\": 12}, \"2\": {\"frequency\": 8, \"value\": 2}, \"45\": {\"frequency\": 1, \"value\": 45}, \"3\": {\"frequency\": 12, \"value\": 3}, \"52\": {\"frequency\": 1, \"value\": 52}, \"27\": {\"frequency\": 1, \"value\": 27}, \"61\": {\"frequency\": 1, \"value\": 61}, \"0\": {\"frequency\": 3, \"value\": 0}}, \"mean\": 9.609375}, \"watson_count\": {\"std\": 3.0783629349664086, \"complete\": true, \"min\": 0.0, \"max\": 17.0, \"quantile\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 4.0, 7.0, 8.0, 8.0, 11.0, 12.0, 12.0, 17.0, 17.0], \"median\": 1.0, \"numeric\": true, \"num_unique\": 10, \"num_undefined\": 0, \"var\": 9.476318359375, \"progress\": 1.0, \"size\": 64, \"frequent_items\": {\"0\": {\"frequency\": 28, \"value\": 0}, \"1\": {\"frequency\": 16, \"value\": 1}, \"2\": {\"frequency\": 8, \"value\": 2}, \"3\": {\"frequency\": 4, \"value\": 3}, \"4\": {\"frequency\": 3, \"value\": 4}, \"7\": {\"frequency\": 1, \"value\": 7}, \"8\": {\"frequency\": 1, \"value\": 8}, \"11\": {\"frequency\": 1, \"value\": 11}, \"12\": {\"frequency\": 1, \"value\": 12}, \"17\": {\"frequency\": 1, \"value\": 17}}, \"mean\": 1.734375}, \"elementary_count\": {\"std\": 0.35869500885153116, \"complete\": true, \"min\": 0.0, \"max\": 2.0, \"quantile\": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 2.0, 2.0], \"median\": 0.0, \"numeric\": true, \"num_unique\": 3, \"num_undefined\": 0, \"var\": 0.128662109375, \"progress\": 1.0, \"size\": 64, \"frequent_items\": {\"0\": {\"frequency\": 58, \"value\": 0}, \"1\": {\"frequency\": 5, \"value\": 1}, \"2\": {\"frequency\": 1, \"value\": 2}}, \"mean\": 0.109375}}, \"selected_variable\": {\"name\": [\"<SFrame>\"], \"descriptives\": {\"rows\": 64, \"columns\": 3}, \"view_component\": \"Summary\", \"view_file\": \"sframe\", \"view_params\": {\"y\": null, \"x\": null, \"columns\": [\"sherlock_count\", \"watson_count\", \"elementary_count\"], \"view\": null}, \"view_components\": [\"Summary\", \"Table\", \"Bar Chart\", \"BoxWhisker Plot\", \"Line Chart\", \"Scatter Plot\", \"Heat Map\", \"Plots\"], \"type\": \"SFrame\", \"columns\": [{\"dtype\": \"int\", \"name\": \"sherlock_count\"}, {\"dtype\": \"int\", \"name\": \"watson_count\"}, {\"dtype\": \"int\", \"name\": \"elementary_count\"}], \"column_identifiers\": [\"sherlock_count\", \"watson_count\", \"elementary_count\"]}, \"columns\": [{\"dtype\": \"int\", \"name\": \"sherlock_count\"}, {\"dtype\": \"int\", \"name\": \"watson_count\"}, {\"dtype\": \"int\", \"name\": \"elementary_count\"}]}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It is nice to see that the mean number of times the word 'Sherlock' appear in the stories is 9.609 times while the mean the word 'Watson' appear is only 1.734.\n",
      "Moreover, there are stories, such as [The Adventure of the Lion's Mane](https://sherlock-holm.es/stories/pdf/a4/1-sided/lion.pdf) that the word 'Sherlock' doesn't appear even once. \n",
      "\n",
      "Let's try to use simple linear regression to predict the number of times the word 'Sherlock' appear in a text based on the number of time the word 'Watson' appear in the text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "linear_reg = gl.linear_regression.create(sf, target='sherlock_count', features=['watson_count'])\n",
      "linear_reg.show()"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PROGRESS: Linear regression:\n",
        "PROGRESS: --------------------------------------------------------\n",
        "PROGRESS: Number of examples          : 64\n",
        "PROGRESS: Number of features          : 1\n",
        "PROGRESS: Number of unpacked features : 1\n",
        "PROGRESS: Number of coefficients    : 2\n",
        "PROGRESS: Starting Newton Method\n",
        "PROGRESS: --------------------------------------------------------\n",
        "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
        "PROGRESS: | Iteration | Passes   | Elapsed Time | Training-max_error | Training-rmse |\n",
        "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
        "PROGRESS: | 1         | 2        | 1.005666     | 81.541394          | 14.740310     |\n",
        "PROGRESS: +-----------+----------+--------------+--------------------+---------------+\n",
        "PROGRESS: SUCCESS: Optimal solution found.\n",
        "PROGRESS:\n"
       ]
      },
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('model','Summary', {\"comparison\": null, \"selected_variable\": {\"comparison\": null, \"name\": [\"linear_reg\"], \"view_file\": \"model\", \"view_component\": \"Summary\", \"view_params\": {\"model_type\": \"regression\", \"view\": \"Summary\"}, \"view_components\": [\"Summary\", \"Evaluation\", \"Comparison\"], \"model_type\": \"regression\", \"attributes\": {\"section_titles\": [\"Schema\", \"Hyperparameters\", \"Training Summary\", \"Settings\", \"Highest Positive Coefficients\", \"Lowest Negative Coefficients\"], \"sections\": [[[\"Number of coefficients\", 2], [\"Number of examples\", 64], [\"Number of feature columns\", 1], [\"Number of unpacked features\", 1]], [[\"L1 penalty\", 0.0], [\"L2 penalty\", 0.01]], [[\"Solver\", \"auto\"], [\"Solver iterations\", 1], [\"Solver status\", \"SUCCESS: Optimal solution found.\"], [\"Training time (sec)\", 1.0069]], [[\"Residual sum of squares\", 13905.7118], [\"Training RMSE\", 14.7403]], [[\"(intercept)\", 5.8972], [\"watson_count\", 2.1404]], [[\"No Negative Coefficients\", \"\"]]]}, \"evaluations\": [], \"type\": \"Model\"}, \"ipython\": true, \"view_params\": {\"model_type\": \"regression\", \"view\": \"Summary\"}, \"model_type\": \"regression\", \"attributes\": {\"section_titles\": [\"Schema\", \"Hyperparameters\", \"Training Summary\", \"Settings\", \"Highest Positive Coefficients\", \"Lowest Negative Coefficients\"], \"sections\": [[[\"Number of coefficients\", 2], [\"Number of examples\", 64], [\"Number of feature columns\", 1], [\"Number of unpacked features\", 1]], [[\"L1 penalty\", 0.0], [\"L2 penalty\", 0.01]], [[\"Solver\", \"auto\"], [\"Solver iterations\", 1], [\"Solver status\", \"SUCCESS: Optimal solution found.\"], [\"Training time (sec)\", 1.0069]], [[\"Residual sum of squares\", 13905.7118], [\"Training RMSE\", 14.7403]], [[\"(intercept)\", 5.8972], [\"watson_count\", 2.1404]], [[\"No Negative Coefficients\", \"\"]]]}, \"evaluations\": []}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "According to the simple linear regression, we have the following equation:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$$sherlock_{count} = 2.1404*watson_{count} + 5.8972$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a lot of other really interesting insights that one can discover using similar methodology. I leave the reader to discover these insights by themselves. Let's move to the next section\n",
      "and create some nice graphs using various entity extraction tools."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <a id=\"sn\"></a>3. Constructing Social Networks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "<i>\"Listen, what I said before John, I meant it. I don\u2019t have friends; I\u2019ve just got one.\"\n",
      "                                               -Sherlock, The Hounds of Baskerville, 2012 </i>\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of my main fields of interest are social networks. I love to study and visualize graphs of [various types of networks](http://proj.ise.bgu.ac.il/sns/gallery.html). One of the nice [studies](http://www.technologyreview.com/view/516081/the-remarkable-properties-of-mythological-social-networks/) that I read not long ago showes that it is possible to create the social network of book characters. For example, [Miranda et al.](http://arxiv.org/abs/1306.2537) built and analyzed a social network utilizing the Odyssey of Homer. \n",
      "\n",
      "To manually create a precise social network between Sherlock Holmes characters, we can read the stories and whenever two characters have a conversation, or appear in the same scene, we add to the network nodes with the two characters names (if there are not in the network already), and create a link between the two characters. In case, we want to create a weighted social network, we can also add a weight to each link with the number of times each two characters talked to each other. \n",
      "\n",
      "When processing a large text corpus, manually using this process to construct a social network can very time-consuming. Therefore, we would like to perform this process automatically. One of the ways to consturct the social network is by using various NLP algorithms that analyze the text and \"understand\" the relationships between two entities. However, I am not familiar with open source tools that can analyze a text corpus and infer the connections between two entities with high precision. \n",
      "\n",
      "In this section, I will demonstrate some very simple techniques that can be utilized to study the social connections among characters in Sherlock Holmes stories. These techniques won't create the most precise social network. However, the created network is sufficient to observe some interesting insights about the relationships among the stories' characters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.1 Constructing Social Network using Names List"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using this techniques, we will split the downloaded Sherlock Holmes stories into sentences, and using a predefined list of names of book characters we will create a social network with links among the stories characters by adding a link between each two characters that appear in the same sentence. Let start constructing the social network by splitting the stories into sentences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import graphlab as gl\n",
      "import re,nltk\n",
      "gl.canvas.set_target('ipynb')\n",
      "\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "def txt2sentences(txt, remove_none_english_chars=True):\n",
      "    \"\"\"\n",
      "    Split the English text into sentences using NLTK\n",
      "    :param txt: input text.    \n",
      "    :param remove_none_english_chars: if True then remove none English chars from text\n",
      "    :return: string in which each line consists of single sentence from the original input text.\n",
      "    :rtype: str\n",
      "    \"\"\"        \n",
      "    txt = txt.decode(\"utf8\") \n",
      "    # split text into sentences using nltk packages\n",
      "    for s in tokenizer.tokenize(txt):\n",
      "        if remove_none_english_chars:\n",
      "            #remove none English chars\n",
      "            s = re.sub(\"[^a-zA-Z]\", \" \", s)\n",
      "        yield s\n",
      "        \n",
      "sf = gl.load_sframe(\"%s/books.sframe\" % BASE_DIR)\n",
      "sf['sentences'] = sf['text'].apply(lambda t: list(txt2sentences(t)))"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf_sentences = sf.flat_map(['title', 'text'], lambda t: [[t['title'],s.strip()] for s in txt2sentences(t['text'])])\n",
      "sf_sentences = sf_sentences.rename({'text': 'sentence'})\n",
      "re_words_split = re.compile(\"(\\w+)\")\n",
      "\n",
      "#split each sentence into words\n",
      "sf_sentences['words'] = sf_sentences['sentence'].apply(lambda s:re_words_split.findall(s))\n",
      "sf_sentences.save(\"%s/sentences.sframe\" % BASE_DIR)\n",
      "sf_sentences.head(3)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false,
      "scrolled": true
     },
     "outputs": [
      {
       "data": {
        "text/html": [
         "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
         "    <tr>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">title</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">sentence</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">words</th>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Adventures of<br>Sherlock Holmes ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">THE ADVENTURES OF<br>SHERLOCK HOLMES ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[THE, ADVENTURES, OF,<br>SHERLOCK, HOLMES, Art ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Adventures of<br>Sherlock Holmes ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I have seldom heard him<br>mention her under any ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[I, have, seldom, heard,<br>him, mention, her, un ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Adventures of<br>Sherlock Holmes ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">In his eyes she eclipses<br>and      predominates ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[In, his, eyes, she,<br>eclipses, and, ...</td>\n",
         "    </tr>\n",
         "</table>\n",
         "[3 rows x 3 columns]<br/>\n",
         "</div>"
        ],
        "text/plain": [
         "Columns:\n",
         "\ttitle\tstr\n",
         "\tsentence\tstr\n",
         "\twords\tlist\n",
         "\n",
         "Rows: 3\n",
         "\n",
         "Data:\n",
         "+-------------------------------+-------------------------------+\n",
         "|             title             |            sentence           |\n",
         "+-------------------------------+-------------------------------+\n",
         "| The Adventures of Sherlock... | THE ADVENTURES OF SHERLOCK... |\n",
         "| The Adventures of Sherlock... | I have seldom heard him   ... |\n",
         "| The Adventures of Sherlock... | In his eyes she eclipses a... |\n",
         "+-------------------------------+-------------------------------+\n",
         "+-------------------------------+\n",
         "|             words             |\n",
         "+-------------------------------+\n",
         "| [THE, ADVENTURES, OF, SHER... |\n",
         "| [I, have, seldom, heard, h... |\n",
         "| [In, his, eyes, she, eclip... |\n",
         "+-------------------------------+\n",
         "[3 rows x 3 columns]"
        ]
       },
       "execution_count": 17,
       "metadata": {},
       "output_type": "execute_result"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": false
     },
     "source": [
      "We created a SFrame named <i>sf_sentences</i> in which each row contains a single sentence. Now let's find out which two or more characters from the following [link](http://www.wikiwand.com/en/Category:Sherlock_Holmes_characters) appear in the same sentences. Notice that we only use the characters unique names so we don't mix up between characters with similar names. For example, the name Holmes can represent both Sherlock Holmes and Mycroft Holmes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main_characters_set = set([\"Irene\",\"Mycroft\",\"Lestrade\",\"Sherlock\",\"Moran\",\"Moriarty\",\"Watson\" ])\n",
      "sf_sentences['characters'] = sf_sentences['words'].apply(lambda w: list(set(w) & main_characters_set))"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, the <i>'characters'</i> column contain the names of the main characters that appear in the same sentences together. Let's use this information to create the characters social network by constructing a SGraph object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "from collections import Counter\n",
      "from graphlab import SGraph, Vertex, Edge\n",
      "\n",
      "def get_characters_graph(sf, min_edge_strength=1):\n",
      "    \"\"\"\n",
      "    Constructs a social network from the an input SFrame. In the social network the verticies are the characters\n",
      "    and the edges are only between characters that appear in the same sentence at least min_edge_strength times\n",
      "    :param sf: input SFrame object that contains 'characters' column   \n",
      "    :param min_edge_strength: minimal connetion strength between two characters.  \n",
      "    :return: SGraph object constructed from the input SFrame. The graph only contains edges with \n",
      "        the at least the input minimal strength between between the characters.\n",
      "    :rtype: gl.SGraph\n",
      "    \"\"\"\n",
      "    #filter sentences with less than two characters\n",
      "    sf['characters_num'] = sf['characters'].apply(lambda l: len(l))\n",
      "    sf = sf_sentences[sf['characters_num'] > 1]\n",
      "    characters_links = []\n",
      "    for l in sf['characters']:    \n",
      "        # if there are more than two characters in the same sentences. Create all link combinations between\n",
      "        # all the characters (order doesn't matter)\n",
      "        characters_links += itertools.combinations(l,2)\n",
      "\n",
      "    #calculating the connections strength between each two characters\n",
      "    c = Counter(characters_links)\n",
      "    g = SGraph()\n",
      "\n",
      "    edges_list = []\n",
      "    for l,s in c.iteritems():    \n",
      "        if s < min_edge_strength:\n",
      "            # filter out connections that appear less than min_edge_strength\n",
      "            continue\n",
      "        edges_list.append(Edge(l[0], l[1], attr={'strength':s}))\n",
      "\n",
      "    g = g.add_edges(edges_list)\n",
      "    return g\n",
      "\n",
      "g = get_characters_graph(sf_sentences)\n",
      "g.show(vlabel=\"__id\", elabel=\"strength\", node_size=200)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sgraph','View', {\"edges_labels\": [6, 6, 11, 1, 4, 2, 4, 6, 14], \"selected_variable\": {\"name\": [\"g\"], \"view_file\": \"sgraph\", \"view_component\": \"View\", \"view_params\": {\"elabel_hover\": false, \"vertex_positions\": null, \"h_offset\": 0.0, \"node_size\": 200, \"ecolor\": [0.37, 0.33, 0.33], \"elabel\": \"strength\", \"arrows\": false, \"ewidth\": 1, \"vlabel\": \"__id\", \"highlight_color\": [0.69, 0.0, 0.498], \"vcolor\": [0.522, 0.741, 0.0], \"vlabel_hover\": false, \"highlight\": {}, \"v_offset\": 0.03}, \"view_components\": [\"View\"], \"type\": \"SGraph\", \"descriptives_links\": {\"edges\": \"edges\", \"vertices\": \"vertices\"}, \"descriptives\": {\"edges\": 9, \"vertices\": 7}}, \"positions\": null, \"error_type\": 0, \"vertices\": [\"Mycroft\", \"Lestrade\", \"Moran\", \"Moriarty\", \"Irene\", \"Sherlock\", \"Watson\"], \"vertices_labels\": [\"Mycroft\", \"Lestrade\", \"Moran\", \"Moriarty\", \"Irene\", \"Sherlock\", \"Watson\"], \"edges\": [[\"Mycroft\", \"Sherlock\"], [\"Lestrade\", \"Mycroft\"], [\"Lestrade\", \"Sherlock\"], [\"Moriarty\", \"Moran\"], [\"Moriarty\", \"Sherlock\"], [\"Sherlock\", \"Moran\"], [\"Irene\", \"Sherlock\"], [\"Watson\", \"Moriarty\"], [\"Watson\", \"Sherlock\"]], \"ipython\": true, \"error_msg\": \"\"}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "According to Sherlock's social network, it can be noticed that Sherlock has two main social circles. The first one is circle of friends that include Mycroft and Lestrade. Additionally, he has a circle of enemies that include Moriaty and Moran. Additionally, we can notice Watson is strongly connected to Sherlock and Sherlock's nemesis Moriaty. <br>Let's repeat the experiments only this time we also add minor characters from the following [link](http://www.wikiwand.com/en/Minor_Sherlock_Holmes_characters)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "minor_characters_set = set([\"Irene\",\"Mycroft\",\"Lestrade\",\"Sherlock\",\"Moran\",\"Moriarty\",\"Watson\",\"Baynes\",\"Billy\",\"Bradstreet\",\"Gregson\"\n",
      "                            ,\"Hopkins\",\"Hudson\",\"Shinwell\",\"Athelney\",\"Mary\",\"Langdale\",\"Toby\",\"Wiggins\"])\n",
      "\n",
      "sf_sentences['characters'] = sf_sentences['words'].apply(lambda w: list(set(w) & minor_characters_set))\n",
      "sf_sentences['characters_num'] = sf_sentences['characters'].apply(lambda l: len(l))\n",
      "sf_sentences = sf_sentences[sf_sentences['characters_num'] > 1]\n",
      "\n",
      "g = get_characters_graph(sf_sentences)\n",
      "g.show(vlabel=\"__id\", elabel=\"strength\", node_size=200)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sgraph','View', {\"edges_labels\": [6, 1, 6, 11, 15, 1, 4, 1, 3, 4, 2, 2, 2, 1, 2, 4, 3, 4, 1, 6, 4, 14, 2, 1], \"selected_variable\": {\"name\": [\"g\"], \"view_file\": \"sgraph\", \"view_component\": \"View\", \"view_params\": {\"elabel_hover\": false, \"vertex_positions\": null, \"h_offset\": 0.0, \"node_size\": 200, \"ecolor\": [0.37, 0.33, 0.33], \"elabel\": \"strength\", \"arrows\": false, \"ewidth\": 1, \"vlabel\": \"__id\", \"highlight_color\": [0.69, 0.0, 0.498], \"vcolor\": [0.522, 0.741, 0.0], \"vlabel_hover\": false, \"highlight\": {}, \"v_offset\": 0.03}, \"view_components\": [\"View\"], \"type\": \"SGraph\", \"descriptives_links\": {\"edges\": \"edges\", \"vertices\": \"vertices\"}, \"descriptives\": {\"edges\": 24, \"vertices\": 15}}, \"positions\": null, \"error_type\": 0, \"vertices\": [\"Langdale\", \"Mycroft\", \"Lestrade\", \"Moran\", \"Hudson\", \"Mary\", \"Moriarty\", \"Billy\", \"Irene\", \"Sherlock\", \"Gregson\", \"Bradstreet\", \"Watson\", \"Athelney\", \"Hopkins\"], \"vertices_labels\": [\"Langdale\", \"Mycroft\", \"Lestrade\", \"Moran\", \"Hudson\", \"Mary\", \"Moriarty\", \"Billy\", \"Irene\", \"Sherlock\", \"Gregson\", \"Bradstreet\", \"Watson\", \"Athelney\", \"Hopkins\"], \"edges\": [[\"Mycroft\", \"Sherlock\"], [\"Langdale\", \"Watson\"], [\"Lestrade\", \"Mycroft\"], [\"Lestrade\", \"Sherlock\"], [\"Lestrade\", \"Gregson\"], [\"Lestrade\", \"Athelney\"], [\"Hudson\", \"Mycroft\"], [\"Moriarty\", \"Moran\"], [\"Hudson\", \"Sherlock\"], [\"Moriarty\", \"Sherlock\"], [\"Hudson\", \"Gregson\"], [\"Hudson\", \"Hopkins\"], [\"Sherlock\", \"Moran\"], [\"Billy\", \"Hudson\"], [\"Irene\", \"Mary\"], [\"Irene\", \"Sherlock\"], [\"Sherlock\", \"Gregson\"], [\"Sherlock\", \"Hopkins\"], [\"Gregson\", \"Athelney\"], [\"Watson\", \"Moriarty\"], [\"Watson\", \"Hudson\"], [\"Watson\", \"Sherlock\"], [\"Bradstreet\", \"Sherlock\"], [\"Watson\", \"Gregson\"]], \"ipython\": true, \"error_msg\": \"\"}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We got a more complex social network with the additional minor characters.\n",
      "I believe this social network graph can be improved by increasing the scope of characters search from single sentence to multiple sentences, or by using characters additional names and nick names. I leave the reader to try to improve the graph by themselves."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### 3.2 Constructing Social Network using Named Entity Recognition "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One of the disadvantages of the above method is that you need a predefined list of names to create the social network. However, in many cases this list is unavailable. Therefore, we need another method to find entities in the text. One common method to achieve this is using [Named Entity Recognition](http://www.wikiwand.com/en/Named-entity_recognition) (NER). By using NER algorithms, we can classify elements in the text into pre-defined categories, such as the names of persons, organizations, and locations. There are many tools that can perform NER, such as [OpenNLP](http://www.wikiwand.com/en/OpenNLP), [Stanford Named Entity Recognizer]( http://nlp.stanford.edu/software/CRF-NER.shtml), [Rosette Entity Extractor](http://www.basistech.com/text-analytics/rosette/entity-extractor/). In this notebook, we will use the Stanford Named Entity Recognizer via NLTK. We will use NER algorithms to automatically  construct an entity list of the most common characters of the book.\n",
      "\n",
      "Please note that making NLTK run Stanford Named Entity Recognizer can be non-trivial. For more details, on how to make NLTK work with Stanford Named Entity Recognizer please read the information provided in the following links [1](http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages),[2](http://www.nltk.org/api/nltk.tag.html#module-nltk.tag.stanford), & [3](http://nlp.stanford.edu/software/parser-faq.shtml#). \n",
      "\n",
      "NOTE: running the next code section can take several minutes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tag import StanfordNERTagger\n",
      "\n",
      "sf_books =  gl.load_sframe(\"%s/books.sframe\" % BASE_DIR)\n",
      "\n",
      "#IMPORTANT: The directory that include the Stanford Named Entity Recognizer files it need to be updated according \n",
      "# to the local installation directory\n",
      "#STANFORD_DIR = BASE_DIR + \"/stanford-ner-2015-06-16/\"\n",
      "\n",
      "#need to insert as parameters the stanford-ner.jar and the type of classifier we want to use\n",
      "st = StanfordNERTagger('/Users/pablo/fromHomeMac/sherlock/stanford-ner-2015-04-20/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
      "                       '/Users/pablo/Downloads/stanford-ner-2015-04-20/stanford-ner.jar')\n",
      "st.java_options = \"-Xmx4096m\"\n",
      "\n",
      "sf_books['sentences'] = sf_books['text'].apply(lambda t: list(txt2sentences(t)))\n",
      "sf_books['words'] = sf_books['sentences'].apply(lambda l: [re_words_split.findall(s) for s in l])\n",
      "sf_books['NER'] = sf_books['words'].apply(lambda w: st.tag_sents(w))\n",
      "sf_books['person'] = sf_books['NER'].apply(lambda n: [e[0] for s in n for e in s if e[1] == 'PERSON'])\n",
      "\n",
      "person_list = []\n",
      "for p in sf_books['person']:\n",
      "    person_list += p\n",
      "    \n",
      "print len(set(person_list))"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1248\n"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "c = Counter(person_list)\n",
      "# We are removing some mistken classified words, too common names, and etc. to make the constructed social network\n",
      "# more readable.\n",
      "characters_set = set(i[0] for i in c.most_common(200)) - set(['the', 'You', 'Mrs', 'He', 'Dr', 'me','did', 'Mr', \n",
      "                                      'Now', 'My', 'Miss', 'of', 'Sir', 'Here', 'All', 'Our', 'sir',\n",
      "                                      'man', 'father', 'What', 'There', 'When', 'no', 'Lord', 'you', 'St',\n",
      "                                      'John', 'James',  'Holmes', 'Arthur', 'Conan', 'Doyle', 'Lady'])\n",
      "\n",
      "sf_sentences = gl.load_sframe(\"%s/sentences.sframe\" % BASE_DIR)\n",
      "sf_sentences['characters'] = sf_sentences['words'].apply(lambda w: list(set(w) & characters_set))\n",
      "sf_sentences['characters_num'] = sf_sentences['characters'].apply(lambda l: len(l))\n",
      "sf_sentences = sf_sentences[sf_sentences['characters_num'] > 1]\n",
      "g = get_characters_graph(sf_sentences, min_edge_strength=3)\n",
      "print g.summary()\n",
      "g.show(vlabel=\"__id\", elabel=\"strength\", node_size=200)\n"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'num_edges': 351, 'num_vertices': 184}\n"
       ]
      },
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sgraph','View', {\"edges_labels\": [4, 8, 5, 4, 3, 16, 8, 8, 4, 3, 12, 8, 6, 17, 4, 6, 4, 3, 25, 3, 4, 3, 27, 3, 15, 5, 3, 6, 19, 13, 4, 5, 3, 3, 6, 21, 9, 6, 48, 3, 4, 11, 5, 6, 15, 4, 3, 6, 6, 4, 4, 4, 3, 17, 4, 16, 4, 4, 12, 3, 3, 4, 5, 18, 4, 18, 70, 42, 14, 4, 36, 4, 8, 22, 32, 11, 6, 3, 14, 10, 3, 4, 4, 4, 6, 18, 4, 6, 26, 6, 3, 4, 4, 4, 4, 22, 4, 22, 8, 407, 38, 10, 8, 34, 12, 24, 6, 49, 20, 9, 24, 8, 8, 64, 15, 8, 18, 22, 3, 3, 22, 8, 8, 20, 10, 6, 11, 108, 18, 20, 14, 10, 3, 6, 21, 34, 6, 16, 14, 31, 6, 13, 34, 18, 9, 5, 17, 8, 6, 11, 9, 28, 8, 44, 14, 31, 12, 6, 53, 11, 10, 13, 12, 6, 22, 10, 5, 10, 19, 14, 6, 10, 17, 15, 18, 4, 16, 18, 6, 70, 5, 205, 22, 8, 12, 39, 12, 18, 16, 24, 6, 20, 4, 8, 6, 14, 17, 14, 9, 8, 8, 29, 8, 4, 18, 9, 29, 14, 3, 12, 8, 8, 4, 7, 12, 4, 3, 18, 593, 4, 14, 8, 6, 4, 8, 36, 33, 12, 106, 6, 12, 5, 5, 18, 12, 10, 16, 6, 7, 3, 12, 8, 8, 18, 18, 20, 24, 26, 44, 22, 26, 23, 8, 5, 42, 9, 20, 8, 8, 14, 14, 8, 18, 23, 6, 18, 44, 13, 3, 4, 14, 9, 6, 10, 7, 10, 12, 10, 15, 33, 18, 6, 5, 10, 4, 18, 4, 34, 6, 19, 14, 9, 4, 8, 4, 3, 3, 6, 7, 6, 6, 3, 7, 3, 4, 7, 3, 3, 8, 4, 4, 30, 28, 6, 3, 14, 4, 21, 4, 4, 4, 8, 10, 4, 4, 6, 6, 5, 8, 4, 4, 42, 6, 6, 3, 5, 4, 5, 3, 8, 8, 4, 8, 4, 37, 5, 20, 8, 4, 10, 56], \"selected_variable\": {\"name\": [\"g\"], \"view_file\": \"sgraph\", \"view_component\": \"View\", \"view_params\": {\"elabel_hover\": false, \"vertex_positions\": null, \"h_offset\": 0.0, \"node_size\": 200, \"ecolor\": [0.37, 0.33, 0.33], \"elabel\": \"strength\", \"arrows\": false, \"ewidth\": 1, \"vlabel\": \"__id\", \"highlight_color\": [0.69, 0.0, 0.498], \"vcolor\": [0.522, 0.741, 0.0], \"vlabel_hover\": false, \"highlight\": {}, \"v_offset\": 0.03}, \"view_components\": [\"View\"], \"type\": \"SGraph\", \"descriptives_links\": {\"edges\": \"edges\", \"vertices\": \"vertices\"}, \"descriptives\": {\"edges\": 351, \"vertices\": 184}}, \"positions\": null, \"error_type\": 0, \"vertices\": [\"Adair\", \"Angel\", \"Barker\", \"Coombe\", \"God\", \"Godfrey\", \"Harrison\", \"Jack\", \"Jones\", \"Joseph\", \"Lyons\", \"MacDonald\", \"McCarthy\", \"McGinty\", \"Mycroft\", \"Robert\", \"Slaney\", \"Straker\", \"Sutherland\", \"White\", \"don\", \"Adler\", \"Baron\", \"Baskerville\", \"Baynes\", \"Black\", \"Charles\", \"Clair\", \"Elsie\", \"Ettie\", \"Fitzroy\", \"Frances\", \"Jim\", \"Lee\", \"Lestrade\", \"McPherson\", \"Milverton\", \"Moran\", \"Peters\", \"Ross\", \"Sholto\", \"Tregennis\", \"Wilder\", \"Bannister\", \"Carruthers\", \"Ferrier\", \"Harker\", \"Hudson\", \"Lucy\", \"Martin\", \"Mary\", \"Merryweather\", \"Moriarty\", \"Morris\", \"Napoleon\", \"Peter\", \"Pinner\", \"Small\", \"Stapleton\", \"Sterndale\", \"Theresa\", \"Tracey\", \"Von\", \"Warren\", \"West\", \"Wilson\", \"Alice\", \"Billy\", \"Blessington\", \"Brackenstall\", \"Cecil\", \"Cushing\", \"Cyril\", \"Don\", \"Drebber\", \"Gorgiano\", \"Henry\", \"Hope\", \"Hosmer\", \"Huxtable\", \"Irene\", \"Jabez\", \"Johnson\", \"Jonas\", \"Jonathan\", \"Jove\", \"Mortimer\", \"Overton\", \"Percy\", \"Phelps\", \"Sherlock\", \"Simon\", \"Soames\", \"Susan\", \"Trevor\", \"Willoughby\", \"Acton\", \"Bartholomew\", \"Bennett\", \"Culverton\", \"Cunningham\", \"Gennaro\", \"Gregson\", \"Hugo\", \"Hunter\", \"I\", \"Lucas\", \"Murdoch\", \"Musgrave\", \"Peterson\", \"Prendergast\", \"Pycroft\", \"Ralph\", \"Scott\", \"Stangerson\", \"Staunton\", \"Woodley\", \"Alec\", \"Armstrong\", \"Baker\", \"Brown\", \"Cadogan\", \"Eccles\", \"Eustace\", \"Evans\", \"Garcia\", \"Henderson\", \"Mason\", \"Morrison\", \"Openshaw\", \"Scanlan\", \"Smith\", \"Stoner\", \"Toby\", \"Watson\", \"Baldwin\", \"Barclay\", \"Barrymore\", \"Bork\", \"Boss\", \"Browner\", \"Brunton\", \"Carey\", \"Doran\", \"Frank\", \"Gloria\", \"Hall\", \"Hayes\", \"Hilton\", \"Mawson\", \"McFarlane\", \"Neville\", \"Oberstein\", \"Oldacre\", \"Roylott\", \"Rucastle\", \"Simpson\", \"Thaddeus\", \"Toller\", \"William\", \"Athelney\", \"Beppo\", \"Brother\", \"Cubitt\", \"Douglas\", \"Ferguson\", \"Garrideb\", \"George\", \"Gibson\", \"Gilchrist\", \"Gregory\", \"Hilda\", \"Hopkins\", \"Jefferson\", \"McMurdo\", \"Melas\", \"Munro\", \"Sarah\", \"Stanley\", \"Trevelyan\", \"Turner\", \"Victor\", \"Williamson\", \"Windibank\"], \"vertices_labels\": [\"Adair\", \"Angel\", \"Barker\", \"Coombe\", \"God\", \"Godfrey\", \"Harrison\", \"Jack\", \"Jones\", \"Joseph\", \"Lyons\", \"MacDonald\", \"McCarthy\", \"McGinty\", \"Mycroft\", \"Robert\", \"Slaney\", \"Straker\", \"Sutherland\", \"White\", \"don\", \"Adler\", \"Baron\", \"Baskerville\", \"Baynes\", \"Black\", \"Charles\", \"Clair\", \"Elsie\", \"Ettie\", \"Fitzroy\", \"Frances\", \"Jim\", \"Lee\", \"Lestrade\", \"McPherson\", \"Milverton\", \"Moran\", \"Peters\", \"Ross\", \"Sholto\", \"Tregennis\", \"Wilder\", \"Bannister\", \"Carruthers\", \"Ferrier\", \"Harker\", \"Hudson\", \"Lucy\", \"Martin\", \"Mary\", \"Merryweather\", \"Moriarty\", \"Morris\", \"Napoleon\", \"Peter\", \"Pinner\", \"Small\", \"Stapleton\", \"Sterndale\", \"Theresa\", \"Tracey\", \"Von\", \"Warren\", \"West\", \"Wilson\", \"Alice\", \"Billy\", \"Blessington\", \"Brackenstall\", \"Cecil\", \"Cushing\", \"Cyril\", \"Don\", \"Drebber\", \"Gorgiano\", \"Henry\", \"Hope\", \"Hosmer\", \"Huxtable\", \"Irene\", \"Jabez\", \"Johnson\", \"Jonas\", \"Jonathan\", \"Jove\", \"Mortimer\", \"Overton\", \"Percy\", \"Phelps\", \"Sherlock\", \"Simon\", \"Soames\", \"Susan\", \"Trevor\", \"Willoughby\", \"Acton\", \"Bartholomew\", \"Bennett\", \"Culverton\", \"Cunningham\", \"Gennaro\", \"Gregson\", \"Hugo\", \"Hunter\", \"I\", \"Lucas\", \"Murdoch\", \"Musgrave\", \"Peterson\", \"Prendergast\", \"Pycroft\", \"Ralph\", \"Scott\", \"Stangerson\", \"Staunton\", \"Woodley\", \"Alec\", \"Armstrong\", \"Baker\", \"Brown\", \"Cadogan\", \"Eccles\", \"Eustace\", \"Evans\", \"Garcia\", \"Henderson\", \"Mason\", \"Morrison\", \"Openshaw\", \"Scanlan\", \"Smith\", \"Stoner\", \"Toby\", \"Watson\", \"Baldwin\", \"Barclay\", \"Barrymore\", \"Bork\", \"Boss\", \"Browner\", \"Brunton\", \"Carey\", \"Doran\", \"Frank\", \"Gloria\", \"Hall\", \"Hayes\", \"Hilton\", \"Mawson\", \"McFarlane\", \"Neville\", \"Oberstein\", \"Oldacre\", \"Roylott\", \"Rucastle\", \"Simpson\", \"Thaddeus\", \"Toller\", \"William\", \"Athelney\", \"Beppo\", \"Brother\", \"Cubitt\", \"Douglas\", \"Ferguson\", \"Garrideb\", \"George\", \"Gibson\", \"Gilchrist\", \"Gregory\", \"Hilda\", \"Hopkins\", \"Jefferson\", \"McMurdo\", \"Melas\", \"Munro\", \"Sarah\", \"Stanley\", \"Trevelyan\", \"Turner\", \"Victor\", \"Williamson\", \"Windibank\"], \"edges\": [[\"MacDonald\", \"White\"], [\"God\", \"Jack\"], [\"McGinty\", \"Jack\"], [\"Lyons\", \"Coombe\"], [\"God\", \"don\"], [\"Joseph\", \"Harrison\"], [\"don\", \"Jack\"], [\"Adair\", \"Moran\"], [\"Lyons\", \"Tracey\"], [\"Jones\", \"Small\"], [\"Sutherland\", \"Mary\"], [\"Robert\", \"Simon\"], [\"Mycroft\", \"Sherlock\"], [\"Barker\", \"Cecil\"], [\"Joseph\", \"Drebber\"], [\"don\", \"Sherlock\"], [\"don\", \"Henry\"], [\"God\", \"Sherlock\"], [\"Jones\", \"I\"], [\"Joseph\", \"Smith\"], [\"MacDonald\", \"Mason\"], [\"God\", \"Watson\"], [\"White\", \"Mason\"], [\"Jones\", \"Watson\"], [\"McGinty\", \"Boss\"], [\"Robert\", \"Ferguson\"], [\"McGinty\", \"Brother\"], [\"Mycroft\", \"Brother\"], [\"Jones\", \"Athelney\"], [\"Barker\", \"Douglas\"], [\"Slaney\", \"Cubitt\"], [\"McGinty\", \"McMurdo\"], [\"Lestrade\", \"don\"], [\"Black\", \"Jack\"], [\"Lestrade\", \"Mycroft\"], [\"Charles\", \"Baskerville\"], [\"Milverton\", \"Charles\"], [\"Lee\", \"Clair\"], [\"Tregennis\", \"Mortimer\"], [\"Charles\", \"Mortimer\"], [\"Adler\", \"Sherlock\"], [\"Lestrade\", \"Sherlock\"], [\"Baskerville\", \"Henry\"], [\"Sholto\", \"Bartholomew\"], [\"Lestrade\", \"Gregson\"], [\"Milverton\", \"Watson\"], [\"Baskerville\", \"Hall\"], [\"Charles\", \"Hall\"], [\"Jim\", \"Browner\"], [\"Moran\", \"Roylott\"], [\"Charles\", \"Barrymore\"], [\"Hudson\", \"Mycroft\"], [\"Hudson\", \"don\"], [\"Tracey\", \"Coombe\"], [\"Peter\", \"Lee\"], [\"Peter\", \"Black\"], [\"Von\", \"Baron\"], [\"Hudson\", \"Napoleon\"], [\"Lucy\", \"Ferrier\"], [\"Hudson\", \"Sherlock\"], [\"Lucy\", \"Hope\"], [\"Moriarty\", \"Sherlock\"], [\"Ferrier\", \"Hope\"], [\"Wilson\", \"Jabez\"], [\"Bannister\", \"Soames\"], [\"Small\", \"Jonathan\"], [\"Von\", \"Bork\"], [\"Peter\", \"Carey\"], [\"Morris\", \"Brother\"], [\"Morris\", \"McMurdo\"], [\"Hosmer\", \"Angel\"], [\"Huxtable\", \"Wilder\"], [\"Mortimer\", \"Baskerville\"], [\"Henry\", \"Baskerville\"], [\"Irene\", \"Adler\"], [\"Henry\", \"Stapleton\"], [\"Henry\", \"Mortimer\"], [\"Mortimer\", \"Henry\"], [\"Percy\", \"Phelps\"], [\"Cyril\", \"Overton\"], [\"Drebber\", \"Hope\"], [\"Irene\", \"Sherlock\"], [\"Brackenstall\", \"I\"], [\"Willoughby\", \"I\"], [\"Brackenstall\", \"Eustace\"], [\"Willoughby\", \"Smith\"], [\"Alice\", \"Rucastle\"], [\"Simon\", \"Doran\"], [\"Jonas\", \"Oldacre\"], [\"Jonas\", \"McFarlane\"], [\"Henry\", \"Hall\"], [\"Hope\", \"Hilda\"], [\"Percy\", \"Trevelyan\"], [\"Trevor\", \"Victor\"], [\"Sherlock\", \"Hopkins\"], [\"I\", \"Barker\"], [\"Woodley\", \"Jack\"], [\"I\", \"McCarthy\"], [\"I\", \"Slaney\"], [\"I\", \"don\"], [\"Staunton\", \"Godfrey\"], [\"Stangerson\", \"Joseph\"], [\"I\", \"White\"], [\"I\", \"Jack\"], [\"I\", \"Angel\"], [\"I\", \"Straker\"], [\"I\", \"MacDonald\"], [\"I\", \"Godfrey\"], [\"I\", \"Harrison\"], [\"I\", \"Lyons\"], [\"I\", \"Joseph\"], [\"I\", \"McGinty\"], [\"I\", \"Coombe\"], [\"I\", \"God\"], [\"I\", \"Robert\"], [\"I\", \"Sutherland\"], [\"I\", \"Mycroft\"], [\"I\", \"Tregennis\"], [\"Bartholomew\", \"Sholto\"], [\"Stangerson\", \"Lestrade\"], [\"I\", \"Milverton\"], [\"I\", \"Baynes\"], [\"I\", \"McPherson\"], [\"I\", \"Ross\"], [\"I\", \"Clair\"], [\"I\", \"Elsie\"], [\"I\", \"Fitzroy\"], [\"I\", \"Lestrade\"], [\"I\", \"Wilder\"], [\"I\", \"Moran\"], [\"I\", \"Frances\"], [\"Hugo\", \"Baskerville\"], [\"I\", \"Jim\"], [\"I\", \"Lee\"], [\"I\", \"Sholto\"], [\"I\", \"Baskerville\"], [\"I\", \"Baron\"], [\"I\", \"Adler\"], [\"I\", \"Ettie\"], [\"I\", \"Charles\"], [\"I\", \"Peters\"], [\"I\", \"Black\"], [\"I\", \"Hudson\"], [\"Woodley\", \"Carruthers\"], [\"I\", \"Tracey\"], [\"I\", \"Napoleon\"], [\"I\", \"Wilson\"], [\"I\", \"Martin\"], [\"I\", \"Harker\"], [\"I\", \"Small\"], [\"I\", \"Ferrier\"], [\"I\", \"Peter\"], [\"I\", \"Bannister\"], [\"I\", \"Moriarty\"], [\"I\", \"Pinner\"], [\"I\", \"Stapleton\"], [\"I\", \"Von\"], [\"I\", \"Sterndale\"], [\"I\", \"Mary\"], [\"I\", \"Lucy\"], [\"I\", \"Theresa\"], [\"I\", \"Morris\"], [\"I\", \"Warren\"], [\"I\", \"Merryweather\"], [\"I\", \"Carruthers\"], [\"I\", \"West\"], [\"I\", \"Cecil\"], [\"I\", \"Jonas\"], [\"I\", \"Trevor\"], [\"I\", \"Hosmer\"], [\"I\", \"Jabez\"], [\"I\", \"Percy\"], [\"Stangerson\", \"Drebber\"], [\"I\", \"Hope\"], [\"I\", \"Soames\"], [\"I\", \"Brackenstall\"], [\"I\", \"Alice\"], [\"I\", \"Cushing\"], [\"I\", \"Johnson\"], [\"I\", \"Henry\"], [\"I\", \"Susan\"], [\"I\", \"Sherlock\"], [\"I\", \"Simon\"], [\"I\", \"Cyril\"], [\"I\", \"Overton\"], [\"I\", \"Mortimer\"], [\"I\", \"Don\"], [\"I\", \"Blessington\"], [\"I\", \"Huxtable\"], [\"I\", \"Phelps\"], [\"I\", \"Jove\"], [\"I\", \"Drebber\"], [\"I\", \"Billy\"], [\"I\", \"Jonathan\"], [\"I\", \"Gorgiano\"], [\"I\", \"Irene\"], [\"I\", \"Scott\"], [\"I\", \"Woodley\"], [\"Stangerson\", \"I\"], [\"I\", \"Lucas\"], [\"I\", \"Acton\"], [\"I\", \"Musgrave\"], [\"I\", \"Bartholomew\"], [\"I\", \"Peterson\"], [\"I\", \"Staunton\"], [\"I\", \"Bennett\"], [\"I\", \"Gregson\"], [\"I\", \"Pycroft\"], [\"I\", \"Hugo\"], [\"I\", \"Cunningham\"], [\"I\", \"Prendergast\"], [\"I\", \"Hunter\"], [\"I\", \"Culverton\"], [\"I\", \"Ralph\"], [\"I\", \"Gennaro\"], [\"Acton\", \"Cunningham\"], [\"I\", \"Murdoch\"], [\"Culverton\", \"Smith\"], [\"I\", \"Watson\"], [\"I\", \"Henderson\"], [\"I\", \"Stoner\"], [\"I\", \"Eccles\"], [\"I\", \"Eustace\"], [\"I\", \"Scanlan\"], [\"I\", \"Openshaw\"], [\"Scott\", \"Eccles\"], [\"I\", \"Smith\"], [\"I\", \"Mason\"], [\"I\", \"Baker\"], [\"I\", \"Morrison\"], [\"I\", \"Alec\"], [\"I\", \"Toby\"], [\"I\", \"Evans\"], [\"Cunningham\", \"Alec\"], [\"I\", \"Garcia\"], [\"I\", \"Brown\"], [\"I\", \"Armstrong\"], [\"Ralph\", \"Smith\"], [\"I\", \"Thaddeus\"], [\"I\", \"William\"], [\"I\", \"Mawson\"], [\"I\", \"Neville\"], [\"I\", \"Hayes\"], [\"I\", \"Oldacre\"], [\"I\", \"Carey\"], [\"Scott\", \"Gloria\"], [\"I\", \"Barclay\"], [\"Pycroft\", \"Hall\"], [\"I\", \"Hall\"], [\"I\", \"Brunton\"], [\"I\", \"Frank\"], [\"I\", \"Barrymore\"], [\"I\", \"Baldwin\"], [\"I\", \"Boss\"], [\"I\", \"Rucastle\"], [\"I\", \"Gloria\"], [\"I\", \"Hilton\"], [\"I\", \"Roylott\"], [\"I\", \"Browner\"], [\"I\", \"McFarlane\"], [\"I\", \"Simpson\"], [\"I\", \"Bork\"], [\"I\", \"Cubitt\"], [\"I\", \"Douglas\"], [\"I\", \"Victor\"], [\"I\", \"George\"], [\"I\", \"Hopkins\"], [\"I\", \"Ferguson\"], [\"Stangerson\", \"Brother\"], [\"I\", \"Hilda\"], [\"I\", \"Turner\"], [\"I\", \"Gibson\"], [\"I\", \"Beppo\"], [\"I\", \"Windibank\"], [\"I\", \"Jefferson\"], [\"I\", \"Gilchrist\"], [\"I\", \"Stanley\"], [\"I\", \"Munro\"], [\"I\", \"Brother\"], [\"I\", \"McMurdo\"], [\"I\", \"Melas\"], [\"I\", \"Garrideb\"], [\"I\", \"Athelney\"], [\"I\", \"Trevelyan\"], [\"Armstrong\", \"Godfrey\"], [\"Watson\", \"don\"], [\"Watson\", \"Hudson\"], [\"Cadogan\", \"West\"], [\"Watson\", \"Moriarty\"], [\"Baker\", \"Henry\"], [\"Watson\", \"Sherlock\"], [\"Baker\", \"Sherlock\"], [\"Watson\", \"Huxtable\"], [\"Smith\", \"Sherlock\"], [\"Watson\", \"Don\"], [\"Baker\", \"Hope\"], [\"Watson\", \"Trevor\"], [\"Armstrong\", \"Staunton\"], [\"Watson\", \"Musgrave\"], [\"Garcia\", \"Scott\"], [\"Garcia\", \"Eccles\"], [\"Baker\", \"Watson\"], [\"Watson\", \"Baker\"], [\"Watson\", \"Smith\"], [\"Stoner\", \"Roylott\"], [\"Scanlan\", \"McMurdo\"], [\"Baker\", \"Jefferson\"], [\"Scanlan\", \"Brother\"], [\"Simpson\", \"Straker\"], [\"Hilton\", \"Slaney\"], [\"Carey\", \"Lee\"], [\"Hall\", \"Baskerville\"], [\"Neville\", \"Clair\"], [\"Neville\", \"Lee\"], [\"Barrymore\", \"Baskerville\"], [\"Simpson\", \"Fitzroy\"], [\"Roylott\", \"Moran\"], [\"Thaddeus\", \"Sholto\"], [\"William\", \"Morris\"], [\"Frank\", \"Simon\"], [\"Barrymore\", \"Henry\"], [\"William\", \"I\"], [\"Toller\", \"I\"], [\"William\", \"Cunningham\"], [\"Oberstein\", \"Hugo\"], [\"Barclay\", \"Morrison\"], [\"William\", \"Alec\"], [\"Hall\", \"Barrymore\"], [\"McFarlane\", \"Oldacre\"], [\"Baldwin\", \"Brother\"], [\"Baldwin\", \"McMurdo\"], [\"Hilton\", \"Cubitt\"], [\"Sarah\", \"don\"], [\"Turner\", \"McCarthy\"], [\"McMurdo\", \"don\"], [\"McMurdo\", \"Jack\"], [\"Gregory\", \"Ross\"], [\"Jefferson\", \"Ferrier\"], [\"Jefferson\", \"Lucy\"], [\"Williamson\", \"Carruthers\"], [\"Sarah\", \"Mary\"], [\"Trevelyan\", \"Blessington\"], [\"Sarah\", \"Cushing\"], [\"Gilchrist\", \"Jabez\"], [\"Jefferson\", \"Hope\"], [\"Brother\", \"Bartholomew\"], [\"Sarah\", \"I\"], [\"Williamson\", \"Woodley\"], [\"Williamson\", \"Hall\"], [\"McMurdo\", \"Brother\"], [\"Stanley\", \"Hopkins\"]], \"ipython\": true, \"error_msg\": \"\"}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# adding a function to clean the graph as in some cases the Stanford NER maps 'I' as a person.\n",
      "def clean_graph(g, remove_entities_set):\n",
      "    vertices = g.vertices[g.vertices[\"__id\"].apply(lambda v: v not in remove_entities_set)] \n",
      "    edges = g.edges[g.edges.apply(lambda e: e[\"__src_id\"] not in remove_entities_set and e[\"__dst_id\"] not in remove_entities_set)]\n",
      "    return gl.SGraph(vertices, edges)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": true
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#cleaning the graph and displaying it again\n",
      "g = clean_graph(g, {\"I\"})\n",
      "g.show(vlabel=\"__id\", elabel=\"strength\", node_size=200)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sgraph','View', {\"edges_labels\": [4, 8, 5, 4, 3, 16, 8, 8, 4, 3, 12, 8, 6, 17, 4, 6, 4, 3, 3, 4, 3, 27, 3, 15, 5, 3, 6, 19, 13, 4, 5, 3, 3, 6, 21, 9, 6, 48, 3, 4, 11, 5, 6, 15, 4, 3, 6, 6, 4, 4, 4, 3, 17, 4, 16, 4, 4, 12, 3, 3, 4, 5, 18, 4, 18, 70, 42, 14, 4, 36, 4, 8, 22, 32, 11, 6, 3, 14, 10, 3, 4, 6, 18, 4, 6, 26, 6, 3, 4, 4, 4, 4, 4, 38, 10, 3, 3, 10, 18, 17, 4, 18, 36, 18, 6, 20, 26, 3, 4, 18, 4, 34, 6, 19, 14, 9, 4, 8, 4, 3, 3, 6, 7, 6, 6, 3, 7, 3, 4, 7, 3, 3, 8, 4, 4, 30, 28, 6, 3, 14, 4, 21, 4, 4, 4, 4, 4, 6, 6, 5, 8, 4, 4, 42, 6, 6, 3, 5, 4, 5, 3, 8, 8, 4, 8, 4, 37, 5, 8, 4, 10, 56], \"selected_variable\": {\"name\": [\"g\"], \"view_file\": \"sgraph\", \"view_component\": \"View\", \"view_params\": {\"elabel_hover\": false, \"vertex_positions\": null, \"h_offset\": 0.0, \"node_size\": 200, \"ecolor\": [0.37, 0.33, 0.33], \"elabel\": \"strength\", \"arrows\": false, \"ewidth\": 1, \"vlabel\": \"__id\", \"highlight_color\": [0.69, 0.0, 0.498], \"vcolor\": [0.522, 0.741, 0.0], \"vlabel_hover\": false, \"highlight\": {}, \"v_offset\": 0.03}, \"view_components\": [\"View\"], \"type\": \"SGraph\", \"descriptives_links\": {\"edges\": \"edges\", \"vertices\": \"vertices\"}, \"descriptives\": {\"edges\": 172, \"vertices\": 183}}, \"positions\": null, \"error_type\": 0, \"vertices\": [\"Straker\", \"Slaney\", \"Robert\", \"Sutherland\", \"Mycroft\", \"McGinty\", \"Joseph\", \"Godfrey\", \"McCarthy\", \"MacDonald\", \"Coombe\", \"Lyons\", \"Adair\", \"Jones\", \"don\", \"White\", \"Jack\", \"God\", \"Angel\", \"Barker\", \"Harrison\", \"Wilder\", \"Sholto\", \"Ross\", \"Peters\", \"Moran\", \"Lestrade\", \"Jim\", \"Fitzroy\", \"Milverton\", \"McPherson\", \"Charles\", \"Ettie\", \"Tregennis\", \"Elsie\", \"Frances\", \"Clair\", \"Black\", \"Adler\", \"Baskerville\", \"Lee\", \"Baynes\", \"Baron\", \"Wilson\", \"Von\", \"Tracey\", \"Theresa\", \"Sterndale\", \"Small\", \"Pinner\", \"Peter\", \"Napoleon\", \"Moriarty\", \"Merryweather\", \"Morris\", \"Mary\", \"Ferrier\", \"Martin\", \"Warren\", \"Lucy\", \"Carruthers\", \"Hudson\", \"Harker\", \"West\", \"Stapleton\", \"Bannister\", \"Willoughby\", \"Trevor\", \"Simon\", \"Sherlock\", \"Billy\", \"Percy\", \"Overton\", \"Johnson\", \"Jabez\", \"Hope\", \"Brackenstall\", \"Phelps\", \"Henry\", \"Blessington\", \"Drebber\", \"Don\", \"Jonas\", \"Huxtable\", \"Cushing\", \"Susan\", \"Cyril\", \"Jove\", \"Cecil\", \"Mortimer\", \"Soames\", \"Jonathan\", \"Gorgiano\", \"Irene\", \"Hosmer\", \"Alice\", \"Woodley\", \"Staunton\", \"Scott\", \"Ralph\", \"Stangerson\", \"Prendergast\", \"Peterson\", \"Musgrave\", \"Gennaro\", \"Hugo\", \"Gregson\", \"Cunningham\", \"Murdoch\", \"Culverton\", \"Bartholomew\", \"Bennett\", \"Hunter\", \"Lucas\", \"Pycroft\", \"Acton\", \"Stoner\", \"Scanlan\", \"Evans\", \"Mason\", \"Garcia\", \"Henderson\", \"Watson\", \"Eustace\", \"Openshaw\", \"Morrison\", \"Brown\", \"Toby\", \"Baker\", \"Eccles\", \"Armstrong\", \"Smith\", \"Cadogan\", \"Alec\", \"William\", \"Toller\", \"Simpson\", \"Oberstein\", \"McFarlane\", \"Hilton\", \"Thaddeus\", \"Hayes\", \"Rucastle\", \"Oldacre\", \"Barclay\", \"Baldwin\", \"Mawson\", \"Gloria\", \"Roylott\", \"Frank\", \"Bork\", \"Carey\", \"Brunton\", \"Hall\", \"Doran\", \"Browner\", \"Boss\", \"Barrymore\", \"Neville\", \"Williamson\", \"Victor\", \"Stanley\", \"Munro\", \"Melas\", \"Gregory\", \"McMurdo\", \"Cubitt\", \"Turner\", \"Trevelyan\", \"Jefferson\", \"Windibank\", \"Brother\", \"George\", \"Ferguson\", \"Hilda\", \"Athelney\", \"Douglas\", \"Sarah\", \"Gilchrist\", \"Gibson\", \"Garrideb\", \"Hopkins\", \"Beppo\"], \"vertices_labels\": [\"Straker\", \"Slaney\", \"Robert\", \"Sutherland\", \"Mycroft\", \"McGinty\", \"Joseph\", \"Godfrey\", \"McCarthy\", \"MacDonald\", \"Coombe\", \"Lyons\", \"Adair\", \"Jones\", \"don\", \"White\", \"Jack\", \"God\", \"Angel\", \"Barker\", \"Harrison\", \"Wilder\", \"Sholto\", \"Ross\", \"Peters\", \"Moran\", \"Lestrade\", \"Jim\", \"Fitzroy\", \"Milverton\", \"McPherson\", \"Charles\", \"Ettie\", \"Tregennis\", \"Elsie\", \"Frances\", \"Clair\", \"Black\", \"Adler\", \"Baskerville\", \"Lee\", \"Baynes\", \"Baron\", \"Wilson\", \"Von\", \"Tracey\", \"Theresa\", \"Sterndale\", \"Small\", \"Pinner\", \"Peter\", \"Napoleon\", \"Moriarty\", \"Merryweather\", \"Morris\", \"Mary\", \"Ferrier\", \"Martin\", \"Warren\", \"Lucy\", \"Carruthers\", \"Hudson\", \"Harker\", \"West\", \"Stapleton\", \"Bannister\", \"Willoughby\", \"Trevor\", \"Simon\", \"Sherlock\", \"Billy\", \"Percy\", \"Overton\", \"Johnson\", \"Jabez\", \"Hope\", \"Brackenstall\", \"Phelps\", \"Henry\", \"Blessington\", \"Drebber\", \"Don\", \"Jonas\", \"Huxtable\", \"Cushing\", \"Susan\", \"Cyril\", \"Jove\", \"Cecil\", \"Mortimer\", \"Soames\", \"Jonathan\", \"Gorgiano\", \"Irene\", \"Hosmer\", \"Alice\", \"Woodley\", \"Staunton\", \"Scott\", \"Ralph\", \"Stangerson\", \"Prendergast\", \"Peterson\", \"Musgrave\", \"Gennaro\", \"Hugo\", \"Gregson\", \"Cunningham\", \"Murdoch\", \"Culverton\", \"Bartholomew\", \"Bennett\", \"Hunter\", \"Lucas\", \"Pycroft\", \"Acton\", \"Stoner\", \"Scanlan\", \"Evans\", \"Mason\", \"Garcia\", \"Henderson\", \"Watson\", \"Eustace\", \"Openshaw\", \"Morrison\", \"Brown\", \"Toby\", \"Baker\", \"Eccles\", \"Armstrong\", \"Smith\", \"Cadogan\", \"Alec\", \"William\", \"Toller\", \"Simpson\", \"Oberstein\", \"McFarlane\", \"Hilton\", \"Thaddeus\", \"Hayes\", \"Rucastle\", \"Oldacre\", \"Barclay\", \"Baldwin\", \"Mawson\", \"Gloria\", \"Roylott\", \"Frank\", \"Bork\", \"Carey\", \"Brunton\", \"Hall\", \"Doran\", \"Browner\", \"Boss\", \"Barrymore\", \"Neville\", \"Williamson\", \"Victor\", \"Stanley\", \"Munro\", \"Melas\", \"Gregory\", \"McMurdo\", \"Cubitt\", \"Turner\", \"Trevelyan\", \"Jefferson\", \"Windibank\", \"Brother\", \"George\", \"Ferguson\", \"Hilda\", \"Athelney\", \"Douglas\", \"Sarah\", \"Gilchrist\", \"Gibson\", \"Garrideb\", \"Hopkins\", \"Beppo\"], \"edges\": [[\"MacDonald\", \"White\"], [\"God\", \"Jack\"], [\"McGinty\", \"Jack\"], [\"Lyons\", \"Coombe\"], [\"God\", \"don\"], [\"Joseph\", \"Harrison\"], [\"don\", \"Jack\"], [\"Adair\", \"Moran\"], [\"Lyons\", \"Tracey\"], [\"Jones\", \"Small\"], [\"Sutherland\", \"Mary\"], [\"Robert\", \"Simon\"], [\"Mycroft\", \"Sherlock\"], [\"Barker\", \"Cecil\"], [\"Joseph\", \"Drebber\"], [\"don\", \"Sherlock\"], [\"don\", \"Henry\"], [\"God\", \"Sherlock\"], [\"Joseph\", \"Smith\"], [\"MacDonald\", \"Mason\"], [\"God\", \"Watson\"], [\"White\", \"Mason\"], [\"Jones\", \"Watson\"], [\"McGinty\", \"Boss\"], [\"Robert\", \"Ferguson\"], [\"McGinty\", \"Brother\"], [\"Mycroft\", \"Brother\"], [\"Jones\", \"Athelney\"], [\"Barker\", \"Douglas\"], [\"Slaney\", \"Cubitt\"], [\"McGinty\", \"McMurdo\"], [\"Lestrade\", \"don\"], [\"Black\", \"Jack\"], [\"Lestrade\", \"Mycroft\"], [\"Charles\", \"Baskerville\"], [\"Milverton\", \"Charles\"], [\"Lee\", \"Clair\"], [\"Tregennis\", \"Mortimer\"], [\"Charles\", \"Mortimer\"], [\"Adler\", \"Sherlock\"], [\"Lestrade\", \"Sherlock\"], [\"Baskerville\", \"Henry\"], [\"Sholto\", \"Bartholomew\"], [\"Lestrade\", \"Gregson\"], [\"Milverton\", \"Watson\"], [\"Baskerville\", \"Hall\"], [\"Charles\", \"Hall\"], [\"Jim\", \"Browner\"], [\"Moran\", \"Roylott\"], [\"Charles\", \"Barrymore\"], [\"Hudson\", \"Mycroft\"], [\"Hudson\", \"don\"], [\"Tracey\", \"Coombe\"], [\"Peter\", \"Lee\"], [\"Peter\", \"Black\"], [\"Von\", \"Baron\"], [\"Hudson\", \"Napoleon\"], [\"Lucy\", \"Ferrier\"], [\"Hudson\", \"Sherlock\"], [\"Lucy\", \"Hope\"], [\"Moriarty\", \"Sherlock\"], [\"Ferrier\", \"Hope\"], [\"Wilson\", \"Jabez\"], [\"Bannister\", \"Soames\"], [\"Small\", \"Jonathan\"], [\"Von\", \"Bork\"], [\"Peter\", \"Carey\"], [\"Morris\", \"Brother\"], [\"Morris\", \"McMurdo\"], [\"Hosmer\", \"Angel\"], [\"Huxtable\", \"Wilder\"], [\"Mortimer\", \"Baskerville\"], [\"Henry\", \"Baskerville\"], [\"Irene\", \"Adler\"], [\"Henry\", \"Stapleton\"], [\"Henry\", \"Mortimer\"], [\"Mortimer\", \"Henry\"], [\"Percy\", \"Phelps\"], [\"Cyril\", \"Overton\"], [\"Drebber\", \"Hope\"], [\"Irene\", \"Sherlock\"], [\"Brackenstall\", \"Eustace\"], [\"Willoughby\", \"Smith\"], [\"Alice\", \"Rucastle\"], [\"Simon\", \"Doran\"], [\"Jonas\", \"Oldacre\"], [\"Jonas\", \"McFarlane\"], [\"Henry\", \"Hall\"], [\"Hope\", \"Hilda\"], [\"Percy\", \"Trevelyan\"], [\"Trevor\", \"Victor\"], [\"Sherlock\", \"Hopkins\"], [\"Woodley\", \"Jack\"], [\"Staunton\", \"Godfrey\"], [\"Stangerson\", \"Joseph\"], [\"Bartholomew\", \"Sholto\"], [\"Stangerson\", \"Lestrade\"], [\"Hugo\", \"Baskerville\"], [\"Woodley\", \"Carruthers\"], [\"Stangerson\", \"Drebber\"], [\"Acton\", \"Cunningham\"], [\"Culverton\", \"Smith\"], [\"Scott\", \"Eccles\"], [\"Cunningham\", \"Alec\"], [\"Ralph\", \"Smith\"], [\"Scott\", \"Gloria\"], [\"Pycroft\", \"Hall\"], [\"Stangerson\", \"Brother\"], [\"Armstrong\", \"Godfrey\"], [\"Watson\", \"don\"], [\"Watson\", \"Hudson\"], [\"Cadogan\", \"West\"], [\"Watson\", \"Moriarty\"], [\"Baker\", \"Henry\"], [\"Watson\", \"Sherlock\"], [\"Baker\", \"Sherlock\"], [\"Watson\", \"Huxtable\"], [\"Smith\", \"Sherlock\"], [\"Watson\", \"Don\"], [\"Baker\", \"Hope\"], [\"Watson\", \"Trevor\"], [\"Armstrong\", \"Staunton\"], [\"Watson\", \"Musgrave\"], [\"Garcia\", \"Scott\"], [\"Garcia\", \"Eccles\"], [\"Baker\", \"Watson\"], [\"Watson\", \"Baker\"], [\"Watson\", \"Smith\"], [\"Stoner\", \"Roylott\"], [\"Scanlan\", \"McMurdo\"], [\"Baker\", \"Jefferson\"], [\"Scanlan\", \"Brother\"], [\"Simpson\", \"Straker\"], [\"Hilton\", \"Slaney\"], [\"Carey\", \"Lee\"], [\"Hall\", \"Baskerville\"], [\"Neville\", \"Clair\"], [\"Neville\", \"Lee\"], [\"Barrymore\", \"Baskerville\"], [\"Simpson\", \"Fitzroy\"], [\"Roylott\", \"Moran\"], [\"Thaddeus\", \"Sholto\"], [\"William\", \"Morris\"], [\"Frank\", \"Simon\"], [\"Barrymore\", \"Henry\"], [\"William\", \"Cunningham\"], [\"Oberstein\", \"Hugo\"], [\"Barclay\", \"Morrison\"], [\"William\", \"Alec\"], [\"Hall\", \"Barrymore\"], [\"McFarlane\", \"Oldacre\"], [\"Baldwin\", \"Brother\"], [\"Baldwin\", \"McMurdo\"], [\"Hilton\", \"Cubitt\"], [\"Sarah\", \"don\"], [\"Turner\", \"McCarthy\"], [\"McMurdo\", \"don\"], [\"McMurdo\", \"Jack\"], [\"Gregory\", \"Ross\"], [\"Jefferson\", \"Ferrier\"], [\"Jefferson\", \"Lucy\"], [\"Williamson\", \"Carruthers\"], [\"Sarah\", \"Mary\"], [\"Trevelyan\", \"Blessington\"], [\"Sarah\", \"Cushing\"], [\"Gilchrist\", \"Jabez\"], [\"Jefferson\", \"Hope\"], [\"Brother\", \"Bartholomew\"], [\"Williamson\", \"Woodley\"], [\"Williamson\", \"Hall\"], [\"McMurdo\", \"Brother\"], [\"Stanley\", \"Hopkins\"]], \"ipython\": true, \"error_msg\": \"\"}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The NER algorithm did pretty good job, and most of the names of the identified entities looks logical (at least to me).\n",
      "Additionally, we can understand the link between the various book characters. We can also notice that in many of the graph's components that have only two vertices  the connection is between each characfter first and it's last names. Let use GraphLab [graph_analytics toolkit](https://dato.com/products/create/docs/graphlab.toolkits.graph_analytics.html) and focus on the social network's largest component."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_graph_largest_compnent(g):\n",
      "    \"\"\"\n",
      "    Returns a graph with the largest component of the input graph\n",
      "    :param g: input graph (SGraph object)\n",
      "    :return: a graph of the largest component in the input object\n",
      "    :rtype: gl.SGraph\n",
      "    \"\"\"\n",
      "    \n",
      "    cc = gl.connected_components.create(g)    \n",
      "    #add each vertices its component id\n",
      "    g.vertices['component_id'] = cc['graph'].vertices['component_id']\n",
      "    # calculate the component id of the largest component\n",
      "    largest_component_id = cc['component_size'].sort('Count', ascending=False)[0]['component_id']\n",
      "    largest_component_verticies = g.vertices.filter_by(largest_component_id, 'component_id')['__id']\n",
      "    h = g.get_neighborhood(largest_component_verticies, 1)\n",
      "    return h\n",
      "\n",
      "h = get_graph_largest_compnent(g)  \n",
      "h.show(vlabel=\"__id\", elabel=\"strength\", node_size=300)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PROGRESS: +-----------------------------+\n",
        "PROGRESS: | Number of components merged |\n",
        "PROGRESS: +-----------------------------+\n",
        "PROGRESS: | 166                         |\n",
        "PROGRESS: | 0                           |\n",
        "PROGRESS: +-----------------------------+\n"
       ]
      },
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sgraph','View', {\"edges_labels\": [8, 5, 3, 16, 8, 3, 12, 6, 6, 4, 3, 4, 3, 3, 3, 15, 3, 19, 6, 5, 3, 3, 6, 21, 6, 9, 11, 4, 3, 5, 48, 15, 6, 4, 6, 3, 4, 3, 4, 16, 4, 12, 4, 18, 3, 4, 3, 5, 42, 14, 4, 8, 22, 4, 32, 11, 3, 4, 3, 6, 18, 3, 4, 4, 4, 10, 4, 3, 3, 10, 18, 17, 4, 18, 6, 18, 26, 3, 18, 6, 4, 14, 9, 4, 3, 3, 8, 4, 19, 7, 7, 3, 3, 7, 3, 3, 28, 4, 6, 21, 30, 3, 4, 4, 4, 4, 6, 5, 4, 4, 3, 5, 6, 8, 5, 8, 3, 8, 37, 8, 5, 4, 56, 10], \"selected_variable\": {\"name\": [\"h\"], \"view_file\": \"sgraph\", \"view_component\": \"View\", \"view_params\": {\"elabel_hover\": false, \"vertex_positions\": null, \"h_offset\": 0.0, \"node_size\": 300, \"ecolor\": [0.37, 0.33, 0.33], \"elabel\": \"strength\", \"arrows\": false, \"ewidth\": 1, \"vlabel\": \"__id\", \"highlight_color\": [0.69, 0.0, 0.498], \"vcolor\": [0.522, 0.741, 0.0], \"vlabel_hover\": false, \"highlight\": {}, \"v_offset\": 0.03}, \"view_components\": [\"View\"], \"type\": \"SGraph\", \"descriptives_links\": {\"edges\": \"edges\", \"vertices\": \"vertices\"}, \"descriptives\": {\"edges\": 124, \"vertices\": 78}}, \"positions\": null, \"error_type\": 0, \"vertices\": [\"Harrison\", \"Jack\", \"don\", \"God\", \"Mycroft\", \"Jones\", \"Joseph\", \"McGinty\", \"Sutherland\", \"Lee\", \"Black\", \"Clair\", \"Tregennis\", \"Milverton\", \"Charles\", \"Baskerville\", \"Adler\", \"Lestrade\", \"Sholto\", \"Wilder\", \"Stapleton\", \"Hudson\", \"Carruthers\", \"Lucy\", \"Ferrier\", \"Napoleon\", \"Mary\", \"Morris\", \"Peter\", \"Moriarty\", \"Small\", \"Irene\", \"Jonathan\", \"Mortimer\", \"Cushing\", \"Huxtable\", \"Henry\", \"Don\", \"Drebber\", \"Hope\", \"Sherlock\", \"Trevor\", \"Willoughby\", \"Acton\", \"Bartholomew\", \"Culverton\", \"Cunningham\", \"Hugo\", \"Gregson\", \"Stangerson\", \"Musgrave\", \"Ralph\", \"Pycroft\", \"Woodley\", \"Alec\", \"Smith\", \"Baker\", \"Watson\", \"Scanlan\", \"Neville\", \"Thaddeus\", \"Carey\", \"Barrymore\", \"Boss\", \"Hall\", \"Baldwin\", \"Oberstein\", \"William\", \"Hopkins\", \"Sarah\", \"Athelney\", \"Jefferson\", \"Williamson\", \"McMurdo\", \"Stanley\", \"Hilda\", \"Brother\", \"Victor\"], \"vertices_labels\": [\"Harrison\", \"Jack\", \"don\", \"God\", \"Mycroft\", \"Jones\", \"Joseph\", \"McGinty\", \"Sutherland\", \"Lee\", \"Black\", \"Clair\", \"Tregennis\", \"Milverton\", \"Charles\", \"Baskerville\", \"Adler\", \"Lestrade\", \"Sholto\", \"Wilder\", \"Stapleton\", \"Hudson\", \"Carruthers\", \"Lucy\", \"Ferrier\", \"Napoleon\", \"Mary\", \"Morris\", \"Peter\", \"Moriarty\", \"Small\", \"Irene\", \"Jonathan\", \"Mortimer\", \"Cushing\", \"Huxtable\", \"Henry\", \"Don\", \"Drebber\", \"Hope\", \"Sherlock\", \"Trevor\", \"Willoughby\", \"Acton\", \"Bartholomew\", \"Culverton\", \"Cunningham\", \"Hugo\", \"Gregson\", \"Stangerson\", \"Musgrave\", \"Ralph\", \"Pycroft\", \"Woodley\", \"Alec\", \"Smith\", \"Baker\", \"Watson\", \"Scanlan\", \"Neville\", \"Thaddeus\", \"Carey\", \"Barrymore\", \"Boss\", \"Hall\", \"Baldwin\", \"Oberstein\", \"William\", \"Hopkins\", \"Sarah\", \"Athelney\", \"Jefferson\", \"Williamson\", \"McMurdo\", \"Stanley\", \"Hilda\", \"Brother\", \"Victor\"], \"edges\": [[\"God\", \"Jack\"], [\"McGinty\", \"Jack\"], [\"God\", \"don\"], [\"Joseph\", \"Harrison\"], [\"don\", \"Jack\"], [\"Jones\", \"Small\"], [\"Sutherland\", \"Mary\"], [\"Mycroft\", \"Sherlock\"], [\"don\", \"Sherlock\"], [\"Joseph\", \"Drebber\"], [\"God\", \"Sherlock\"], [\"don\", \"Henry\"], [\"Jones\", \"Watson\"], [\"Joseph\", \"Smith\"], [\"God\", \"Watson\"], [\"McGinty\", \"Boss\"], [\"McGinty\", \"Brother\"], [\"Jones\", \"Athelney\"], [\"Mycroft\", \"Brother\"], [\"McGinty\", \"McMurdo\"], [\"Lestrade\", \"don\"], [\"Black\", \"Jack\"], [\"Lestrade\", \"Mycroft\"], [\"Charles\", \"Baskerville\"], [\"Lee\", \"Clair\"], [\"Milverton\", \"Charles\"], [\"Lestrade\", \"Sherlock\"], [\"Adler\", \"Sherlock\"], [\"Charles\", \"Mortimer\"], [\"Baskerville\", \"Henry\"], [\"Tregennis\", \"Mortimer\"], [\"Lestrade\", \"Gregson\"], [\"Sholto\", \"Bartholomew\"], [\"Milverton\", \"Watson\"], [\"Charles\", \"Hall\"], [\"Baskerville\", \"Hall\"], [\"Charles\", \"Barrymore\"], [\"Hudson\", \"don\"], [\"Hudson\", \"Mycroft\"], [\"Peter\", \"Black\"], [\"Peter\", \"Lee\"], [\"Lucy\", \"Ferrier\"], [\"Hudson\", \"Napoleon\"], [\"Small\", \"Jonathan\"], [\"Hudson\", \"Sherlock\"], [\"Moriarty\", \"Sherlock\"], [\"Lucy\", \"Hope\"], [\"Ferrier\", \"Hope\"], [\"Peter\", \"Carey\"], [\"Morris\", \"Brother\"], [\"Morris\", \"McMurdo\"], [\"Mortimer\", \"Baskerville\"], [\"Henry\", \"Baskerville\"], [\"Huxtable\", \"Wilder\"], [\"Irene\", \"Adler\"], [\"Henry\", \"Stapleton\"], [\"Mortimer\", \"Henry\"], [\"Irene\", \"Sherlock\"], [\"Drebber\", \"Hope\"], [\"Henry\", \"Mortimer\"], [\"Willoughby\", \"Smith\"], [\"Henry\", \"Hall\"], [\"Sherlock\", \"Hopkins\"], [\"Hope\", \"Hilda\"], [\"Trevor\", \"Victor\"], [\"Stangerson\", \"Joseph\"], [\"Woodley\", \"Jack\"], [\"Stangerson\", \"Lestrade\"], [\"Bartholomew\", \"Sholto\"], [\"Hugo\", \"Baskerville\"], [\"Woodley\", \"Carruthers\"], [\"Stangerson\", \"Drebber\"], [\"Acton\", \"Cunningham\"], [\"Culverton\", \"Smith\"], [\"Ralph\", \"Smith\"], [\"Cunningham\", \"Alec\"], [\"Pycroft\", \"Hall\"], [\"Stangerson\", \"Brother\"], [\"Watson\", \"don\"], [\"Watson\", \"Moriarty\"], [\"Watson\", \"Hudson\"], [\"Watson\", \"Sherlock\"], [\"Baker\", \"Sherlock\"], [\"Watson\", \"Don\"], [\"Watson\", \"Trevor\"], [\"Baker\", \"Hope\"], [\"Smith\", \"Sherlock\"], [\"Watson\", \"Huxtable\"], [\"Baker\", \"Henry\"], [\"Watson\", \"Musgrave\"], [\"Watson\", \"Baker\"], [\"Watson\", \"Smith\"], [\"Baker\", \"Watson\"], [\"Scanlan\", \"McMurdo\"], [\"Scanlan\", \"Brother\"], [\"Baker\", \"Jefferson\"], [\"Neville\", \"Clair\"], [\"Carey\", \"Lee\"], [\"Neville\", \"Lee\"], [\"Thaddeus\", \"Sholto\"], [\"Hall\", \"Baskerville\"], [\"Barrymore\", \"Baskerville\"], [\"William\", \"Morris\"], [\"Barrymore\", \"Henry\"], [\"Oberstein\", \"Hugo\"], [\"William\", \"Cunningham\"], [\"William\", \"Alec\"], [\"Hall\", \"Barrymore\"], [\"Baldwin\", \"Brother\"], [\"Baldwin\", \"McMurdo\"], [\"McMurdo\", \"don\"], [\"McMurdo\", \"Jack\"], [\"Sarah\", \"don\"], [\"Sarah\", \"Mary\"], [\"Jefferson\", \"Ferrier\"], [\"Williamson\", \"Carruthers\"], [\"Jefferson\", \"Lucy\"], [\"Sarah\", \"Cushing\"], [\"Jefferson\", \"Hope\"], [\"Williamson\", \"Woodley\"], [\"Brother\", \"Bartholomew\"], [\"Williamson\", \"Hall\"], [\"Stanley\", \"Hopkins\"], [\"McMurdo\", \"Brother\"]], \"ipython\": true, \"error_msg\": \"\"}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "h = clean_graph(g, {\"I\"})\n",
      "h.show(vlabel=\"__id\", elabel=\"strength\", node_size=300)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "application/javascript": [
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.1.0/css/font-awesome.min.css\"\n",
         "}));\n",
         "$(\"head\").append($(\"<link/>\").attr({\n",
         "  rel:  \"stylesheet\",\n",
         "  type: \"text/css\",\n",
         "  href: \"//dato.com/files/canvas/1.8/css/canvas.css\"\n",
         "}));\n",
         "\n",
         "            (function(){\n",
         "\n",
         "                var e = null;\n",
         "                if (typeof element == 'undefined') {\n",
         "                    var scripts = document.getElementsByTagName('script');\n",
         "                    var thisScriptTag = scripts[scripts.length-1];\n",
         "                    var parentDiv = thisScriptTag.parentNode;\n",
         "                    e = document.createElement('div');\n",
         "                    parentDiv.appendChild(e);\n",
         "                } else {\n",
         "                    e = element[0];\n",
         "                }\n",
         "\n",
         "                if (typeof requirejs !== 'undefined') {\n",
         "                    // disable load timeout; ipython_app.js is large and can take a while to load.\n",
         "                    requirejs.config({waitSeconds: 0});\n",
         "                }\n",
         "\n",
         "                require(['//dato.com/files/canvas/1.8/js/ipython_app.js'], function(IPythonApp){\n",
         "                    var app = new IPythonApp();\n",
         "                    app.attachView('sgraph','View', {\"edges_labels\": [4, 8, 5, 4, 3, 16, 8, 8, 4, 3, 12, 8, 6, 17, 4, 6, 4, 3, 3, 4, 3, 27, 3, 15, 5, 3, 6, 19, 13, 4, 5, 3, 3, 6, 21, 9, 6, 48, 3, 4, 11, 5, 6, 15, 4, 3, 6, 6, 4, 4, 4, 3, 17, 4, 16, 4, 4, 12, 3, 3, 4, 5, 18, 4, 18, 70, 42, 14, 4, 36, 4, 8, 22, 32, 11, 6, 3, 14, 10, 3, 4, 6, 18, 4, 6, 26, 6, 3, 4, 4, 4, 4, 4, 38, 10, 3, 3, 10, 18, 17, 4, 18, 36, 18, 6, 20, 26, 3, 4, 18, 4, 34, 6, 19, 14, 9, 4, 8, 4, 3, 3, 6, 7, 6, 6, 3, 7, 3, 4, 7, 3, 3, 8, 4, 4, 30, 28, 6, 3, 14, 4, 21, 4, 4, 4, 4, 4, 6, 6, 5, 8, 4, 4, 42, 6, 6, 3, 5, 4, 5, 3, 8, 8, 4, 8, 4, 37, 5, 8, 4, 10, 56], \"selected_variable\": {\"name\": [\"h\"], \"view_file\": \"sgraph\", \"view_component\": \"View\", \"view_params\": {\"elabel_hover\": false, \"vertex_positions\": null, \"h_offset\": 0.0, \"node_size\": 300, \"ecolor\": [0.37, 0.33, 0.33], \"elabel\": \"strength\", \"arrows\": false, \"ewidth\": 1, \"vlabel\": \"__id\", \"highlight_color\": [0.69, 0.0, 0.498], \"vcolor\": [0.522, 0.741, 0.0], \"vlabel_hover\": false, \"highlight\": {}, \"v_offset\": 0.03}, \"view_components\": [\"View\"], \"type\": \"SGraph\", \"descriptives_links\": {\"edges\": \"edges\", \"vertices\": \"vertices\"}, \"descriptives\": {\"edges\": 172, \"vertices\": 183}}, \"positions\": null, \"error_type\": 0, \"vertices\": [\"Harrison\", \"Barker\", \"Angel\", \"God\", \"Jack\", \"White\", \"don\", \"Jones\", \"Adair\", \"Lyons\", \"MacDonald\", \"McCarthy\", \"Coombe\", \"Godfrey\", \"Joseph\", \"McGinty\", \"Robert\", \"Mycroft\", \"Sutherland\", \"Slaney\", \"Straker\", \"Baron\", \"Baynes\", \"Lee\", \"Baskerville\", \"Adler\", \"Black\", \"Clair\", \"Frances\", \"Elsie\", \"Tregennis\", \"Fitzroy\", \"Jim\", \"Lestrade\", \"Ettie\", \"McPherson\", \"Charles\", \"Milverton\", \"Moran\", \"Peters\", \"Ross\", \"Sholto\", \"Wilder\", \"Von\", \"Wilson\", \"Bannister\", \"Stapleton\", \"West\", \"Harker\", \"Hudson\", \"Carruthers\", \"Lucy\", \"Warren\", \"Martin\", \"Ferrier\", \"Moriarty\", \"Mary\", \"Morris\", \"Peter\", \"Small\", \"Sterndale\", \"Theresa\", \"Napoleon\", \"Pinner\", \"Merryweather\", \"Tracey\", \"Alice\", \"Hosmer\", \"Irene\", \"Gorgiano\", \"Jonathan\", \"Soames\", \"Mortimer\", \"Cyril\", \"Susan\", \"Cushing\", \"Huxtable\", \"Jonas\", \"Don\", \"Blessington\", \"Drebber\", \"Cecil\", \"Jove\", \"Henry\", \"Phelps\", \"Simon\", \"Trevor\", \"Johnson\", \"Hope\", \"Billy\", \"Percy\", \"Brackenstall\", \"Overton\", \"Sherlock\", \"Jabez\", \"Willoughby\", \"Acton\", \"Pycroft\", \"Lucas\", \"Hunter\", \"Bennett\", \"Bartholomew\", \"Culverton\", \"Murdoch\", \"Cunningham\", \"Hugo\", \"Gennaro\", \"Gregson\", \"Stangerson\", \"Prendergast\", \"Scott\", \"Ralph\", \"Musgrave\", \"Peterson\", \"Staunton\", \"Woodley\", \"Alec\", \"Cadogan\", \"Smith\", \"Armstrong\", \"Eccles\", \"Baker\", \"Toby\", \"Openshaw\", \"Morrison\", \"Brown\", \"Watson\", \"Eustace\", \"Henderson\", \"Scanlan\", \"Garcia\", \"Mason\", \"Evans\", \"Stoner\", \"Neville\", \"Barrymore\", \"Boss\", \"Browner\", \"Brunton\", \"Carey\", \"Bork\", \"Frank\", \"Roylott\", \"Mawson\", \"Baldwin\", \"Barclay\", \"Gloria\", \"Oldacre\", \"Rucastle\", \"McFarlane\", \"Hilton\", \"Doran\", \"Hall\", \"Toller\", \"Oberstein\", \"William\", \"Hayes\", \"Thaddeus\", \"Simpson\", \"Beppo\", \"Gilchrist\", \"Gibson\", \"Sarah\", \"Douglas\", \"Hilda\", \"Athelney\", \"Garrideb\", \"George\", \"Brother\", \"Windibank\", \"Jefferson\", \"Cubitt\", \"Turner\", \"Trevelyan\", \"Ferguson\", \"Munro\", \"Gregory\", \"McMurdo\", \"Melas\", \"Stanley\", \"Victor\", \"Hopkins\", \"Williamson\"], \"vertices_labels\": [\"Harrison\", \"Barker\", \"Angel\", \"God\", \"Jack\", \"White\", \"don\", \"Jones\", \"Adair\", \"Lyons\", \"MacDonald\", \"McCarthy\", \"Coombe\", \"Godfrey\", \"Joseph\", \"McGinty\", \"Robert\", \"Mycroft\", \"Sutherland\", \"Slaney\", \"Straker\", \"Baron\", \"Baynes\", \"Lee\", \"Baskerville\", \"Adler\", \"Black\", \"Clair\", \"Frances\", \"Elsie\", \"Tregennis\", \"Fitzroy\", \"Jim\", \"Lestrade\", \"Ettie\", \"McPherson\", \"Charles\", \"Milverton\", \"Moran\", \"Peters\", \"Ross\", \"Sholto\", \"Wilder\", \"Von\", \"Wilson\", \"Bannister\", \"Stapleton\", \"West\", \"Harker\", \"Hudson\", \"Carruthers\", \"Lucy\", \"Warren\", \"Martin\", \"Ferrier\", \"Moriarty\", \"Mary\", \"Morris\", \"Peter\", \"Small\", \"Sterndale\", \"Theresa\", \"Napoleon\", \"Pinner\", \"Merryweather\", \"Tracey\", \"Alice\", \"Hosmer\", \"Irene\", \"Gorgiano\", \"Jonathan\", \"Soames\", \"Mortimer\", \"Cyril\", \"Susan\", \"Cushing\", \"Huxtable\", \"Jonas\", \"Don\", \"Blessington\", \"Drebber\", \"Cecil\", \"Jove\", \"Henry\", \"Phelps\", \"Simon\", \"Trevor\", \"Johnson\", \"Hope\", \"Billy\", \"Percy\", \"Brackenstall\", \"Overton\", \"Sherlock\", \"Jabez\", \"Willoughby\", \"Acton\", \"Pycroft\", \"Lucas\", \"Hunter\", \"Bennett\", \"Bartholomew\", \"Culverton\", \"Murdoch\", \"Cunningham\", \"Hugo\", \"Gennaro\", \"Gregson\", \"Stangerson\", \"Prendergast\", \"Scott\", \"Ralph\", \"Musgrave\", \"Peterson\", \"Staunton\", \"Woodley\", \"Alec\", \"Cadogan\", \"Smith\", \"Armstrong\", \"Eccles\", \"Baker\", \"Toby\", \"Openshaw\", \"Morrison\", \"Brown\", \"Watson\", \"Eustace\", \"Henderson\", \"Scanlan\", \"Garcia\", \"Mason\", \"Evans\", \"Stoner\", \"Neville\", \"Barrymore\", \"Boss\", \"Browner\", \"Brunton\", \"Carey\", \"Bork\", \"Frank\", \"Roylott\", \"Mawson\", \"Baldwin\", \"Barclay\", \"Gloria\", \"Oldacre\", \"Rucastle\", \"McFarlane\", \"Hilton\", \"Doran\", \"Hall\", \"Toller\", \"Oberstein\", \"William\", \"Hayes\", \"Thaddeus\", \"Simpson\", \"Beppo\", \"Gilchrist\", \"Gibson\", \"Sarah\", \"Douglas\", \"Hilda\", \"Athelney\", \"Garrideb\", \"George\", \"Brother\", \"Windibank\", \"Jefferson\", \"Cubitt\", \"Turner\", \"Trevelyan\", \"Ferguson\", \"Munro\", \"Gregory\", \"McMurdo\", \"Melas\", \"Stanley\", \"Victor\", \"Hopkins\", \"Williamson\"], \"edges\": [[\"MacDonald\", \"White\"], [\"God\", \"Jack\"], [\"McGinty\", \"Jack\"], [\"Lyons\", \"Coombe\"], [\"God\", \"don\"], [\"Joseph\", \"Harrison\"], [\"don\", \"Jack\"], [\"Adair\", \"Moran\"], [\"Lyons\", \"Tracey\"], [\"Jones\", \"Small\"], [\"Sutherland\", \"Mary\"], [\"Robert\", \"Simon\"], [\"Mycroft\", \"Sherlock\"], [\"Barker\", \"Cecil\"], [\"Joseph\", \"Drebber\"], [\"don\", \"Sherlock\"], [\"don\", \"Henry\"], [\"God\", \"Sherlock\"], [\"Joseph\", \"Smith\"], [\"MacDonald\", \"Mason\"], [\"God\", \"Watson\"], [\"White\", \"Mason\"], [\"Jones\", \"Watson\"], [\"McGinty\", \"Boss\"], [\"Robert\", \"Ferguson\"], [\"McGinty\", \"Brother\"], [\"Mycroft\", \"Brother\"], [\"Jones\", \"Athelney\"], [\"Barker\", \"Douglas\"], [\"Slaney\", \"Cubitt\"], [\"McGinty\", \"McMurdo\"], [\"Lestrade\", \"don\"], [\"Black\", \"Jack\"], [\"Lestrade\", \"Mycroft\"], [\"Charles\", \"Baskerville\"], [\"Milverton\", \"Charles\"], [\"Lee\", \"Clair\"], [\"Tregennis\", \"Mortimer\"], [\"Charles\", \"Mortimer\"], [\"Adler\", \"Sherlock\"], [\"Lestrade\", \"Sherlock\"], [\"Baskerville\", \"Henry\"], [\"Sholto\", \"Bartholomew\"], [\"Lestrade\", \"Gregson\"], [\"Milverton\", \"Watson\"], [\"Baskerville\", \"Hall\"], [\"Charles\", \"Hall\"], [\"Jim\", \"Browner\"], [\"Moran\", \"Roylott\"], [\"Charles\", \"Barrymore\"], [\"Hudson\", \"Mycroft\"], [\"Hudson\", \"don\"], [\"Tracey\", \"Coombe\"], [\"Peter\", \"Lee\"], [\"Peter\", \"Black\"], [\"Von\", \"Baron\"], [\"Hudson\", \"Napoleon\"], [\"Lucy\", \"Ferrier\"], [\"Hudson\", \"Sherlock\"], [\"Lucy\", \"Hope\"], [\"Moriarty\", \"Sherlock\"], [\"Ferrier\", \"Hope\"], [\"Wilson\", \"Jabez\"], [\"Bannister\", \"Soames\"], [\"Small\", \"Jonathan\"], [\"Von\", \"Bork\"], [\"Peter\", \"Carey\"], [\"Morris\", \"Brother\"], [\"Morris\", \"McMurdo\"], [\"Hosmer\", \"Angel\"], [\"Huxtable\", \"Wilder\"], [\"Mortimer\", \"Baskerville\"], [\"Henry\", \"Baskerville\"], [\"Irene\", \"Adler\"], [\"Henry\", \"Stapleton\"], [\"Henry\", \"Mortimer\"], [\"Mortimer\", \"Henry\"], [\"Percy\", \"Phelps\"], [\"Cyril\", \"Overton\"], [\"Drebber\", \"Hope\"], [\"Irene\", \"Sherlock\"], [\"Brackenstall\", \"Eustace\"], [\"Willoughby\", \"Smith\"], [\"Alice\", \"Rucastle\"], [\"Simon\", \"Doran\"], [\"Jonas\", \"Oldacre\"], [\"Jonas\", \"McFarlane\"], [\"Henry\", \"Hall\"], [\"Hope\", \"Hilda\"], [\"Percy\", \"Trevelyan\"], [\"Trevor\", \"Victor\"], [\"Sherlock\", \"Hopkins\"], [\"Woodley\", \"Jack\"], [\"Staunton\", \"Godfrey\"], [\"Stangerson\", \"Joseph\"], [\"Bartholomew\", \"Sholto\"], [\"Stangerson\", \"Lestrade\"], [\"Hugo\", \"Baskerville\"], [\"Woodley\", \"Carruthers\"], [\"Stangerson\", \"Drebber\"], [\"Acton\", \"Cunningham\"], [\"Culverton\", \"Smith\"], [\"Scott\", \"Eccles\"], [\"Cunningham\", \"Alec\"], [\"Ralph\", \"Smith\"], [\"Scott\", \"Gloria\"], [\"Pycroft\", \"Hall\"], [\"Stangerson\", \"Brother\"], [\"Armstrong\", \"Godfrey\"], [\"Watson\", \"don\"], [\"Watson\", \"Hudson\"], [\"Cadogan\", \"West\"], [\"Watson\", \"Moriarty\"], [\"Baker\", \"Henry\"], [\"Watson\", \"Sherlock\"], [\"Baker\", \"Sherlock\"], [\"Watson\", \"Huxtable\"], [\"Smith\", \"Sherlock\"], [\"Watson\", \"Don\"], [\"Baker\", \"Hope\"], [\"Watson\", \"Trevor\"], [\"Armstrong\", \"Staunton\"], [\"Watson\", \"Musgrave\"], [\"Garcia\", \"Scott\"], [\"Garcia\", \"Eccles\"], [\"Baker\", \"Watson\"], [\"Watson\", \"Baker\"], [\"Watson\", \"Smith\"], [\"Stoner\", \"Roylott\"], [\"Scanlan\", \"McMurdo\"], [\"Baker\", \"Jefferson\"], [\"Scanlan\", \"Brother\"], [\"Simpson\", \"Straker\"], [\"Hilton\", \"Slaney\"], [\"Carey\", \"Lee\"], [\"Hall\", \"Baskerville\"], [\"Neville\", \"Clair\"], [\"Neville\", \"Lee\"], [\"Barrymore\", \"Baskerville\"], [\"Simpson\", \"Fitzroy\"], [\"Roylott\", \"Moran\"], [\"Thaddeus\", \"Sholto\"], [\"William\", \"Morris\"], [\"Frank\", \"Simon\"], [\"Barrymore\", \"Henry\"], [\"William\", \"Cunningham\"], [\"Oberstein\", \"Hugo\"], [\"Barclay\", \"Morrison\"], [\"William\", \"Alec\"], [\"Hall\", \"Barrymore\"], [\"McFarlane\", \"Oldacre\"], [\"Baldwin\", \"Brother\"], [\"Baldwin\", \"McMurdo\"], [\"Hilton\", \"Cubitt\"], [\"Sarah\", \"don\"], [\"Turner\", \"McCarthy\"], [\"McMurdo\", \"don\"], [\"McMurdo\", \"Jack\"], [\"Gregory\", \"Ross\"], [\"Jefferson\", \"Ferrier\"], [\"Jefferson\", \"Lucy\"], [\"Williamson\", \"Carruthers\"], [\"Sarah\", \"Mary\"], [\"Trevelyan\", \"Blessington\"], [\"Sarah\", \"Cushing\"], [\"Gilchrist\", \"Jabez\"], [\"Jefferson\", \"Hope\"], [\"Brother\", \"Bartholomew\"], [\"Williamson\", \"Woodley\"], [\"Williamson\", \"Hall\"], [\"McMurdo\", \"Brother\"], [\"Stanley\", \"Hopkins\"]], \"ipython\": true, \"error_msg\": \"\"}, e);\n",
         "                });\n",
         "            })();\n",
         "        "
        ]
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": false
     },
     "source": [
      "According the above graph that can be created almost automatically. We can easily identify the main characters in the stories. Additionally, we can observe that the strongest connection is between Sherlock and Watson.\n",
      "Moreover, we can see various connections among the main and minor characters of the book. However, from only looking at the graph, it is non-trivial to understand the various communities and their relationships.\n",
      "\n",
      "Using similar methods, we can learn more on each character by finding connections among person and location and person and organization. I leave the reader to find additional insights on the various characters on their own."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": false
     },
     "source": [
      "## <a id=\"topic\"></a> 4. Topic Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "<i>\"I have known him for some time,\" said I, \"but I never knew him do anything yet without a very good reason, and with that our conversation drifted off on to other topics.</i>\n",
      "                                                                 -Memoirs of Sherlock Holmes\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "According to Wikipedia [article](https://en.wikipedia.org/wiki/Topic_model), topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. I personally find topic models an interesting tool to explore a large text corpus. In this section, we are going to demonstrate how it is possible to utilze GraphLab's [topic model toolkit](https://dato.com/products/create/docs/graphlab.toolkits.topic_model.html) with the [pyLDAvis](https://github.com/bmabey/pyLDAvis) package to uncover topics in a set of documents. Namely, we will use GraphLab's topic model toolkit to analyze paragraphs in Sherlock Holmes stories. \n",
      "\n",
      "We will start by separating each story into paragraphs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import graphlab as gl\n",
      "import re\n",
      "\n",
      "sf =  gl.load_sframe(\"%s/books.sframe\" % BASE_DIR)\n",
      "sf_paragraphs = sf.flat_map(['title', 'text'], lambda t: [[t['title'],p.strip()] for p in t['text'].split(\"\\n\\n\")])\n",
      "sf_paragraphs = sf_paragraphs.rename({'text': 'paragraph'})"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's calculate the number of words in each paragraph, and filter the paragraph that have less than 25 words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "re_words_split = re.compile(\"(\\w+)\")\n",
      "sf_paragraphs['paragraph_words_number'] = sf_paragraphs['paragraph'].apply(lambda p: len(re_words_split.findall(p)) )\n",
      "sf_paragraphs = sf_paragraphs[sf_paragraphs['paragraph_words_number'] >=25]"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using the stories' paragraphs as documents, we can utilize GraphLab's topic model toolkit to discover topics that appear in these paragraph.\n",
      "We create a topic model with 10 topics to learn.\n",
      "\n",
      "Note: the topic model results may be different in each run."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "docs =  gl.text_analytics.count_ngrams(sf_paragraphs['paragraph'], n=1)\n",
      "stopwords = gl.text_analytics.stopwords()\n",
      "# adding some additional stopwords to make the topic model more clear\n",
      "stopwords |= set(['man', 'mr', 'sir', 'make', 'made', 'll', 'door', 'long', 'day', 'small']) \n",
      "docs = docs.dict_trim_by_keys(stopwords, exclude=True)\t\n",
      "docs = docs.dropna()\n",
      "topic_model = gl.topic_model.create(docs, num_topics=10)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PROGRESS: Learning a topic model\n",
        "PROGRESS:        Number of documents     11657\n",
        "PROGRESS:            Vocabulary size     18734\n",
        "PROGRESS:    Running collapsed Gibbs sampling\n",
        "PROGRESS: +-----------+---------------+----------------+-----------------+\n",
        "PROGRESS: | Iteration | Elapsed Time  | Tokens/Second  | Est. Perplexity |\n",
        "PROGRESS: +-----------+---------------+----------------+-----------------+\n",
        "PROGRESS: | 10        | 691.907ms     | 5.10963e+06    | 0               |\n",
        "PROGRESS: +-----------+---------------+----------------+-----------------+\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's view the most common word in each topic"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topic_model.get_topics().print_rows(100)\n",
      "topic_model.save(\"%s/topic_model\" % BASE_DIR)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "+-------+----------+------------------+\n",
        "| topic |   word   |      score       |\n",
        "+-------+----------+------------------+\n",
        "|   0   |  watson  | 0.0249172864212  |\n",
        "|   0   |   case   | 0.0183231362925  |\n",
        "|   0   |  matter  | 0.0131338268433  |\n",
        "|   0   |  young   | 0.0101521241764  |\n",
        "|   0   |   dear   | 0.00934935807382 |\n",
        "|   1   |  heard   | 0.0148204289721  |\n",
        "|   1   |    \"     | 0.0140823880733  |\n",
        "|   1   |  london  | 0.0124359891451  |\n",
        "|   1   |   put    | 0.0107612040286  |\n",
        "|   1   |   back   | 0.0107044316517  |\n",
        "|   2   |   life   | 0.0126994044378  |\n",
        "|   2   |   room   | 0.0113810279897  |\n",
        "|   2   |  thing   | 0.00934614260247 |\n",
        "|   2   |  clear   | 0.00888757688141 |\n",
        "|   2   |   head   | 0.00791312472414 |\n",
        "|   3   |   room   |  0.026319693207  |\n",
        "|   3   |   hand   | 0.0229133243962  |\n",
        "|   3   |   left   | 0.0192597191394  |\n",
        "|   3   |   side   | 0.0136831637474  |\n",
        "|   3   |   half   | 0.0133809858691  |\n",
        "|   4   | morning  | 0.0217436676387  |\n",
        "|   4   |   back   |  0.01319229855   |\n",
        "|   4   |  window  | 0.0118008893424  |\n",
        "|   4   |  round   | 0.0100906155246  |\n",
        "|   4   |  young   | 0.00832236632326 |\n",
        "|   5   |  night   | 0.0180842149072  |\n",
        "|   5   |  light   | 0.0142598435382  |\n",
        "|   5   | brought  | 0.0112509631229  |\n",
        "|   5   |  street  | 0.0100980276367  |\n",
        "|   5   |   told   | 0.00857952724021 |\n",
        "|   6   |  holmes  | 0.0378032415173  |\n",
        "|   6   |   time   | 0.0299076826969  |\n",
        "|   6   | thought  |  0.019438113858  |\n",
        "|   6   | sherlock | 0.0148685230388  |\n",
        "|   6   |  found   | 0.0132199997686  |\n",
        "|   7   |    \"     |  0.206028291106  |\n",
        "|   7   |    \"i    | 0.0349375618017  |\n",
        "|   7   |   \"it    |  0.019365019214  |\n",
        "|   7   |   \"the   | 0.00912148359554 |\n",
        "|   7   |   wife   | 0.00891424443563 |\n",
        "|   8   |    \"     |  0.111275157582  |\n",
        "|   8   |  holmes  | 0.0559314006385  |\n",
        "|   8   |   face   | 0.0275724760557  |\n",
        "|   8   |    \"i    | 0.0234501993895  |\n",
        "|   8   |   \"you   | 0.0146209259627  |\n",
        "|   9   |  great   | 0.0149938904351  |\n",
        "|   9   |  house   |  0.013777634467  |\n",
        "|   9   |  friend  | 0.0115148326658  |\n",
        "|   9   |   work   |  0.010920847193  |\n",
        "|   9   |  \"well   | 0.0100157264725  |\n",
        "+-------+----------+------------------+\n",
        "[50 rows x 3 columns]\n",
        "\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Reading the above table, we can understand some of the topics. However, it still hard to get good overall overview.\n",
      "Therefore, we will use the excellent [pyLDAvis package](https://github.com/bmabey/pyLDAvis/blob/master/README.rst), developed by Ben Mabey,\n",
      "to better the various topics in the books."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pyLDAvis\n",
      "import pyLDAvis.graphlab\n",
      "pyLDAvis.enable_notebook()\n",
      "pyLDAvis.graphlab.prepare(topic_model, docs)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "text/html": [
         "\n",
         "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
         "\n",
         "\n",
         "<div id=\"ldavis_el109345929019688114613504\"></div>\n",
         "<script type=\"text/javascript\">\n",
         "\n",
         "var ldavis_el109345929019688114613504_data = {\"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [4, 6, 10, 2, 3, 1, 7, 5, 9, 8], \"token.table\": {\"Topic\": [1, 4, 5, 6, 9, 10, 4, 9, 10, 3, 10, 3, 6, 7, 8, 1, 6, 9, 10, 10, 5, 4, 8, 8, 2, 3, 1, 5, 6, 7, 8, 9, 8, 10, 3, 2, 5, 10, 5, 7, 9, 5, 4, 8, 4, 9, 10, 5, 7, 9, 10, 4, 8, 6, 7, 10, 3, 10, 2, 10, 3, 7, 9, 5, 2, 3, 9, 8, 3, 10, 4, 2, 9, 1, 3, 6, 7, 8, 1, 6, 7, 9, 10, 2, 4, 5, 6, 8, 9, 10, 3, 9, 10, 1, 7, 4, 5, 8, 10, 1, 2, 3, 6, 4, 6, 2, 3, 6, 7, 2, 10, 7, 1, 3, 4, 5, 7, 8, 10, 5, 6, 7, 8, 9, 10, 10, 2, 9, 10, 3, 8, 10, 5, 2, 1, 3, 10, 2, 3, 6, 3, 10, 9, 1, 3, 6, 8, 6, 3, 5, 8, 9, 4, 1, 6, 10, 5, 6, 3, 9, 2, 3, 7, 8, 1, 4, 7, 2, 3, 5, 7, 6, 10, 2, 10, 1, 1, 3, 5, 8, 9, 7, 5, 10, 3, 6, 3, 6, 10, 6, 7, 10, 2, 8, 5, 10, 6, 10, 1, 3, 5, 6, 9, 3, 5, 6, 9, 8, 6, 2, 7, 9, 6, 1, 6, 1, 9, 4, 7, 2, 6, 8, 7, 8, 9, 5, 8, 3, 10, 1, 6, 6, 2, 4, 6, 7, 9, 8, 3, 6, 2, 1, 3, 4, 5, 6, 7, 8, 7, 6, 2, 8, 10, 2, 7, 3, 6, 4, 1, 4, 10, 2, 6, 7, 2, 2, 2, 6, 8, 10, 8, 2, 6, 7, 7, 8, 9, 9, 2, 1, 7, 9, 10, 9, 7, 2, 7, 8, 9, 10, 5, 4, 8, 2, 3, 1, 10, 1, 4, 7, 8, 7, 7, 1, 2, 9, 6, 8, 2, 10, 2, 7, 9, 10, 7, 1, 4, 8, 9, 10, 6, 10, 9, 4, 9, 7, 4, 3, 7, 9, 9, 9, 9, 1, 5, 7, 8, 10, 1, 3, 10, 3, 4, 3, 9, 10, 2, 4, 1, 7, 5, 9, 5, 7, 1, 2, 3, 5, 6, 2, 4, 7, 9, 1, 2, 9, 9, 7, 10, 6, 10, 7, 2, 2, 2, 5, 6, 8, 9, 10, 5, 6, 9, 2, 1, 2, 4, 7, 8, 9, 10, 8, 3, 4, 7, 10, 5, 9, 9, 2, 4, 8, 3, 8, 10, 3, 6, 1, 8, 10, 3, 4, 4, 5, 6, 7, 8, 4, 9, 1, 4, 6, 7, 10, 7, 7, 10, 5, 7, 1, 1, 2, 6, 9, 10, 10, 5, 6, 7, 5, 4, 3, 7, 1, 4, 9, 4, 4, 10, 3, 1, 9, 6, 7, 3, 4, 6, 7, 9, 7, 6, 3, 4, 6, 2, 9, 4, 5, 3, 9, 10, 10, 1, 4, 5, 10, 2, 4, 5, 6, 7, 9, 10, 1, 2, 5, 9, 9, 5, 1, 1, 6, 7, 1, 2, 4, 6, 9, 10, 1, 1, 2, 3, 5, 6, 8, 9, 3, 3, 5, 5, 2, 7, 5, 4, 9, 1, 1, 2, 4, 8, 5, 9, 7, 3, 1, 6, 5, 10, 6, 9, 1, 9, 6, 4, 2, 3, 6, 8, 10, 3, 2, 9, 1, 2, 2, 10, 7, 6, 5, 7, 9, 8, 6, 3, 9, 8, 9, 7, 4, 1, 4, 10, 6, 3, 6, 10, 7, 7, 1, 3, 4, 5, 7, 8, 1, 2, 4, 7, 8, 2, 4, 7, 8, 9, 10, 8, 8, 1, 2, 9, 6, 8, 8, 10, 4, 5, 8, 1, 7, 10, 3, 6, 8, 9, 10, 1, 4, 2, 9, 3, 1, 3, 5, 6, 4, 8, 2, 6, 10, 1, 10, 1, 2, 4, 5, 6, 3, 8, 3, 4, 6, 2, 10, 4, 1, 2, 5, 6, 8, 1, 1, 3, 4, 8, 8, 6, 9, 2, 3, 5, 9, 10, 10, 5, 1, 4, 10, 6, 3, 1, 2, 5, 1, 2, 3, 8, 8, 7, 3, 1, 6, 9, 8, 3, 2, 6, 9, 7, 2, 1, 2, 1, 3, 10, 9, 10, 1, 3, 5, 4, 10, 3, 8, 7, 1, 1, 10, 1, 1, 2, 7, 2, 2, 5, 9, 1, 4, 2, 5, 6, 7, 10, 2, 5, 8, 7, 1, 5, 9, 10, 1, 6, 3, 4, 5, 1, 2, 3, 6, 7, 9, 4, 7, 2, 3, 6, 7, 8, 9, 2, 7, 2, 3, 5, 7, 9, 2, 4, 9, 10, 2, 4, 5, 10, 6, 1, 5, 10, 10, 10, 8, 9, 5, 6, 7, 10, 1, 5, 6, 7, 2, 7, 8, 9, 1, 3, 1, 2, 4, 10, 10, 1, 3, 4, 6, 7, 6, 10, 5, 5, 8, 8, 5, 9, 4, 3, 4, 7, 7, 3, 7, 6, 4, 3, 2, 1, 3, 4, 7, 8, 10, 4, 8, 6, 2, 3, 8, 10, 1, 2, 3, 4, 6, 7, 7, 3, 5, 7, 8, 9, 10, 6, 6, 6, 1, 10, 3, 6, 5, 7, 2, 3, 2, 2, 3, 5, 9, 10, 4, 2, 3, 6, 8, 9, 10, 3, 6, 8, 1, 3, 4, 5, 6, 1, 2, 4, 8, 8, 7, 8, 10, 10, 5, 8, 1, 8, 9, 4, 1, 2, 5, 8, 9, 1, 2, 3, 7, 9, 10, 6, 1, 2, 3, 4, 9, 10, 10, 2, 4, 6, 7, 9, 2, 3, 7, 10, 10, 5, 2, 3, 4, 5, 1, 3, 4, 5, 7, 8, 9, 9, 1, 5, 6, 7, 8, 10, 9, 6, 10, 3, 3, 5, 10, 1, 3, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 9, 2, 2, 9, 2, 7, 3, 8, 9, 10, 9, 4, 7, 9, 10, 4, 8, 5, 8, 7, 9, 10, 3, 8, 1, 6, 7, 8, 8, 2, 9, 9, 9, 2, 4, 7, 10, 7, 2, 1, 1, 4, 7, 10, 1, 2, 2, 4, 6, 7, 10, 3, 6, 7, 6, 9, 10, 5, 3, 4, 10, 4, 5, 6, 7, 9, 10, 2, 3, 4, 6, 3, 9, 5, 10, 1, 3, 10, 3, 10, 2, 3, 8, 9, 2, 2, 1, 2, 4, 8, 1, 3, 3, 4, 8, 9, 3, 2, 3, 5, 7, 10, 3, 6, 5, 1, 3, 7, 10, 1, 5, 8, 9, 10, 10, 2, 3, 5, 6, 7, 9, 6, 10, 5, 2, 5, 6, 7, 10, 1, 2, 4, 5, 6, 7, 9, 10, 6, 1, 7, 5, 2, 5, 6, 7, 6, 2, 5, 7, 9, 2, 3, 4, 5, 8, 10, 3, 6, 8, 9, 5, 1, 10, 8, 2, 3, 9, 1, 6, 8, 8, 1, 10, 5, 10, 2, 4, 5, 7, 8, 5, 1, 6, 9, 10, 1, 2, 3, 5, 9, 1, 5, 7, 1, 5, 1, 2, 6, 7, 8, 9, 10, 1, 4, 5, 10, 3, 1, 4, 5, 6, 7, 10, 3, 3, 4, 10, 3, 4, 7, 8, 10, 4, 6, 8, 9, 3, 1, 4, 5, 7, 8, 10, 4, 7, 9, 1, 1, 5, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 5, 4, 1, 5, 7, 8, 1, 3, 4, 6, 10, 5, 10, 3, 4, 8, 9, 8, 9, 4, 5, 8, 10, 2, 2, 10, 9, 5, 7, 1, 3, 4, 5, 7, 8, 9, 10, 3, 10, 2, 5, 7, 8, 8, 7, 6, 8, 9, 1, 7, 9, 1, 10, 1, 4, 6, 7, 8, 1, 3, 4, 5, 6, 7, 10, 6, 5, 6, 6, 6, 6, 7, 9, 4, 5, 8, 1, 1, 1, 1, 2, 3, 5, 10, 9, 6, 4, 4, 3, 6, 9, 8, 7, 8, 10, 7, 2, 10, 7, 8, 5, 8, 4, 3, 8, 10, 2, 7, 9, 3, 4, 5, 7, 9, 2, 5, 8, 10, 4, 3, 4, 1, 3, 8, 10, 1, 2, 4, 5, 6, 4, 8, 2, 3, 4, 8, 10, 4, 7, 6, 10, 2, 3, 4, 10, 2, 3, 4, 5, 4, 5, 5, 6, 2, 2, 9, 1, 3, 6, 9, 4, 10, 3, 7, 4, 9, 9, 1, 3, 5, 7, 8, 1, 7, 2, 2, 3, 5, 6, 9, 10, 8, 9, 4, 6, 9, 10, 9, 10, 6, 8, 9, 10, 3, 8, 10, 1, 8, 1, 2, 5, 10, 7, 2, 2, 4, 7, 9, 1, 7, 1, 2, 3, 5, 10, 5, 2, 4, 6, 1, 6, 1, 1, 5, 8, 3, 7, 9, 3, 6, 9, 3, 1, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 8, 9, 4, 7, 6, 1, 2, 5, 9, 6, 6, 10, 2, 7, 3, 7, 8, 1, 3, 4, 5, 6, 6, 7, 5, 7, 8, 9, 10, 5, 2, 4, 6, 10, 8, 6, 1, 4, 1, 4, 5, 7, 8, 1, 2, 3, 6, 7, 7, 3, 1, 6, 7, 1, 2, 4, 10, 2, 4, 7, 8, 10, 5, 8, 9, 3, 1, 2, 4, 6, 8, 9, 3, 5, 8, 10, 1, 3, 1, 4, 6, 8, 9, 10, 10, 5, 2, 9, 9, 4, 5, 1, 9, 5, 1, 6, 2, 3, 6, 8, 9, 10, 3, 4, 10, 10, 2, 3, 4, 5, 6, 7, 9, 3, 1, 1, 3, 4, 10, 4, 10, 4, 5, 6, 8, 9, 3, 5, 5, 7, 9, 10, 1, 9, 8, 10, 1, 3, 9, 1, 5, 8, 10, 5, 8, 7, 8, 8, 1, 3, 4, 5, 8, 5, 1, 6, 6, 7, 2, 2, 4, 5, 6, 7, 3, 4, 7, 8, 10, 1, 6, 2, 5, 6, 9, 1, 2, 7, 4, 6, 3, 4, 1, 3, 5, 6, 9, 1, 9, 2, 1, 3, 4, 8, 1, 2, 3, 6, 7, 8, 10, 5, 2, 4, 9, 6, 2, 8, 3, 1, 2, 4, 5, 6, 7, 8, 10, 1, 2, 4, 9, 2, 8, 3, 8, 1, 2, 7, 2, 1, 6, 9, 3, 2, 3, 7, 6, 1, 3, 7, 4, 1, 9, 9, 2, 3, 5, 7, 10, 9, 6, 2, 4, 5, 8, 3, 5, 2, 3, 8, 10, 5, 5, 6, 4, 3, 7, 9, 3, 5, 7, 10, 3, 1, 10, 9, 8, 3, 4, 8, 10, 3, 7, 4, 2, 6, 9, 3, 2, 4, 7, 10, 1, 3, 5, 6, 7, 8, 10, 9, 9, 10, 3, 5, 5, 1, 7, 6, 5, 3, 10, 3, 3, 4, 5, 8, 9, 5, 6, 10, 5, 9, 8, 2, 4, 7, 1, 2, 4, 5, 6, 10, 2, 3, 5, 8, 2, 9, 10, 6, 6, 2, 4, 8, 5, 4, 10, 9, 10, 4, 10, 1, 2, 3, 8, 1, 2, 4, 5, 6, 8, 10, 7, 5, 9, 1, 2, 7, 4, 9, 3, 3, 4, 6, 8, 10, 5, 9, 7, 5, 10, 1, 3, 3, 5, 6, 8, 9, 6, 8, 8, 9, 6, 4, 1, 5, 6, 2, 6, 7, 8, 10, 2, 3, 4, 10, 9, 1, 2, 2, 3, 5, 6, 7, 10, 10, 2, 7, 2, 5, 10, 9, 4, 2, 4, 5, 6, 7, 10, 3, 4, 5, 10, 2, 5, 7, 8, 10, 7, 8, 3, 4, 6, 7, 10, 9, 1, 8, 7, 3, 4, 5, 6, 8, 8, 3, 6, 6, 1, 3, 5, 7, 7, 9, 3, 9, 10, 10, 5, 1, 3, 5, 6, 1, 2, 3, 7, 8, 9, 10, 7, 3, 5, 4, 5, 9, 6, 7, 10, 8, 1, 10, 10, 3, 2, 10, 5, 3, 7, 8, 6, 7, 6, 8, 5, 8, 1, 6, 6, 8, 2, 3, 8, 10, 8, 2, 4, 9, 8, 9, 6, 6, 3, 1, 4, 6, 10, 2, 5, 7, 8, 9, 1, 4, 6, 8, 2, 5, 3, 6, 6, 1, 2, 3, 4, 6, 7, 2, 3, 3, 6, 3, 5, 9, 3, 5, 6, 8, 3, 2, 6, 6, 9, 2, 2, 2, 7, 7, 4, 1, 2, 4, 6, 7, 8, 1, 3, 10, 6, 7, 7, 2, 5, 2, 2, 8, 1, 5, 6, 7, 5, 9, 10, 2, 5, 8, 1, 4, 8, 6, 9, 9, 3, 3, 10, 6, 1, 6, 4, 2, 2, 8, 2, 6, 8, 2, 10, 3, 6, 1, 7, 4, 3, 6, 1, 5, 7, 10, 10, 7, 8, 1, 2, 3, 4, 5, 9, 2, 10, 1, 4, 1, 2, 4, 7, 8, 9, 10, 5, 3, 3, 5, 9, 8, 9, 7, 5, 1, 5, 6, 7, 2, 1, 2, 4, 6, 9, 8, 9, 4, 1, 8, 3, 1, 3, 8, 4, 2, 6, 10, 1, 2, 3, 4, 7, 8, 9, 10, 8, 8, 10, 3, 2, 3, 7, 7, 6, 4, 8, 1, 2, 8, 3, 2, 2, 3, 4, 5, 6, 4, 10, 1, 3, 4, 6, 10, 1, 5, 1, 2, 9, 3, 2, 3, 2, 2, 5, 6, 7, 10, 3, 9, 10, 1, 4, 2, 7, 9, 2, 2, 3, 4, 10, 8, 2, 2, 2, 3, 5, 1, 7, 8, 9, 1, 4, 5, 9, 3, 4, 5, 6, 7, 2, 3, 5, 10, 8, 2, 10, 3, 8, 1, 2, 6, 7, 8, 9, 6, 2, 4, 6, 9, 9, 1, 7, 10, 10, 10, 1, 1, 1, 4, 6, 8, 9, 2, 4, 1, 3, 5, 8, 9, 8, 8, 1, 6, 7, 6, 5, 5, 8, 1, 3, 9, 9, 5, 6, 7, 8, 6, 1, 5, 1, 8, 9, 7, 9, 1, 1, 4, 5, 8, 10, 1, 2, 4, 8, 5, 2, 3, 4, 8, 10, 1, 2, 3, 5, 7, 3, 10, 4, 6, 7, 9, 2, 4, 7, 3, 7, 8, 9, 8, 2, 6, 9, 3, 2, 5, 9, 3, 7, 9, 10, 1, 2, 3, 5, 8, 7, 1, 5, 5, 8, 1, 7, 9, 9, 8, 5, 3, 7, 3, 8, 3, 8, 9, 1, 2, 3, 4, 5, 7, 8, 7, 3, 4, 5, 6, 7, 2, 3, 4, 6, 9, 10, 7, 8, 4, 9, 1, 2, 6, 10, 8, 9, 2, 4, 5, 8, 8, 5, 8, 4, 7, 7, 3, 1, 2, 3, 5, 6, 7, 8, 10, 1, 2, 3, 4, 5, 7, 8, 9, 10, 5, 10, 5, 8, 10, 2, 4, 5, 9, 7, 6, 5, 1, 8, 5, 10, 2, 9, 1, 3, 5, 7, 9, 6, 1, 5, 8, 9, 6, 1, 5, 7, 9, 9, 3, 5, 5, 2, 5, 1, 4, 6, 7, 8, 9, 10, 7, 8, 6, 1, 3, 5, 8, 3, 9, 10, 8, 3, 4, 6, 5, 4, 7, 10, 10, 1, 2, 5, 7, 3, 4, 5, 6, 10, 5, 6, 9, 4, 2, 4, 6, 7, 1, 3, 5, 9, 10, 2, 7, 9, 5, 8, 8, 1, 2, 4, 5, 7, 1, 3, 5, 6, 9, 10, 6, 7, 1, 2, 3, 4, 6, 7, 8, 8, 9, 3, 5, 6, 9, 10, 7, 3, 4, 3, 1, 4, 6, 8, 10, 8, 5, 2, 3, 6, 10, 2, 1, 2, 3, 5, 3, 5, 7, 9, 7, 2, 6, 7, 1, 1, 5, 1, 5, 1, 1, 8, 2, 4, 7, 9, 3, 9, 6, 7, 6, 7, 5, 3, 1, 1, 7, 6, 7, 1, 6, 4, 6, 9, 3, 1, 8, 9, 7, 8, 10, 2, 10, 7, 1, 9, 1, 5, 6, 7, 8, 10, 8, 10, 3, 1, 10, 7, 9, 3, 9, 9, 4, 3, 3, 2, 2, 3, 7, 4, 5, 6, 7, 9, 10, 4, 10, 1, 4, 6, 1, 6, 3, 2, 3, 4, 5, 6, 7, 1, 4, 7, 7, 1, 2, 3, 4, 8, 10, 2, 7, 2, 5, 8, 8, 8, 2, 4, 2, 3, 4, 6, 7, 1, 3, 5, 6, 7, 9, 8, 2, 3, 10, 3, 8, 10, 2, 7, 5, 9, 4, 3, 1, 4, 5, 9, 9, 10, 4, 8, 9, 10, 10, 9, 7, 9, 10, 3, 4, 6, 8, 10, 9, 1, 3, 4, 10, 1, 1, 2, 3, 4, 5, 9, 1, 5, 7, 10, 1, 3, 4, 6, 8, 10, 1, 2, 5, 6, 9, 10, 3, 6, 8, 9, 10, 2, 4, 5, 9, 1, 4, 6, 10, 2, 3, 5, 9, 6, 3, 2, 4, 8, 3, 4, 10, 1, 3, 4, 7, 8, 9, 1, 5, 6, 10, 3, 4, 8, 6, 4, 6, 8, 9, 8], \"Freq\": [0.005178163945936458, 0.044935931191855363, 0.0068457082675092151, 8.7765490609092507e-05, 0.33579076707038791, 0.60716166403370198, 1.0, 0.012195121951219513, 0.98170731707317072, 1.0, 1.0, 0.81196581196581197, 0.068376068376068383, 0.059829059829059832, 0.05128205128205128, 0.075471698113207544, 0.48301886792452831, 0.011320754716981131, 0.42641509433962266, 1.0, 1.0, 0.024390243902439025, 0.96747967479674801, 1.0, 1.0, 1.0, 0.20080321285140562, 0.03614457831325301, 0.068273092369477914, 0.52208835341365467, 0.060240963855421686, 0.10843373493975904, 0.045454545454545456, 0.90909090909090906, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.032258064516129031, 0.39393939393939392, 0.57331378299120239, 0.020408163265306121, 0.16326530612244897, 0.020408163265306121, 0.7857142857142857, 0.93999999999999995, 0.040000000000000001, 0.0040871934604904629, 0.10899182561307902, 0.88555858310626701, 1.0, 1.0, 1.0, 0.88888888888888884, 1.0, 1.0, 1.0, 1.0, 0.94999999999999996, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0089820359281437123, 0.017964071856287425, 0.1497005988023952, 0.0029940119760479044, 0.82035928143712578, 0.0099800399201596807, 0.32534930139720558, 0.0099800399201596807, 0.04790419161676647, 0.60479041916167664, 0.0068728522336769758, 0.0034364261168384879, 0.3436426116838488, 0.50859106529209619, 0.0034364261168384879, 0.0034364261168384879, 0.12714776632302405, 1.0, 1.0, 1.0, 1.0, 1.0, 0.025925925925925925, 0.46296296296296297, 0.14814814814814814, 0.35925925925925928, 0.0015772870662460567, 0.43059936908517349, 0.55678233438485802, 0.0094637223974763408, 0.14999999999999999, 0.84166666666666667, 0.023255813953488372, 0.011627906976744186, 0.95348837209302328, 0.95238095238095233, 0.045454545454545456, 0.90909090909090906, 0.94444444444444442, 0.0037037037037037038, 0.014814814814814815, 0.0037037037037037038, 0.088888888888888892, 0.07407407407407407, 0.64814814814814814, 0.16296296296296298, 0.046583850931677016, 0.010869565217391304, 0.15527950310559005, 0.007763975155279503, 0.77018633540372672, 0.007763975155279503, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 0.040000000000000001, 0.0066666666666666671, 0.94666666666666666, 1.0, 1.0, 1.0, 0.050420168067226892, 0.096638655462184878, 0.81932773109243695, 0.029411764705882353, 1.0, 0.041666666666666664, 0.82291666666666663, 0.11458333333333333, 0.010416666666666666, 1.0, 1.0, 1.0, 1.0, 0.970873786407767, 0.019417475728155338, 1.0, 1.0, 0.010416666666666666, 0.052083333333333336, 0.82291666666666663, 0.10416666666666667, 0.090909090909090912, 0.86776859504132231, 0.033057851239669422, 0.022222222222222223, 0.8666666666666667, 0.022222222222222223, 0.077777777777777779, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0044843049327354259, 0.66816143497757852, 0.09417040358744394, 0.013452914798206279, 0.21524663677130046, 1.0, 0.95238095238095233, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.010526315789473684, 0.031578947368421054, 0.95438596491228067, 1.0, 1.0, 0.125, 0.86250000000000004, 1.0, 0.90000000000000002, 0.82389937106918243, 0.1069182389937107, 0.037735849056603772, 0.0062893081761006293, 0.018867924528301886, 0.038135593220338986, 0.63559322033898302, 0.046610169491525424, 0.27542372881355931, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.32679738562091504, 0.66666666666666663, 0.023255813953488372, 0.95348837209302328, 0.96551724137931039, 0.029411764705882353, 0.94117647058823528, 0.57723577235772361, 0.37398373983739835, 0.04878048780487805, 1.0, 1.0, 0.94736842105263153, 1.0, 0.026086956521739129, 0.9652173913043478, 1.0, 0.6428571428571429, 0.22321428571428573, 0.013392857142857142, 0.004464285714285714, 0.11160714285714286, 1.0, 1.0, 1.0, 1.0, 0.076730608840700584, 0.12260216847372811, 0.31359466221851545, 0.099249374478732277, 0.0050041701417848205, 0.0016680567139282735, 0.38031693077564638, 1.0, 1.0, 0.74874371859296485, 0.21608040201005024, 0.03015075376884422, 1.0, 0.9821428571428571, 0.03125, 0.953125, 0.97777777777777775, 0.94444444444444442, 1.0, 1.0, 1.0, 0.97826086956521741, 0.010869565217391304, 1.0, 1.0, 0.012048192771084338, 0.86746987951807231, 0.096385542168674704, 0.012048192771084338, 1.0, 0.93877551020408168, 0.020408163265306121, 0.020408163265306121, 1.0, 0.72908366533864544, 0.26693227091633465, 1.0, 0.95454545454545459, 0.34444444444444444, 0.0055555555555555558, 0.03888888888888889, 0.60555555555555551, 1.0, 1.0, 0.041095890410958902, 0.027397260273972601, 0.20547945205479451, 0.0068493150684931503, 0.71232876712328763, 1.0, 0.95588235294117652, 0.029411764705882353, 0.95833333333333337, 1.0, 1.0, 1.0, 0.11578947368421053, 0.86315789473684212, 0.010526315789473684, 1.0, 0.95652173913043481, 1.0, 0.56351039260969982, 0.18706697459584296, 0.24711316397228639, 1.0, 1.0, 1.0, 1.0, 0.032258064516129031, 0.88709677419354838, 0.048387096774193547, 0.016129032258064516, 1.0, 0.052863436123348019, 0.53303964757709255, 0.22026431718061673, 0.013215859030837005, 0.1762114537444934, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97435897435897434, 1.0, 0.003663003663003663, 0.97435897435897434, 0.018315018315018316, 1.0, 1.0, 1.0, 0.015625, 0.13020833333333334, 0.005208333333333333, 0.73958333333333337, 0.10416666666666667, 0.84883720930232553, 0.12790697674418605, 0.011627906976744186, 1.0, 1.0, 0.096774193548387094, 0.31451612903225806, 0.58870967741935487, 1.0, 0.95833333333333337, 1.0, 1.0, 1.0, 1.0, 0.83211678832116787, 0.16058394160583941, 0.11715481171548117, 0.054393305439330547, 0.062761506276150625, 0.020920502092050208, 0.7405857740585774, 0.96376811594202894, 0.007246376811594203, 0.024154589371980676, 0.0024154589371980675, 0.0085470085470085479, 0.85470085470085466, 0.13675213675213677, 0.95454545454545459, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97222222222222221, 1.0, 0.97727272727272729, 0.053268765133171914, 0.44794188861985473, 0.16222760290556901, 0.019370460048426151, 0.31476997578692495, 0.016393442622950821, 0.13114754098360656, 0.83606557377049184, 1.0, 1.0, 0.0085836909871244635, 0.50214592274678116, 0.0042918454935622317, 0.17167381974248927, 0.017167381974248927, 0.29613733905579398, 1.0, 1.0, 1.0, 1.0, 1.0, 0.97297297297297303, 1.0, 1.0, 0.087378640776699032, 0.88349514563106801, 0.019417475728155338, 0.96153846153846156, 0.019230769230769232, 1.0, 1.0, 1.0, 0.12087912087912088, 0.84065934065934067, 0.038461538461538464, 0.98148148148148151, 0.98039215686274506, 0.16306483300589392, 0.012770137524557957, 0.62671905697445973, 0.0137524557956778, 0.18369351669941061, 0.95454545454545459, 1.0, 0.70588235294117652, 0.041176470588235294, 0.17058823529411765, 0.0058823529411764705, 0.076470588235294124, 0.99130434782608701, 1.0, 1.0, 0.9375, 0.94117647058823528, 1.0, 0.01171875, 0.47265625, 0.0625, 0.4453125, 0.00390625, 0.94117647058823528, 1.0, 0.30496453900709219, 0.69503546099290781, 0.9375, 1.0, 0.025862068965517241, 0.96551724137931039, 1.0, 0.95238095238095233, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94444444444444442, 1.0, 0.035087719298245612, 0.035087719298245612, 0.89473684210526316, 0.017543859649122806, 1.0, 1.0, 0.008771929824561403, 0.90350877192982459, 0.078947368421052627, 0.95652173913043481, 0.021739130434782608, 1.0, 1.0, 0.12, 0.02, 0.83999999999999997, 0.91666666666666663, 0.01020408163265306, 0.01020408163265306, 0.89795918367346939, 0.071428571428571425, 0.0039138943248532287, 0.11545988258317025, 0.61056751467710368, 0.045009784735812131, 0.0019569471624266144, 0.0039138943248532287, 0.21526418786692758, 0.079545454545454544, 0.89772727272727271, 0.011363636363636364, 1.0, 1.0, 1.0, 1.0, 0.084291187739463605, 0.54789272030651337, 0.36398467432950193, 0.79166666666666663, 0.018518518518518517, 0.0092592592592592587, 0.055555555555555552, 0.1111111111111111, 0.0092592592592592587, 1.0, 0.94736842105263153, 1.0, 1.0, 0.66304347826086951, 0.016304347826086956, 0.31521739130434784, 1.0, 1.0, 0.94354838709677424, 0.052419354838709679, 0.95454545454545459, 1.0, 0.96666666666666667, 1.0, 1.0, 1.0, 1.0, 0.040145985401459854, 0.79197080291970801, 0.0036496350364963502, 0.16058394160583941, 1.0, 1.0, 1.0, 1.0, 0.94871794871794868, 0.02564102564102564, 0.94999999999999996, 1.0, 0.03125, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.0072992700729927005, 0.75912408759124084, 0.029197080291970802, 0.014598540145985401, 0.18248175182481752, 0.93333333333333335, 1.0, 1.0, 1.0, 1.0, 0.94594594594594594, 0.027027027027027029, 1.0, 1.0, 1.0, 0.94999999999999996, 1.0, 0.97058823529411764, 1.0, 0.94999999999999996, 0.97777777777777775, 1.0, 0.94736842105263153, 1.0, 0.94444444444444442, 0.0029325513196480938, 0.97653958944281527, 0.017595307917888565, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.034739454094292806, 0.33250620347394538, 0.052109181141439205, 0.0049627791563275434, 0.29032258064516131, 0.28287841191066998, 0.0029411764705882353, 0.041176470588235294, 0.25294117647058822, 0.69999999999999996, 0.0029411764705882353, 0.028089887640449437, 0.0056179775280898875, 0.24719101123595505, 0.6910112359550562, 0.011235955056179775, 0.016853932584269662, 1.0, 1.0, 0.96581196581196582, 0.0085470085470085479, 0.017094017094017096, 0.94999999999999996, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98076923076923073, 0.96153846153846156, 1.0, 0.88888888888888884, 0.67469879518072284, 0.19277108433734941, 0.084337349397590355, 0.042168674698795178, 1.0, 0.96666666666666667, 0.016666666666666666, 1.0, 0.97916666666666663, 0.95833333333333337, 0.21296296296296297, 0.20370370370370369, 0.57870370370370372, 0.0046296296296296294, 0.98882681564245811, 0.0055865921787709499, 0.74829931972789121, 0.020408163265306121, 0.22448979591836735, 1.0, 1.0, 0.4238095238095238, 0.48333333333333334, 0.066666666666666666, 0.021428571428571429, 0.0047619047619047623, 0.93333333333333335, 1.0, 0.072072072072072071, 0.90990990990990994, 0.0090090090090090089, 1.0, 1.0, 1.0, 0.099462365591397844, 0.26344086021505375, 0.44086021505376344, 0.12096774193548387, 0.075268817204301078, 1.0, 0.042168674698795178, 0.006024096385542169, 0.1746987951807229, 0.77710843373493976, 0.95238095238095233, 0.7192982456140351, 0.27850877192982454, 0.26455026455026454, 0.63227513227513232, 0.0052910052910052907, 0.0026455026455026454, 0.092592592592592587, 0.92307692307692313, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.026315789473684209, 0.94736842105263153, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.032258064516129031, 0.93548387096774188, 1.0, 0.94736842105263153, 1.0, 0.94736842105263153, 1.0, 0.94736842105263153, 1.0, 1.0, 1.0, 1.0, 0.03896103896103896, 0.064935064935064929, 0.88311688311688308, 1.0, 1.0, 0.91111111111111109, 0.011111111111111112, 0.066666666666666666, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.044444444444444446, 0.93333333333333335, 1.0, 1.0, 1.0, 0.60869565217391308, 0.094861660079051377, 0.1225296442687747, 0.16600790513833993, 0.003952569169960474, 1.0, 1.0, 1.0, 1.0, 0.021113243761996161, 0.059500959692898273, 0.4875239923224568, 0.42994241842610365, 1.0, 1.0, 0.072289156626506021, 0.89156626506024095, 0.024096385542168676, 0.0021276595744680851, 0.36382978723404258, 0.41489361702127658, 0.059574468085106386, 0.010638297872340425, 0.14680851063829786, 0.96491228070175439, 0.017543859649122806, 0.84985835694050993, 0.022662889518413599, 0.0028328611898016999, 0.12181303116147309, 0.060240963855421686, 0.92771084337349397, 1.0, 1.0, 0.0098039215686274508, 0.89215686274509809, 0.0098039215686274508, 0.0098039215686274508, 0.068627450980392163, 0.33540372670807456, 0.055900621118012424, 0.59627329192546585, 0.012422360248447204, 0.94285714285714284, 0.0057142857142857143, 0.028571428571428571, 0.017142857142857144, 1.0, 0.014925373134328358, 0.94029850746268662, 0.029850746268656716, 1.0, 1.0, 1.0, 1.0, 0.9107142857142857, 0.017857142857142856, 0.053571428571428568, 1.0, 0.012987012987012988, 0.88311688311688308, 0.090909090909090912, 1.0, 0.052631578947368418, 0.013157894736842105, 0.875, 0.052631578947368418, 1.0, 1.0, 0.96551724137931039, 0.92982456140350878, 0.017543859649122806, 0.035087719298245612, 1.0, 0.012658227848101266, 0.15189873417721519, 0.12658227848101267, 0.67088607594936711, 0.031645569620253167, 0.9375, 0.97058823529411764, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.014084507042253521, 0.971830985915493, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94444444444444442, 1.0, 0.0016750418760469012, 0.24288107202680068, 0.056951423785594639, 0.47571189279731996, 0.15912897822445563, 0.061976549413735343, 0.97499999999999998, 0.967741935483871, 1.0, 0.11734693877551021, 0.25, 0.020408163265306121, 0.61224489795918369, 0.060344827586206899, 0.060344827586206899, 0.0086206896551724137, 0.034482758620689655, 0.051724137931034482, 0.77586206896551724, 1.0, 0.66548042704626331, 0.20284697508896798, 0.074733096085409248, 0.014234875444839857, 0.039145907473309607, 1.0, 0.97297297297297303, 1.0, 1.0, 0.94117647058823528, 1.0, 1.0, 1.0, 0.984375, 1.0, 0.028571428571428571, 0.95714285714285718, 1.0, 1.0, 0.90196078431372551, 0.019607843137254902, 0.058823529411764705, 1.0, 1.0, 0.063063063063063057, 0.25450450450450451, 0.42117117117117114, 0.19369369369369369, 0.015765765765765764, 0.0518018018018018, 0.021276595744680851, 0.91489361702127658, 0.042553191489361701, 0.036496350364963501, 0.82481751824817517, 0.058394160583941604, 0.029197080291970802, 0.043795620437956206, 0.039408866995073892, 0.088669950738916259, 0.86206896551724133, 0.0049261083743842365, 1.0, 0.94117647058823528, 0.92000000000000004, 0.066666666666666666, 1.0, 1.0, 1.0, 1.0, 0.9885057471264368, 0.94444444444444442, 1.0, 1.0, 0.05844155844155844, 0.74025974025974028, 0.18181818181818182, 0.012987012987012988, 0.12254335260115606, 0.0023121387283236996, 0.23930635838150288, 0.086705202312138727, 0.54682080924855492, 0.0011560693641618498, 1.0, 0.068037974683544306, 0.0007911392405063291, 0.14715189873417722, 0.018987341772151899, 0.74129746835443033, 0.023734177215189875, 1.0, 0.053030303030303032, 0.63257575757575757, 0.14015151515151514, 0.007575757575757576, 0.16287878787878787, 0.25589225589225589, 0.013468013468013467, 0.070707070707070704, 0.65993265993265993, 1.0, 0.99115044247787609, 0.016666666666666666, 0.93333333333333335, 0.033333333333333333, 1.0, 0.036363636363636362, 0.045454545454545456, 0.022727272727272728, 0.036363636363636362, 0.095454545454545459, 0.27272727272727271, 0.48636363636363639, 0.95652173913043481, 0.05027932960893855, 0.20949720670391062, 0.46648044692737428, 0.20670391061452514, 0.058659217877094973, 0.0055865921787709499, 1.0, 0.95999999999999996, 1.0, 1.0, 0.0096153846153846159, 0.96153846153846156, 0.019230769230769232, 0.27956989247311825, 0.16487455197132617, 0.0035842293906810036, 0.54480286738351258, 0.0035842293906810036, 0.0022172949002217295, 0.024390243902439025, 0.077605321507760533, 0.270509977827051, 0.015521064301552107, 0.004434589800443459, 0.55875831485587579, 0.044345898004434593, 1.0, 1.0, 1.0, 1.0, 0.021739130434782608, 0.95652173913043481, 0.43411927877947293, 0.0055478502080443829, 0.16782246879334259, 0.39112343966712898, 0.98947368421052628, 0.16822429906542055, 0.0046728971962616819, 0.81775700934579443, 0.0046728971962616819, 1.0, 0.984375, 0.95238095238095233, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.87878787878787878, 0.060606060606060608, 0.004329004329004329, 0.051948051948051951, 0.94999999999999996, 1.0, 1.0, 1.0, 1.0, 0.074999999999999997, 0.17499999999999999, 0.7416666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 0.034482758620689655, 0.034482758620689655, 0.89655172413793105, 0.9555555555555556, 0.022222222222222223, 0.0083945435466946487, 0.23714585519412382, 0.26337880377754458, 0.48058761804826861, 0.0094438614900314802, 1.0, 0.96875, 0.97777777777777775, 0.041666666666666664, 0.94444444444444442, 1.0, 1.0, 0.90163934426229508, 0.065573770491803282, 0.016393442622950821, 0.010471204188481676, 0.7172774869109948, 0.026178010471204188, 0.20942408376963351, 0.010471204188481676, 0.020942408376963352, 0.0079470198675496689, 0.53774834437086094, 0.04105960264900662, 0.41192052980132449, 0.21014492753623187, 0.78260869565217395, 1.0, 1.0, 0.83791208791208793, 0.15384615384615385, 0.008241758241758242, 1.0, 1.0, 0.9642857142857143, 1.0, 0.94117647058823528, 1.0, 0.94444444444444442, 1.0, 0.0080645161290322578, 0.12903225806451613, 0.84677419354838712, 0.0080645161290322578, 1.0, 1.0, 1.0, 0.086538461538461536, 0.89423076923076927, 0.0096153846153846159, 1.0, 0.11811023622047244, 0.81627296587926512, 0.013123359580052493, 0.0026246719160104987, 0.047244094488188976, 0.95652173913043481, 1.0, 0.97826086956521741, 0.53937007874015752, 0.20866141732283464, 0.11023622047244094, 0.13779527559055119, 0.014814814814814815, 0.014814814814814815, 0.029629629629629631, 0.029629629629629631, 0.90370370370370368, 1.0, 0.17719298245614035, 0.036842105263157891, 0.22280701754385965, 0.23157894736842105, 0.091228070175438603, 0.24210526315789474, 0.95454545454545459, 0.030303030303030304, 0.97058823529411764, 0.1111111111111111, 0.030303030303030304, 0.020202020202020204, 0.050505050505050504, 0.77777777777777779, 0.29530916844349681, 0.041577825159914712, 0.034115138592750532, 0.16204690831556504, 0.3251599147121535, 0.007462686567164179, 0.13326226012793177, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94736842105263153, 1.0, 1.0, 1.0, 1.0, 0.040404040404040407, 0.81818181818181823, 0.090909090909090912, 0.040404040404040407, 0.0044843049327354259, 0.79073243647234681, 0.19282511210762332, 0.0044843049327354259, 0.0044843049327354259, 0.0014947683109118087, 0.032258064516129031, 0.010752688172043012, 0.87096774193548387, 0.075268817204301078, 1.0, 1.0, 1.0, 1.0, 0.091954022988505746, 0.75862068965517238, 0.14367816091954022, 0.94117647058823528, 1.0, 1.0, 1.0, 0.96296296296296291, 0.024691358024691357, 1.0, 0.9285714285714286, 0.010416666666666666, 0.015625, 0.953125, 0.005208333333333333, 0.010416666666666666, 1.0, 0.88086642599277976, 0.0018050541516245488, 0.066787003610108309, 0.048736462093862815, 0.16250000000000001, 0.18437500000000001, 0.640625, 0.0062500000000000003, 0.0031250000000000002, 1.0, 0.95833333333333337, 1.0, 1.0, 0.94117647058823528, 0.81026137463697967, 0.049370764762826716, 0.0087124878993223628, 0.010648596321393998, 0.00096805421103581804, 0.00096805421103581804, 0.11810261374636979, 0.88888888888888884, 0.023809523809523808, 0.0079365079365079361, 0.079365079365079361, 1.0, 0.095238095238095233, 0.037037037037037035, 0.39153439153439151, 0.39153439153439151, 0.0052910052910052907, 0.077601410934744264, 1.0, 0.10714285714285714, 0.21428571428571427, 0.67261904761904767, 0.0038461538461538464, 0.30384615384615382, 0.56923076923076921, 0.096153846153846159, 0.026923076923076925, 0.74251497005988021, 0.023952095808383235, 0.22155688622754491, 0.0059880239520958087, 0.96153846153846156, 1.0, 0.41876046901172531, 0.46398659966499162, 0.0033500837520938024, 0.048576214405360134, 0.065326633165829151, 0.91666666666666663, 0.013888888888888888, 0.055555555555555552, 1.0, 0.057142857142857141, 0.0071428571428571426, 0.41785714285714287, 0.51071428571428568, 0.0035714285714285713, 0.17214191852825231, 0.017082785808147174, 0.047306176084099871, 0.68462549277266749, 0.013140604467805518, 0.0039421813403416554, 0.060446780551905388, 0.96551724137931039, 0.967741935483871, 0.023411371237458192, 0.05016722408026756, 0.39130434782608697, 0.53511705685618727, 0.01282051282051282, 0.019230769230769232, 0.00641025641025641, 0.94230769230769229, 0.01282051282051282, 0.97222222222222221, 0.013888888888888888, 0.86330935251798557, 0.021582733812949641, 0.1079136690647482, 0.0035971223021582736, 0.035714285714285712, 0.9285714285714286, 0.94029850746268662, 0.014925373134328358, 0.014925373134328358, 0.014925373134328358, 1.0, 1.0, 1.0, 1.0, 0.037037037037037035, 0.94444444444444442, 0.00029568302779420464, 0.039325842696629212, 0.00029568302779420464, 0.0079834417504435241, 0.39325842696629215, 0.00029568302779420464, 0.55795387344766412, 0.00029568302779420464, 1.0, 1.0, 0.77738515901060068, 0.11307420494699646, 0.084805653710247356, 0.021201413427561839, 0.98360655737704916, 1.0, 0.054545454545454543, 0.90909090909090906, 0.027272727272727271, 1.0, 0.98333333333333328, 1.0, 1.0, 1.0, 0.065853658536585369, 0.35365853658536583, 0.092682926829268292, 0.053658536585365853, 0.43170731707317073, 0.29322709163346611, 0.38725099601593627, 0.06135458167330677, 0.0055776892430278889, 0.18964143426294822, 0.033466135458167331, 0.028685258964143426, 1.0, 0.96875, 0.015625, 0.94999999999999996, 1.0, 0.070588235294117646, 0.91764705882352937, 1.0, 0.95999999999999996, 0.02, 1.0, 1.0, 1.0, 1.0, 0.19626168224299065, 0.028037383177570093, 0.0093457943925233638, 0.57943925233644855, 0.1822429906542056, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98461538461538467, 0.005434782608695652, 0.42391304347826086, 0.57065217391304346, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.98571428571428577, 1.0, 1.0, 0.064516129032258063, 0.90322580645161288, 1.0, 1.0, 1.0, 0.021390374331550801, 0.032085561497326207, 0.39037433155080214, 0.53475935828877008, 0.021390374331550801, 1.0, 0.97560975609756095, 1.0, 0.967741935483871, 0.98461538461538467, 1.0, 0.94117647058823528, 0.0098039215686274508, 0.18954248366013071, 0.73202614379084963, 0.06535947712418301, 0.0334075723830735, 0.36302895322939865, 0.14253897550111358, 0.23608017817371937, 0.22494432071269488, 1.0, 0.97916666666666663, 0.75657894736842102, 0.026315789473684209, 0.0065789473684210523, 0.026315789473684209, 0.18092105263157895, 1.0, 1.0, 1.0, 0.9375, 0.10191082802547771, 0.12738853503184713, 0.7133757961783439, 0.057324840764331211, 0.83495145631067957, 0.048543689320388349, 0.087378640776699032, 0.019417475728155338, 1.0, 1.0, 1.0, 1.0, 1.0, 0.033333333333333333, 0.93333333333333335, 0.9375, 0.059171597633136092, 0.92899408284023666, 0.0059171597633136093, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96666666666666667, 0.016393442622950821, 0.90163934426229508, 0.016393442622950821, 0.049180327868852458, 1.0, 1.0, 1.0, 1.0, 0.096385542168674704, 0.024096385542168676, 0.69277108433734935, 0.012048192771084338, 0.1144578313253012, 0.060240963855421686, 0.84976525821596249, 0.14553990610328638, 0.97619047619047616, 0.011904761904761904, 0.93333333333333335, 0.033333333333333333, 0.97101449275362317, 0.014492753623188406, 0.010810810810810811, 0.33513513513513515, 0.28828828828828829, 0.36396396396396397, 0.059322033898305086, 0.93220338983050843, 1.0, 1.0, 0.96969696969696972, 0.022727272727272728, 0.94318181818181823, 0.011363636363636364, 0.011363636363636364, 1.0, 1.0, 0.0034904013961605585, 0.0017452006980802793, 0.56369982547993014, 0.4293193717277487, 1.0, 1.0, 0.041884816753926704, 0.20418848167539266, 0.66492146596858637, 0.020942408376963352, 0.068062827225130892, 1.0, 1.0, 1.0, 1.0, 0.94985250737463123, 0.0471976401179941, 0.94736842105263153, 1.0, 0.94444444444444442, 1.0, 0.021834061135371178, 0.9606986899563319, 0.013100436681222707, 0.90277777777777779, 0.069444444444444448, 0.013888888888888888, 1.0, 0.171875, 0.015625, 0.015625, 0.14732142857142858, 0.002232142857142857, 0.6183035714285714, 0.026785714285714284, 0.002232142857142857, 0.031088082901554404, 0.43523316062176165, 0.42746113989637308, 0.064766839378238336, 0.038860103626943004, 1.0, 1.0, 1.0, 0.7858744394618834, 0.19282511210762332, 0.0011210762331838565, 0.020179372197309416, 1.0, 1.0, 0.88888888888888884, 1.0, 1.0, 0.88709677419354838, 0.093548387096774197, 0.016129032258064516, 0.11708860759493671, 0.70886075949367089, 0.15822784810126583, 0.0031645569620253164, 0.0094936708860759497, 1.0, 1.0, 0.73876871880199668, 0.17138103161397669, 0.0083194675540765387, 0.016638935108153077, 0.064891846921797003, 1.0, 0.82978723404255317, 0.12929623567921442, 0.018003273322422259, 0.022913256955810146, 0.99082568807339455, 0.95238095238095233, 1.0, 0.97435897435897434, 0.83620689655172409, 0.051724137931034482, 0.034482758620689655, 0.0086206896551724137, 0.060344827586206899, 0.07746478873239436, 0.75352112676056338, 0.035211267605633804, 0.049295774647887321, 0.07746478873239436, 1.0, 1.0, 0.88421052631578945, 0.031578947368421054, 0.073684210526315783, 0.007246376811594203, 0.09420289855072464, 0.79528985507246375, 0.10144927536231885, 0.30303030303030304, 0.47272727272727272, 0.0060606060606060606, 0.21414141414141413, 0.0020202020202020202, 0.87951807228915657, 0.072289156626506021, 0.03614457831325301, 1.0, 0.084656084656084651, 0.021164021164021163, 0.12698412698412698, 0.71957671957671954, 0.0052910052910052907, 0.037037037037037035, 0.020408163265306121, 0.91836734693877553, 0.020408163265306121, 0.020408163265306121, 0.032258064516129031, 0.93548387096774188, 0.83443708609271527, 0.026490066225165563, 0.013245033112582781, 0.059602649006622516, 0.059602649006622516, 0.0066225165562913907, 0.90000000000000002, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.91666666666666663, 1.0, 1.0, 0.95999999999999996, 0.0070921985815602835, 0.070921985815602842, 0.900709219858156, 0.0070921985815602835, 0.0070921985815602835, 1.0, 0.9375, 1.0, 1.0, 1.0, 0.041474654377880185, 0.0092165898617511521, 0.023041474654377881, 0.76497695852534564, 0.023041474654377881, 0.08294930875576037, 0.055299539170506916, 1.0, 1.0, 0.10416666666666667, 0.125, 0.013888888888888888, 0.74305555555555558, 1.0, 0.91666666666666663, 0.95454545454545459, 0.013630731102850062, 0.56753407682775714, 0.32094175960346966, 0.096654275092936809, 1.0, 1.0, 0.060606060606060608, 0.86363636363636365, 0.015151515151515152, 0.045454545454545456, 1.0, 1.0, 0.98484848484848486, 0.91666666666666663, 0.97058823529411764, 0.94285714285714284, 0.028571428571428571, 0.7029478458049887, 0.26984126984126983, 0.018140589569160998, 0.0068027210884353739, 1.0, 0.984375, 1.0, 0.95454545454545459, 1.0, 0.0070921985815602835, 0.42198581560283688, 0.081560283687943269, 0.47872340425531917, 0.0070921985815602835, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.31144465290806755, 0.097560975609756101, 0.20075046904315197, 0.024390243902439025, 0.36585365853658536, 0.049019607843137254, 0.86274509803921573, 0.0098039215686274508, 0.0098039215686274508, 0.058823529411764705, 0.99642857142857144, 1.0, 1.0, 0.069832402234636867, 0.23743016759776536, 0.68994413407821231, 0.96875, 0.95999999999999996, 1.0, 0.94736842105263153, 1.0, 1.0, 1.0, 0.3949771689497717, 0.15525114155251141, 0.37214611872146119, 0.068493150684931503, 0.0091324200913242004, 0.6053639846743295, 0.39080459770114945, 1.0, 0.018518518518518517, 0.90740740740740744, 0.055555555555555552, 1.0, 0.015345268542199489, 0.0076726342710997444, 0.0076726342710997444, 0.0038363171355498722, 0.0051150895140664966, 0.9578005115089514, 0.0012787723785166241, 0.9642857142857143, 0.069565217391304349, 0.91304347826086951, 0.0086956521739130436, 0.98333333333333328, 1.0, 1.0, 0.93333333333333335, 0.037634408602150539, 0.12096774193548387, 0.44086021505376344, 0.13709677419354838, 0.20161290322580644, 0.059139784946236562, 0.0026881720430107529, 1.0, 0.15920398009950248, 0.17910447761194029, 0.65671641791044777, 1.0, 1.0, 1.0, 0.087499999999999994, 0.90000000000000002, 0.94736842105263153, 0.31666666666666665, 0.68333333333333335, 1.0, 1.0, 0.034482758620689655, 0.93103448275862066, 1.0, 0.94117647058823528, 0.014705882352941176, 0.029411764705882353, 1.0, 0.070707070707070704, 0.86868686868686873, 0.050505050505050504, 1.0, 1.0, 1.0, 1.0, 0.82550335570469802, 0.026845637583892617, 0.12751677852348994, 0.0067114093959731542, 0.0067114093959731542, 1.0, 1.0, 0.57889990982867445, 0.3101893597835888, 0.00090171325518485117, 0.109107303877367, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.006024096385542169, 0.81927710843373491, 0.12650602409638553, 0.042168674698795178, 1.0, 0.96551724137931039, 0.96666666666666667, 0.93333333333333335, 1.0, 0.088397790055248615, 0.850828729281768, 0.016574585635359115, 0.038674033149171269, 0.19626168224299065, 0.79439252336448596, 1.0, 0.94736842105263153, 1.0, 1.0, 1.0, 0.002008032128514056, 0.096385542168674704, 0.87349397590361444, 0.028112449799196786, 0.0029154518950437317, 0.084548104956268216, 0.011661807580174927, 0.0029154518950437317, 0.5276967930029155, 0.3498542274052478, 0.020408163265306121, 1.0, 1.0, 1.0, 0.94117647058823528, 1.0, 1.0, 1.0, 1.0, 0.95454545454545459, 1.0, 0.045454545454545456, 0.90909090909090906, 1.0, 0.9375, 1.0, 1.0, 0.020833333333333332, 0.95833333333333337, 1.0, 0.91666666666666663, 0.066666666666666666, 0.024691358024691357, 0.96296296296296291, 1.0, 1.0, 1.0, 1.0, 0.1152073732718894, 0.052995391705069124, 0.002304147465437788, 0.17972350230414746, 0.10829493087557604, 0.54147465437788023, 0.17585301837270342, 0.013123359580052493, 0.33333333333333331, 0.47769028871391078, 1.0, 0.9285714285714286, 1.0, 1.0, 0.9375, 0.031746031746031744, 0.015873015873015872, 0.93650793650793651, 1.0, 0.029126213592233011, 0.96763754045307449, 1.0, 0.90909090909090906, 1.0, 1.0, 0.043209876543209874, 0.0061728395061728392, 0.024691358024691357, 0.91975308641975306, 0.62653562653562656, 0.076167076167076173, 0.0049140049140049139, 0.054054054054054057, 0.20393120393120392, 0.031941031941031942, 0.0024570024570024569, 0.94117647058823528, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.015810276679841896, 0.81818181818181823, 0.003952569169960474, 0.13438735177865613, 0.023715415019762844, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0035714285714285713, 0.25357142857142856, 0.053571428571428568, 0.52500000000000002, 0.16428571428571428, 0.96721311475409832, 0.016393442622950821, 1.0, 0.94999999999999996, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.020408163265306121, 0.89795918367346939, 0.020408163265306121, 0.040816326530612242, 0.090322580645161285, 0.025806451612903226, 0.25161290322580643, 0.62580645161290327, 1.0, 1.0, 1.0, 0.28849270664505672, 0.037277147487844407, 0.024311183144246355, 0.51053484602917343, 0.0032414910858995136, 0.13452188006482982, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.062745098039215685, 0.71764705882352942, 0.011764705882352941, 0.0039215686274509803, 0.16862745098039217, 0.031372549019607843, 0.0046838407494145199, 0.01873536299765808, 0.36299765807962531, 0.61124121779859486, 0.083798882681564241, 0.0055865921787709499, 0.0055865921787709499, 0.67597765363128492, 0.22905027932960895, 0.95238095238095233, 0.023809523809523808, 0.40343347639484978, 0.0021459227467811159, 0.12660944206008584, 0.43133047210300429, 0.034334763948497854, 1.0, 0.042553191489361701, 0.93617021276595747, 0.96153846153846156, 0.0047393364928909956, 0.014218009478672985, 0.0047393364928909956, 0.94312796208530802, 0.02843601895734597, 1.0, 0.087912087912087919, 0.90109890109890112, 1.0, 0.20567375886524822, 0.73049645390070927, 0.0070921985815602835, 0.049645390070921988, 1.0, 0.98181818181818181, 1.0, 0.027777777777777776, 0.94444444444444442, 0.88888888888888884, 1.0, 0.011904761904761904, 0.86309523809523814, 0.095238095238095233, 0.023809523809523808, 0.046948356807511735, 0.10328638497652583, 0.13145539906103287, 0.032863849765258218, 0.0046948356807511738, 0.018779342723004695, 0.6619718309859155, 0.98245614035087714, 1.0, 0.95999999999999996, 1.0, 0.98148148148148151, 0.0092592592592592587, 0.033333333333333333, 0.93333333333333335, 1.0, 0.94117647058823528, 1.0, 0.98076923076923073, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.011904761904761904, 0.97619047619047616, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94736842105263153, 1.0, 0.0060975609756097563, 0.03048780487804878, 0.95731707317073167, 0.94444444444444442, 1.0, 0.32570422535211269, 0.66549295774647887, 0.0070422535211267607, 0.055555555555555552, 0.91666666666666663, 1.0, 1.0, 1.0, 0.55212355212355213, 0.22393822393822393, 0.22393822393822393, 1.0, 0.05921052631578947, 0.072368421052631582, 0.0065789473684210523, 0.013157894736842105, 0.84210526315789469, 0.94444444444444442, 0.020833333333333332, 0.027777777777777776, 1.0, 1.0, 1.0, 0.017241379310344827, 0.96551724137931039, 1.0, 0.56976744186046513, 0.073643410852713184, 0.1434108527131783, 0.11627906976744186, 0.0077519379844961239, 0.089147286821705432, 1.0, 1.0, 0.97142857142857142, 0.96296296296296291, 0.12820512820512819, 0.78632478632478631, 0.085470085470085472, 0.024691358024691357, 0.12757201646090535, 0.56378600823045266, 0.27983539094650206, 0.95652173913043481, 0.016286644951140065, 0.98045602605863191, 0.96153846153846156, 1.0, 1.0, 0.96153846153846156, 1.0, 1.0, 1.0, 1.0, 0.0076045627376425855, 0.0038022813688212928, 0.61216730038022815, 0.0038022813688212928, 0.36121673003802279, 0.0076045627376425855, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94999999999999996, 0.88297872340425532, 0.10638297872340426, 0.064000000000000001, 0.0040000000000000001, 0.872, 0.056000000000000001, 0.063106796116504854, 0.20873786407766989, 0.72330097087378642, 0.9464285714285714, 0.017857142857142856, 0.017857142857142856, 1.0, 1.0, 1.0, 1.0, 0.91666666666666663, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9642857142857143, 1.0, 0.99626865671641796, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96875, 1.0, 1.0, 1.0, 1.0, 0.038011695906432746, 0.36842105263157893, 0.58479532163742687, 0.008771929824561403, 1.0, 1.0, 1.0, 0.63654485049833887, 0.00066445182724252495, 0.081727574750830562, 0.0013289036544850499, 0.26378737541528241, 0.014617940199335547, 0.99523809523809526, 0.92307692307692313, 0.018181818181818181, 0.96363636363636362, 0.064952638700947224, 0.36400541271989173, 0.0027063599458728013, 0.0040595399188092015, 0.47225981055480382, 0.089309878213802429, 0.0013531799729364006, 1.0, 1.0, 0.95999999999999996, 1.0, 0.98611111111111116, 1.0, 1.0, 0.94117647058823528, 1.0, 0.15492957746478872, 0.79342723004694837, 0.042253521126760563, 0.0046948356807511738, 1.0, 0.15476190476190477, 0.0059523809523809521, 0.6785714285714286, 0.15476190476190477, 0.91666666666666663, 1.0, 1.0, 0.95454545454545459, 1.0, 0.97916666666666663, 1.0, 0.016908212560386472, 0.73671497584541068, 0.24396135265700483, 0.98039215686274506, 0.92753623188405798, 0.043478260869565216, 0.014492753623188406, 0.019292604501607719, 0.096463022508038579, 0.65916398713826363, 0.01607717041800643, 0.067524115755627015, 0.12218649517684887, 0.0032154340836012861, 0.012861736334405145, 1.0, 1.0, 0.90000000000000002, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9642857142857143, 1.0, 1.0, 0.94871794871794868, 0.02564102564102564, 1.0, 1.0, 0.086206896551724144, 0.025862068965517241, 0.0086206896551724137, 0.87068965517241381, 1.0, 0.97297297297297303, 1.0, 0.0027855153203342618, 0.74094707520891367, 0.23398328690807799, 0.0027855153203342618, 0.016713091922005572, 0.95999999999999996, 1.0, 0.14166666666666666, 0.84999999999999998, 0.9285714285714286, 0.93333333333333335, 1.0, 1.0, 1.0, 0.072566371681415928, 0.0070796460176991149, 0.0088495575221238937, 0.90796460176991145, 0.0017699115044247787, 0.94999999999999996, 1.0, 1.0, 0.013888888888888888, 0.97222222222222221, 0.0089285714285714281, 0.035714285714285712, 0.9464285714285714, 1.0, 0.10431654676258993, 0.38848920863309355, 0.5, 0.0071942446043165471, 1.0, 1.0, 1.0, 0.95348837209302328, 0.023255813953488372, 1.0, 0.76344086021505375, 0.0015360983102918587, 0.23348694316436253, 1.0, 0.090425531914893623, 0.1276595744680851, 0.053191489361702128, 0.71808510638297873, 0.018633540372670808, 0.80124223602484468, 0.080745341614906832, 0.068322981366459631, 0.024844720496894408, 0.9642857142857143, 0.0089285714285714281, 0.0089285714285714281, 0.0089285714285714281, 1.0, 1.0, 1.0, 1.0, 1.0, 0.08050847457627118, 0.61864406779661019, 0.038135593220338986, 0.1228813559322034, 0.1228813559322034, 0.012711864406779662, 0.94444444444444442, 1.0, 0.033898305084745763, 0.084745762711864403, 0.86440677966101698, 1.0, 1.0, 1.0, 0.9375, 1.0, 1.0, 1.0, 1.0, 0.82089552238805974, 0.014925373134328358, 0.022388059701492536, 0.007462686567164179, 0.13432835820895522, 1.0, 1.0, 0.10784313725490197, 0.81372549019607843, 0.0098039215686274508, 0.049019607843137254, 0.0098039215686274508, 1.0, 1.0, 0.8902439024390244, 0.073170731707317069, 0.024390243902439025, 1.0, 1.0, 0.9375, 1.0, 0.018018018018018018, 0.86486486486486491, 0.11261261261261261, 1.0, 0.011235955056179775, 0.9157303370786517, 0.0449438202247191, 0.02247191011235955, 1.0, 0.19072164948453607, 0.80412371134020622, 1.0, 1.0, 1.0, 0.88888888888888884, 0.092592592592592587, 1.0, 0.078260869565217397, 0.034782608695652174, 0.052173913043478258, 0.80869565217391304, 0.017391304347826087, 1.0, 1.0, 1.0, 1.0, 1.0, 0.12, 0.22800000000000001, 0.59999999999999998, 0.051999999999999998, 1.0, 0.66161616161616166, 0.19696969696969696, 0.010101010101010102, 0.12626262626262627, 1.0, 1.0, 1.0, 0.0062893081761006293, 0.96226415094339623, 0.0062893081761006293, 0.018867924528301886, 0.90163934426229508, 0.081967213114754092, 1.0, 0.025000000000000001, 0.92500000000000004, 0.025000000000000001, 1.0, 1.0, 1.0, 0.16091954022988506, 0.82758620689655171, 0.94117647058823528, 0.89610389610389607, 0.090909090909090912, 0.0064935064935064939, 0.024390243902439025, 0.06097560975609756, 0.012195121951219513, 0.8902439024390244, 0.040000000000000001, 0.60799999999999998, 0.0080000000000000002, 0.040000000000000001, 0.29999999999999999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0091743119266055051, 0.36238532110091742, 0.62385321100917435, 1.0, 1.0, 1.0, 0.9375, 0.96153846153846156, 0.96666666666666667, 1.0, 0.41787439613526572, 0.004830917874396135, 0.5748792270531401, 0.038461538461538464, 0.83076923076923082, 0.030769230769230771, 0.061538461538461542, 0.0076923076923076927, 0.015384615384615385, 0.0076923076923076927, 1.0, 0.31420765027322406, 0.027322404371584699, 0.016393442622950821, 0.46448087431693991, 0.17486338797814208, 0.56803797468354433, 0.18354430379746836, 0.068037974683544306, 0.13607594936708861, 0.011075949367088608, 0.031645569620253167, 0.96363636363636362, 0.018181818181818181, 0.95652173913043481, 0.95833333333333337, 0.13445378151260504, 0.76890756302521013, 0.088235294117647065, 0.0042016806722689074, 1.0, 0.95652173913043481, 0.014851485148514851, 0.0049504950495049506, 0.77722772277227725, 0.19801980198019803, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.81039755351681952, 0.021406727828746176, 0.012232415902140673, 0.051987767584097858, 0.01834862385321101, 0.073394495412844041, 0.0061162079510703364, 0.0030581039755351682, 1.0, 1.0, 0.9375, 0.1099476439790576, 0.13612565445026178, 0.005235602094240838, 0.04712041884816754, 0.015706806282722512, 0.68586387434554974, 0.94999999999999996, 1.0, 1.0, 1.0, 1.0, 0.011299435028248588, 0.12429378531073447, 0.84745762711864403, 0.011299435028248588, 1.0, 1.0, 1.0, 0.97619047619047616, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94117647058823528, 0.011764705882352941, 0.011764705882352941, 0.023529411764705882, 0.91666666666666663, 1.0, 0.0021186440677966102, 0.0021186440677966102, 0.32203389830508472, 0.67161016949152541, 1.0, 0.018292682926829267, 0.69512195121951215, 0.1524390243902439, 0.12804878048780488, 1.0, 0.95454545454545459, 1.0, 1.0, 1.0, 1.0, 0.66396761133603244, 0.25506072874493929, 0.012145748987854251, 0.052631578947368418, 0.004048582995951417, 0.004048582995951417, 0.0080971659919028341, 1.0, 1.0, 1.0, 0.31016042780748665, 0.66844919786096257, 0.0106951871657754, 0.0053475935828877002, 0.0075471698113207548, 0.53584905660377358, 0.45283018867924529, 1.0, 0.022727272727272728, 0.022727272727272728, 0.93181818181818177, 0.95454545454545459, 1.0, 0.10256410256410256, 0.87179487179487181, 0.9375, 0.93142857142857138, 0.0057142857142857143, 0.028571428571428571, 0.028571428571428571, 0.0046296296296296294, 0.099537037037037035, 0.75694444444444442, 0.0023148148148148147, 0.13425925925925927, 0.65551839464882944, 0.27424749163879597, 0.066889632107023408, 1.0, 0.06344827586206897, 0.0027586206896551722, 0.0068965517241379309, 0.92551724137931035, 0.097826086956521743, 0.83695652173913049, 0.05434782608695652, 0.9285714285714286, 1.0, 0.040404040404040407, 0.80808080808080807, 0.15151515151515152, 1.0, 1.0, 1.0, 0.19333816075307747, 0.0057929036929761039, 0.041998551774076756, 0.0094134685010861703, 0.74873280231716144, 0.0070422535211267607, 0.049295774647887321, 0.19718309859154928, 0.014084507042253521, 0.056338028169014086, 0.66901408450704225, 0.94117647058823528, 1.0, 0.0065217391304347823, 0.66304347826086951, 0.069565217391304349, 0.010869565217391304, 0.24130434782608695, 0.0065217391304347823, 0.0021739130434782609, 1.0, 1.0, 0.41921397379912662, 0.048034934497816595, 0.0043668122270742356, 0.056768558951965066, 0.46724890829694321, 0.967741935483871, 1.0, 1.0, 1.0, 1.0, 0.90123456790123457, 0.024691358024691357, 0.012345679012345678, 0.049382716049382713, 1.0, 1.0, 0.93055555555555558, 0.0069444444444444441, 0.0069444444444444441, 0.048611111111111112, 1.0, 0.16914498141263939, 0.32527881040892193, 0.027881040892193308, 0.47769516728624534, 0.16806722689075632, 0.0084033613445378148, 0.01680672268907563, 0.79831932773109249, 1.0, 0.036809815950920248, 0.76073619631901845, 0.19631901840490798, 0.96153846153846156, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.94444444444444442, 0.22155688622754491, 0.3772455089820359, 0.39820359281437123, 0.93333333333333335, 1.0, 0.10638297872340426, 0.88297872340425532, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.95714285714285718, 0.014285714285714285, 0.014285714285714285, 0.94444444444444442, 1.0, 1.0, 1.0, 0.984375, 1.0, 1.0, 1.0, 0.039711191335740074, 0.061371841155234655, 0.021660649819494584, 0.61010830324909748, 0.0072202166064981952, 0.2563176895306859, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.027777777777777776, 0.94444444444444442, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9642857142857143, 0.97058823529411764, 1.0, 0.004048582995951417, 0.032388663967611336, 0.18623481781376519, 0.012145748987854251, 0.12550607287449392, 0.63562753036437247, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.96666666666666667, 0.060810810810810814, 0.040540540540540543, 0.020270270270270271, 0.11486486486486487, 0.0067567567567567571, 0.75, 0.95588235294117652, 0.022058823529411766, 0.014705882352941176, 1.0, 0.37109375, 0.171875, 0.00390625, 0.453125, 1.0, 0.95238095238095233, 1.0, 1.0, 0.015873015873015872, 0.93650793650793651, 0.031746031746031744, 0.9921875, 0.984375, 1.0, 1.0, 0.010752688172043012, 0.0053763440860215058, 0.60215053763440862, 0.064516129032258063, 0.31182795698924731, 0.050872093023255814, 0.0094476744186046506, 0.095203488372093026, 0.63444767441860461, 0.00072674418604651162, 0.20930232558139536, 0.95833333333333337, 0.88516746411483249, 0.0047846889952153108, 0.10526315789473684, 1.0, 1.0, 1.0, 0.13953488372093023, 0.8527131782945736, 1.0, 1.0, 1.0, 1.0, 0.11881188118811881, 0.42244224422442245, 0.30693069306930693, 0.14851485148514851, 1.0, 1.0, 0.012224938875305624, 0.0024449877750611247, 0.25183374083129584, 0.73105134474327627, 1.0, 1.0, 1.0, 1.0, 0.96153846153846156, 0.2921195652173913, 0.019021739130434784, 0.095108695652173919, 0.55434782608695654, 0.038043478260869568, 1.0, 0.01020408163265306, 0.01020408163265306, 0.91836734693877553, 0.051020408163265307, 1.0, 1.0, 0.11093502377179081, 0.042789223454833596, 0.036450079239302692, 0.16323296354992076, 0.6450079239302694, 0.75496688741721851, 0.026490066225165563, 0.066225165562913912, 0.14569536423841059, 0.048309178743961352, 0.057971014492753624, 0.15458937198067632, 0.68357487922705318, 0.01932367149758454, 0.036231884057971016, 0.46017699115044247, 0.094395280235988199, 0.053097345132743362, 0.088495575221238937, 0.2831858407079646, 0.017699115044247787, 0.83225806451612905, 0.0021505376344086021, 0.036559139784946237, 0.012903225806451613, 0.11397849462365592, 0.013513513513513514, 0.0945945945945946, 0.86486486486486491, 0.013513513513513514, 0.97142857142857142, 0.82677165354330706, 0.14960629921259844, 0.015748031496062992, 0.090909090909090912, 0.007575757575757576, 0.88636363636363635, 0.007575757575757576, 1.0, 1.0, 0.0065359477124183009, 0.15686274509803921, 0.83006535947712423, 0.010526315789473684, 0.97894736842105268, 1.0, 0.035398230088495575, 0.72123893805309736, 0.030973451327433628, 0.013274336283185841, 0.0044247787610619468, 0.19026548672566371, 0.10606060606060606, 0.22510822510822512, 0.32251082251082253, 0.34632034632034631, 1.0, 1.0, 1.0, 1.0, 0.026825633383010434, 0.52608047690014903, 0.42921013412816694, 0.017883755588673621, 1.0], \"Term\": [\"\\\"\", \"\\\"\", \"\\\"\", \"\\\"\", \"\\\"\", \"\\\"\", \"\\\"1884\", \"\\\"a\", \"\\\"a\", \"\\\"absolutely\", \"\\\"adelbert\", \"\\\"ah\", \"\\\"ah\", \"\\\"ah\", \"\\\"ah\", \"\\\"and\", \"\\\"and\", \"\\\"and\", \"\\\"and\", \"\\\"any\", \"\\\"apart\", \"\\\"as\", \"\\\"as\", \"\\\"bartholomew\", \"\\\"boys\", \"\\\"brixton\", \"\\\"but\", \"\\\"but\", \"\\\"but\", \"\\\"but\", \"\\\"but\", \"\\\"but\", \"\\\"certainly\", \"\\\"certainly\", \"\\\"count\", \"\\\"dorak\", \"\\\"down\", \"\\\"easier\", \"\\\"either\", \"\\\"facts\", \"\\\"granting\", \"\\\"guess\", \"\\\"hate\", \"\\\"heap\", \"\\\"i\", \"\\\"i\", \"\\\"i\", \"\\\"in\", \"\\\"in\", \"\\\"in\", \"\\\"in\", \"\\\"is\", \"\\\"is\", \"\\\"it\", \"\\\"it\", \"\\\"it\", \"\\\"lost\", \"\\\"mean\", \"\\\"mine\", \"\\\"miss\", \"\\\"move\", \"\\\"need\", \"\\\"plain\", \"\\\"porlock\", \"\\\"precisely\", \"\\\"pure\", \"\\\"rochester\", \"\\\"rubbish\", \"\\\"singular\", \"\\\"smith\", \"\\\"starts\", \"\\\"subject\", \"\\\"syracusan\", \"\\\"that\", \"\\\"that\", \"\\\"that\", \"\\\"that\", \"\\\"that\", \"\\\"the\", \"\\\"the\", \"\\\"the\", \"\\\"the\", \"\\\"the\", \"\\\"there\", \"\\\"there\", \"\\\"there\", \"\\\"there\", \"\\\"there\", \"\\\"there\", \"\\\"there\", \"\\\"toby\", \"\\\"tonga\", \"\\\"tropical\", \"\\\"tuxbury\", \"\\\"vermissa\", \"\\\"we\", \"\\\"we\", \"\\\"we\", \"\\\"we\", \"\\\"well\", \"\\\"well\", \"\\\"well\", \"\\\"well\", \"\\\"what\", \"\\\"what\", \"\\\"when\", \"\\\"when\", \"\\\"when\", \"\\\"where\", \"\\\"who\", \"\\\"who\", \"\\\"witness\", \"\\\"yes\", \"\\\"yes\", \"\\\"yes\", \"\\\"yes\", \"\\\"yes\", \"\\\"yes\", \"\\\"yes\", \"\\\"you\", \"\\\"you\", \"\\\"you\", \"\\\"you\", \"\\\"you\", \"\\\"you\", \"104\", \"1644\", \"1730\", \"17th\", \"1800\", \"1893\", \"27\", \"97163\", \"abandoned\", \"abated\", \"abetting\", \"abnormality\", \"absolutely\", \"absolutely\", \"absolutely\", \"absurdity\", \"abusing\", \"accepts\", \"account\", \"account\", \"account\", \"account\", \"accredited\", \"act\", \"act\", \"act\", \"act\", \"admeet\", \"administrator\", \"adults\", \"advancement\", \"adventure\", \"adventure\", \"adversary\", \"advocated\", \"afraid\", \"afraid\", \"afraid\", \"afraid\", \"age\", \"age\", \"age\", \"agent\", \"agent\", \"agent\", \"agent\", \"aggressor\", \"aggrieved\", \"agin\", \"agonizing\", \"ailing\", \"air\", \"air\", \"air\", \"air\", \"air\", \"allay\", \"allusion\", \"aloofness\", \"alters\", \"alton\", \"amenities\", \"angelic\", \"annuity\", \"answered\", \"answered\", \"answered\", \"anthropologists\", \"antipathy\", \"anxious\", \"anxious\", \"apocrypha\", \"apologize\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appearance\", \"appeared\", \"appeared\", \"appeared\", \"appeared\", \"appendix\", \"applauded\", \"appliances\", \"appreciating\", \"approaches\", \"approbation\", \"architect\", \"arkansas\", \"arm\", \"arm\", \"arrival\", \"arrival\", \"ashamed\", \"ashes\", \"ashes\", \"asked\", \"asked\", \"asked\", \"aspirations\", \"asprawl\", \"assassin\", \"assessment\", \"assure\", \"assure\", \"astonish\", \"attention\", \"attention\", \"attention\", \"attention\", \"attention\", \"attenuated\", \"auditors\", \"avowed\", \"babe\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"back\", \"badger\", \"bain\", \"baker\", \"baker\", \"baker\", \"ban\", \"bar\", \"barclay\", \"barclay\", \"baronet\", \"barred\", \"barrelled\", \"bart\", \"basic\", \"baskerville\", \"baskerville\", \"bat\", \"bather\", \"bear\", \"bear\", \"bear\", \"bear\", \"beastly\", \"beat\", \"beat\", \"beat\", \"beaune\", \"bed\", \"bed\", \"bedding\", \"beddoes\", \"began\", \"began\", \"began\", \"began\", \"begat\", \"beginnin\", \"bell\", \"bell\", \"bell\", \"bell\", \"bell\", \"bender\", \"beneath\", \"beneath\", \"beppo\", \"berkshires\", \"besought\", \"bewildering\", \"big\", \"big\", \"big\", \"births\", \"bitterly\", \"bitterns\", \"black\", \"black\", \"black\", \"blase\", \"blasting\", \"bleaker\", \"blessings\", \"blind\", \"blind\", \"blind\", \"blind\", \"blobs\", \"blood\", \"blood\", \"blood\", \"blood\", \"blood\", \"blossomed\", \"blossoming\", \"blount\", \"blushed\", \"blushing\", \"board\", \"boccaccio\", \"body\", \"body\", \"body\", \"boilers\", \"boisterously\", \"boldest\", \"book\", \"book\", \"book\", \"book\", \"book\", \"books\", \"books\", \"books\", \"botanist\", \"bottleful\", \"box\", \"box\", \"box\", \"bradford\", \"branches\", \"brilliance\", \"brilliancy\", \"broadmoor\", \"broderick\", \"broke\", \"broke\", \"brother\", \"brother\", \"brother\", \"brother\", \"brother\", \"brought\", \"brought\", \"brought\", \"brought\", \"brown\", \"brown\", \"brown\", \"browner\", \"brusque\", \"buffelsspruit\", \"bulletins\", \"bunker\", \"bunny\", \"burglar\", \"burnin\", \"bushes\", \"business\", \"business\", \"business\", \"business\", \"business\", \"busy\", \"busy\", \"busy\", \"buys\", \"bystanders\", \"called\", \"called\", \"called\", \"called\", \"called\", \"called\", \"callosities\", \"cameos\", \"candidates\", \"canopy\", \"canting\", \"capital\", \"capturing\", \"carboys\", \"card\", \"card\", \"card\", \"cards\", \"cards\", \"caribou\", \"carolina\", \"carouse\", \"carriage\", \"carriage\", \"carriage\", \"carruthers\", \"carrying\", \"case\", \"case\", \"case\", \"case\", \"case\", \"cat\", \"cater\", \"caught\", \"caught\", \"caught\", \"caught\", \"caught\", \"caused\", \"cautionary\", \"celibate\", \"cell\", \"ceremony\", \"chamois\", \"chance\", \"chance\", \"chance\", \"chance\", \"chance\", \"chances\", \"chaos\", \"character\", \"character\", \"characteristics\", \"charlatanism\", \"charles\", \"charles\", \"charms\", \"chart\", \"cheat\", \"cheerfulness\", \"childlike\", \"chills\", \"chilly\", \"chirrup\", \"chokey\", \"choleric\", \"christmas\", \"chuckles\", \"cigarette\", \"cigarette\", \"cigarette\", \"cigarette\", \"circulates\", \"circumspectly\", \"city\", \"city\", \"city\", \"clair\", \"clair\", \"clandestine\", \"clarendon\", \"class\", \"class\", \"class\", \"clattered\", \"clean\", \"clean\", \"clean\", \"clean\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"clear\", \"cleared\", \"cleared\", \"cleared\", \"clearings\", \"clerkship\", \"clipper\", \"clippings\", \"clock\", \"clock\", \"clock\", \"close\", \"close\", \"close\", \"close\", \"close\", \"close\", \"closeted\", \"clues\", \"coarsened\", \"coils\", \"cold\", \"cold\", \"cold\", \"colleges\", \"collision\", \"colonel\", \"colonel\", \"combination\", \"commiseration\", \"commissionaire\", \"commonplaceness\", \"commotion\", \"commune\", \"commutation\", \"companion\", \"companion\", \"companion\", \"companion\", \"compilation\", \"complexity\", \"comprises\", \"conciliatory\", \"condition\", \"condition\", \"conditions\", \"conference\", \"confession\", \"confession\", \"confidante\", \"congratulating\", \"conic\", \"conquistadors\", \"considerable\", \"considerable\", \"considerable\", \"considerable\", \"considerable\", \"constabulary\", \"consumptive\", \"contemporaries\", \"contemporary\", \"context\", \"continually\", \"continually\", \"contrasted\", \"contribution\", \"controlling\", \"cool\", \"cops\", \"cord\", \"cormorant\", \"cornish\", \"coronet\", \"corot\", \"couch\", \"coud\", \"countess\", \"country\", \"country\", \"country\", \"coursed\", \"crams\", \"crave\", \"creep\", \"creosote\", \"crests\", \"cried\", \"cried\", \"cried\", \"cried\", \"cried\", \"cried\", \"crime\", \"crime\", \"crime\", \"crime\", \"crime\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"criminal\", \"criminology\", \"criticizing\", \"cross\", \"cross\", \"cross\", \"croydon\", \"cruellest\", \"crumb\", \"crusade\", \"culminates\", \"cumbered\", \"cunningham\", \"cupboard\", \"curable\", \"curb\", \"curious\", \"curious\", \"curious\", \"curious\", \"curmudgeon\", \"curtain\", \"curtain\", \"curtainless\", \"cushing\", \"custom\", \"cut\", \"cut\", \"cut\", \"cut\", \"danger\", \"danger\", \"dangerous\", \"dangerous\", \"dangerous\", \"danite\", \"danton\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"darkened\", \"darkling\", \"darkness\", \"darkness\", \"darkness\", \"darlin\", \"darwin\", \"dauntless\", \"days\", \"days\", \"days\", \"days\", \"days\", \"da\\u00df\", \"dead\", \"dead\", \"dead\", \"dead\", \"dealing\", \"dear\", \"dear\", \"death\", \"death\", \"death\", \"death\", \"death\", \"decent\", \"decoration\", \"decreased\", \"deeficulty\", \"defying\", \"delhi\", \"democratic\", \"demonstrative\", \"den\", \"den\", \"denizens\", \"dependant\", \"dependents\", \"depressions\", \"deprive\", \"derisively\", \"des\", \"describe\", \"describe\", \"detaching\", \"determine\", \"deutsche\", \"developed\", \"diaphanous\", \"diary\", \"digression\", \"dilation\", \"diminishing\", \"diminutive\", \"dinner\", \"dinner\", \"dinner\", \"disastrous\", \"disbelieve\", \"discovered\", \"discovered\", \"discovered\", \"discoverer\", \"discusses\", \"disfavour\", \"dispelled\", \"displaces\", \"disposes\", \"disproves\", \"disremember\", \"disrepute\", \"dissent\", \"dissolves\", \"distasteful\", \"distend\", \"district\", \"district\", \"ditching\", \"diverting\", \"dockmen\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doctor\", \"doggy\", \"domain\", \"domicile\", \"domineering\", \"don\", \"don\", \"don\", \"don\", \"doomed\", \"doses\", \"double\", \"double\", \"double\", \"doubt\", \"doubt\", \"doubt\", \"doubt\", \"doubt\", \"doubt\", \"dozen\", \"dozen\", \"dr\", \"dr\", \"dr\", \"dr\", \"draw\", \"draw\", \"dreads\", \"dreamless\", \"dressing\", \"dressing\", \"dressing\", \"dressing\", \"dressing\", \"drew\", \"drew\", \"drew\", \"drew\", \"drive\", \"drive\", \"drive\", \"drive\", \"drum\", \"dull\", \"dull\", \"dull\", \"dullest\", \"dumbfounded\", \"dure\", \"dwellers\", \"dying\", \"dying\", \"dying\", \"ead\", \"eager\", \"eager\", \"eager\", \"eagles\", \"early\", \"early\", \"early\", \"early\", \"eased\", \"eases\", \"easier\", \"east\", \"east\", \"east\", \"eastbourne\", \"easy\", \"easy\", \"easy\", \"easy\", \"easy\", \"eat\", \"eccles\", \"edgar\", \"editing\", \"eel\", \"effacement\", \"electro\", \"elevate\", \"elevation\", \"eleven\", \"eleven\", \"elixir\", \"eloquent\", \"elp\", \"emanates\", \"emboldened\", \"emissaries\", \"emsworth\", \"encouragement\", \"end\", \"end\", \"end\", \"end\", \"end\", \"end\", \"endeavoured\", \"endeavouring\", \"endowment\", \"england\", \"england\", \"england\", \"england\", \"english\", \"english\", \"english\", \"english\", \"english\", \"english\", \"entanglement\", \"entered\", \"entered\", \"entered\", \"entered\", \"entered\", \"enteric\", \"entering\", \"enthusiastically\", \"epileptic\", \"equal\", \"equestrian\", \"equipped\", \"era\", \"es\", \"esmeralda\", \"essential\", \"essential\", \"essentially\", \"essex\", \"estate\", \"estate\", \"estate\", \"estimation\", \"evasively\", \"evening\", \"evening\", \"evening\", \"evening\", \"evening\", \"evening\", \"event\", \"event\", \"event\", \"events\", \"events\", \"events\", \"events\", \"events\", \"evidence\", \"evidence\", \"evidence\", \"evidence\", \"excess\", \"exchanged\", \"excitement\", \"excitement\", \"excludes\", \"explanatory\", \"exposition\", \"expounding\", \"expressed\", \"extended\", \"external\", \"extirpation\", \"eye\", \"eye\", \"eye\", \"eye\", \"eyes\", \"eyes\", \"eyes\", \"eyes\", \"eyes\", \"eyes\", \"fabricated\", \"face\", \"face\", \"face\", \"face\", \"face\", \"face\", \"facer\", \"fact\", \"fact\", \"fact\", \"fact\", \"fact\", \"facts\", \"facts\", \"facts\", \"facts\", \"fairest\", \"fall\", \"false\", \"false\", \"false\", \"faltered\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"family\", \"fat\", \"father\", \"father\", \"father\", \"father\", \"father\", \"father\", \"fatiguing\", \"favorite\", \"favours\", \"feast\", \"feeling\", \"feeling\", \"feeling\", \"feet\", \"feet\", \"feet\", \"feet\", \"feet\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"fellow\", \"ferreted\", \"fewest\", \"fiance\", \"fibres\", \"final\", \"final\", \"find\", \"find\", \"find\", \"find\", \"finger\", \"fire\", \"fire\", \"fire\", \"fire\", \"fishermen\", \"fitness\", \"fitted\", \"flakey\", \"flanks\", \"flattering\", \"flickerin\", \"flinch\", \"floats\", \"floor\", \"floor\", \"floor\", \"floor\", \"flowers\", \"fluting\", \"foes\", \"folio\", \"folkestone\", \"force\", \"force\", \"force\", \"forceful\", \"forebears\", \"foregathering\", \"foremen\", \"formation\", \"forms\", \"forms\", \"forms\", \"foul\", \"foul\", \"found\", \"found\", \"found\", \"found\", \"found\", \"fox\", \"frame\", \"frances\", \"frank\", \"frank\", \"frankest\", \"fraternal\", \"french\", \"french\", \"french\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"friend\", \"friend\", \"friend\", \"friend\", \"friends\", \"friends\", \"frocks\", \"frog\", \"front\", \"front\", \"front\", \"fronting\", \"functions\", \"furiously\", \"furrows\", \"furtive\", \"fuse\", \"gale\", \"galleries\", \"game\", \"game\", \"game\", \"game\", \"gaming\", \"gardening\", \"garrison\", \"gate\", \"gate\", \"gate\", \"gatherings\", \"gave\", \"gave\", \"gave\", \"gave\", \"gave\", \"gems\", \"genially\", \"gennaro\", \"gentleman\", \"gentleman\", \"gentleman\", \"gentleman\", \"gentlemen\", \"gentlemen\", \"gentlemen\", \"gentlemen\", \"gentlemen\", \"gilding\", \"give\", \"give\", \"give\", \"give\", \"give\", \"give\", \"giving\", \"giving\", \"gloomy\", \"godfrey\", \"godfrey\", \"godfrey\", \"godfrey\", \"godfrey\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"goodnight\", \"gorilla\", \"governments\", \"graciously\", \"gradients\", \"grange\", \"grappling\", \"gratify\", \"gratifying\", \"gravelled\", \"gray\", \"gray\", \"gray\", \"gray\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"green\", \"green\", \"green\", \"green\", \"greyminster\", \"grisly\", \"grooves\", \"grottos\", \"ground\", \"ground\", \"ground\", \"grove\", \"growled\", \"grudging\", \"grunt\", \"guard\", \"guard\", \"guliolmi\", \"guns\", \"hair\", \"hair\", \"hair\", \"hair\", \"hair\", \"hairiness\", \"half\", \"half\", \"half\", \"half\", \"hall\", \"hall\", \"hall\", \"hall\", \"hall\", \"halle\", \"halted\", \"hammerford\", \"hammers\", \"hampshire\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"hand\", \"handed\", \"handed\", \"handed\", \"handed\", \"handles\", \"hands\", \"hands\", \"hands\", \"hands\", \"hands\", \"hands\", \"hangman\", \"happened\", \"happened\", \"happened\", \"hard\", \"hard\", \"hard\", \"hard\", \"hard\", \"hat\", \"hat\", \"hat\", \"hat\", \"hated\", \"haughty\", \"head\", \"head\", \"head\", \"head\", \"head\", \"headed\", \"headed\", \"headed\", \"heaping\", \"hear\", \"hear\", \"hear\", \"hear\", \"hear\", \"heard\", \"heard\", \"heard\", \"heard\", \"heard\", \"heard\", \"heard\", \"hearing\", \"heath\", \"held\", \"held\", \"held\", \"held\", \"henry\", \"henry\", \"henry\", \"henry\", \"henry\", \"hiding\", \"hiding\", \"high\", \"high\", \"high\", \"high\", \"highly\", \"highly\", \"hill\", \"hill\", \"hill\", \"hill\", \"himalayan\", \"hinging\", \"hippocratic\", \"hisself\", \"history\", \"history\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"holmes\", \"homespun\", \"hoop\", \"hope\", \"hope\", \"hope\", \"hope\", \"hoped\", \"hopelessness\", \"hopkins\", \"hopkins\", \"hopkins\", \"hops\", \"horses\", \"horsewhip\", \"hostel\", \"hotelward\", \"hour\", \"hour\", \"hour\", \"hour\", \"hour\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"house\", \"howard\", \"html\", \"html\", \"hugo\", \"huguenots\", \"hung\", \"hung\", \"hunts\", \"hut\", \"hut\", \"hyam\", \"hymn\", \"hypnotized\", \"hypochondriac\", \"idea\", \"idea\", \"idea\", \"idea\", \"idea\", \"illusions\", \"imaginings\", \"immigrant\", \"impassioned\", \"impediment\", \"impish\", \"implements\", \"implied\", \"importance\", \"importance\", \"importance\", \"impossibilities\", \"imprecations\", \"in\\\"\", \"inability\", \"incarceration\", \"incited\", \"including\", \"increasingly\", \"indexed\", \"indications\", \"indications\", \"infamy\", \"infirm\", \"informant\", \"information\", \"information\", \"information\", \"information\", \"information\", \"inglenook\", \"initials\", \"injection\", \"ink\", \"inn\", \"insistence\", \"inspection\", \"inspector\", \"inspector\", \"inspector\", \"inspector\", \"instant\", \"instant\", \"instant\", \"instant\", \"instant\", \"instructors\", \"intense\", \"interest\", \"interest\", \"interest\", \"interest\", \"interest\", \"interrupting\", \"intervening\", \"intimidation\", \"invariably\", \"investigation\", \"investigation\", \"investigation\", \"investigation\", \"iron\", \"iron\", \"iron\", \"iron\", \"ironhill\", \"ironical\", \"irresolution\", \"irritates\", \"jabber\", \"jacket\", \"jacket\", \"jagged\", \"james\", \"james\", \"james\", \"jansen\", \"jean\", \"jeered\", \"jessamine\", \"jewry\", \"jiffy\", \"join\", \"judge\", \"judge\", \"judge\", \"judge\", \"judges\", \"keening\", \"kennel\", \"kerchief\", \"key\", \"key\", \"key\", \"key\", \"key\", \"key\", \"kind\", \"kind\", \"kindly\", \"kindly\", \"kindness\", \"kindness\", \"knees\", \"knees\", \"knew\", \"knew\", \"knew\", \"knew\", \"knife\", \"knife\", \"knifing\", \"knighthood\", \"knock\", \"knowing\", \"knowing\", \"knowing\", \"knowing\", \"knox\", \"labours\", \"lady\", \"lady\", \"lady\", \"lady\", \"lady\\\"\", \"lagged\", \"lamp\", \"lamp\", \"lamp\", \"lamp\", \"lamp\", \"lander\", \"languorous\", \"lapped\", \"larch\", \"large\", \"large\", \"largest\", \"lark\", \"lashed\", \"lastly\", \"late\", \"late\", \"late\", \"laughing\", \"laughing\", \"laughing\", \"lawlessness\", \"lay\", \"lay\", \"lay\", \"lay\", \"lay\", \"lay\", \"lay\", \"lay\", \"leave\", \"leave\", \"leave\", \"leave\", \"leave\", \"leavetaking\", \"lecoq\", \"leeds\", \"left\", \"left\", \"left\", \"left\", \"legacy\", \"legends\", \"legible\", \"lessened\", \"lesser\", \"lestrade\", \"lestrade\", \"lestrade\", \"letter\", \"letter\", \"letter\", \"letter\", \"letter\", \"levity\", \"libels\", \"life\", \"life\", \"life\", \"life\", \"life\", \"lifts\", \"light\", \"light\", \"light\", \"light\", \"limited\", \"limits\", \"linder\", \"lip\", \"lit\", \"lit\", \"lit\", \"lit\", \"lit\", \"lived\", \"lived\", \"lived\", \"lived\", \"lived\", \"livin\", \"lo\", \"locked\", \"locked\", \"locked\", \"london\", \"london\", \"london\", \"london\", \"looked\", \"looked\", \"looked\", \"looked\", \"looked\", \"loose\", \"loose\", \"loose\", \"looseness\", \"lord\", \"lord\", \"lord\", \"lord\", \"lord\", \"lord\", \"loud\", \"loud\", \"loud\", \"loud\", \"lovely\", \"lovely\", \"low\", \"low\", \"low\", \"low\", \"low\", \"low\", \"loyal\", \"lukewarmness\", \"lull\", \"lumbago\", \"lunatics\", \"lunkah\", \"luxe\", \"mace\", \"magistrate\", \"magnetism\", \"magnifiques\", \"maiden\", \"making\", \"making\", \"making\", \"making\", \"making\", \"malign\", \"malignant\", \"malthus\", \"maned\", \"mangrove\", \"manner\", \"manner\", \"manner\", \"manner\", \"manner\", \"manner\", \"manner\", \"marginal\", \"maria\", \"married\", \"married\", \"married\", \"married\", \"marsh\", \"marvin\", \"mate\", \"matter\", \"matter\", \"matter\", \"matter\", \"max\", \"ma\\u00eetres\", \"mccarthy\", \"mccarthy\", \"mccarthy\", \"mccarthy\", \"meddled\", \"medieval\", \"medium\", \"meets\", \"melas\", \"member\", \"member\", \"men\", \"men\", \"men\", \"men\", \"menschen\", \"merchantablity\", \"merged\", \"merryweather\", \"messrs\", \"met\", \"met\", \"met\", \"met\", \"met\", \"meted\", \"mettle\", \"miasmatic\", \"microcosm\", \"middleton\", \"mildew\", \"mind\", \"mind\", \"mind\", \"mind\", \"mind\", \"minute\", \"minute\", \"minute\", \"minute\", \"minute\", \"minutes\", \"misconception\", \"misers\", \"miss\", \"miss\", \"miss\", \"mission\", \"mixture\", \"moaned\", \"moat\", \"modeller\", \"modulated\", \"mogul\", \"moment\", \"moment\", \"moment\", \"moment\", \"moment\", \"money\", \"money\", \"montpensier\", \"moran\", \"moran\", \"moran\", \"mornin\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"morning\", \"morris\", \"mortimer\", \"mortimer\", \"mortimer\", \"motive\", \"motley\", \"mound\", \"moustached\", \"mrs\", \"mrs\", \"mrs\", \"mrs\", \"mrs\", \"mrs\", \"mrs\", \"mummy\", \"murder\", \"murder\", \"murder\", \"murger\", \"musing\", \"naively\", \"narrative\", \"narrative\", \"native\", \"nature\", \"nature\", \"natures\", \"nauvoo\", \"neat\", \"neat\", \"neckcloth\", \"needed\", \"needed\", \"needed\", \"needing\", \"nervous\", \"nervous\", \"nervous\", \"nestling\", \"nettle\", \"neurotic\", \"newmarket\", \"news\", \"news\", \"news\", \"news\", \"news\", \"nicholson\", \"nigger\", \"night\", \"night\", \"night\", \"night\", \"nights\\\"\", \"nineteen\", \"ninth\", \"nitrates\", \"noiselessness\", \"noises\", \"nomad\", \"northwest\", \"nostrums\", \"notepaper\", \"nouveaux\", \"oasis\", \"obscuring\", \"observed\", \"observed\", \"observed\", \"observed\", \"obstructed\", \"occupied\", \"odd\", \"odds\", \"offender\", \"office\", \"office\", \"office\", \"office\", \"official\", \"official\", \"officious\", \"oil\", \"oiled\", \"onerous\", \"onyxes\", \"open\", \"open\", \"open\", \"open\", \"opened\", \"opened\", \"opened\", \"opened\", \"opened\", \"opened\", \"opened\", \"orbital\", \"ornamentation\", \"outfitter\", \"outhouse\", \"outlawed\", \"outnumbered\", \"overgrown\", \"overseer\", \"overtaken\", \"overthrow\", \"overton\", \"overton\", \"oxen\", \"oxshott\", \"oyster\", \"pacify\", \"packet\", \"packet\", \"paddled\", \"painful\", \"painful\", \"pair\", \"pair\", \"pairs\", \"palatable\", \"palpable\", \"panted\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"paper\", \"papers\", \"papers\", \"papers\", \"papers\", \"paradoxes\", \"parcel\", \"pard\", \"pardoned\", \"parents\", \"park\", \"park\", \"park\", \"parkhurst\", \"part\", \"part\", \"particle\", \"partington\", \"partners\", \"partridge\", \"passage\", \"passage\", \"passage\", \"passage\", \"passed\", \"passed\", \"passed\", \"passed\", \"passed\", \"passed\", \"passed\", \"pathway\", \"patriarchs\", \"patrician\", \"peaceable\", \"peacefully\", \"peajacket\", \"pearly\", \"penned\", \"pennyworth\", \"people\", \"people\", \"people\", \"people\", \"people\", \"percentages\", \"permits\", \"perpendicular\", \"persecuting\", \"persecutor\", \"persist\", \"persisted\", \"person\", \"person\", \"person\", \"person\", \"person\", \"personal\", \"personal\", \"pervert\", \"peterson\", \"phantom\", \"philosophical\", \"phoned\", \"picker\", \"pickpockets\", \"picnic\", \"picture\", \"picture\", \"picture\", \"picture\", \"pipe\", \"pipe\", \"pipe\", \"pipe\", \"piqued\", \"pistoling\", \"placards\", \"place\", \"place\", \"place\", \"place\", \"place\", \"place\", \"plaudits\", \"playful\", \"playmate\", \"plotted\", \"plotter\", \"ploughs\", \"plundered\", \"plunger\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"pocket\", \"point\", \"point\", \"point\", \"point\", \"points\", \"points\", \"points\", \"points\", \"points\", \"poison\", \"poison\", \"police\", \"police\", \"police\", \"police\", \"police\", \"portalis\", \"porter\", \"porter\", \"portion\", \"position\", \"position\", \"position\", \"position\", \"position\", \"possessor\", \"post\", \"post\", \"postcards\", \"power\", \"power\", \"power\", \"power\", \"praised\", \"pray\", \"precipitation\", \"prefer\", \"prefer\", \"prefers\", \"preoccupation\", \"presence\", \"presence\", \"presence\", \"presence\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"present\", \"presented\", \"presiding\", \"prime\", \"priori\", \"prisoner\", \"prisoner\", \"proceedings\", \"proceedings\", \"prodded\", \"profound\", \"projections\", \"property\", \"proportions\", \"propositions\", \"prospectors\", \"protestations\", \"proudly\", \"proverb\", \"provided\", \"provided\", \"pseudo\", \"psychology\", \"puffs\", \"pulsated\", \"pulsed\", \"punctual\", \"punishes\", \"purchasable\", \"purely\", \"purloined\", \"purpose\", \"purpose\", \"purpose\", \"purse\", \"pushes\", \"put\", \"put\", \"put\", \"pycroft\", \"pycroft\", \"quarries\", \"queerness\", \"querulously\", \"question\", \"question\", \"question\", \"quests\", \"quick\", \"quick\", \"quick\", \"quick\", \"quick\", \"quiet\", \"quiet\", \"quiet\", \"quitting\", \"rafter\", \"railroads\", \"rain\", \"rain\", \"rampant\", \"ran\", \"ran\", \"ran\", \"ran\", \"ran\", \"ran\", \"rankling\", \"raphael\", \"rate\", \"rattled\", \"reach\", \"reach\", \"reach\", \"reached\", \"reached\", \"reached\", \"reached\", \"reaching\", \"read\", \"read\", \"readily\", \"reap\", \"reapers\", \"reasoner\", \"recapture\", \"reciprocate\", \"reclined\", \"recognizes\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"refilling\", \"reflector\", \"refugees\", \"refurnished\", \"registering\", \"reigned\", \"rein\", \"reined\", \"remaining\", \"remains\", \"remains\", \"remarkable\", \"remarkable\", \"remarkable\", \"remarkable\", \"remarked\", \"remarked\", \"remarked\", \"removed\", \"removed\", \"removed\", \"removes\", \"renting\", \"repaired\", \"repairer\", \"replace\", \"representatives\", \"reproof\", \"rescuing\", \"resides\", \"resounding\", \"restoration\", \"retentive\", \"retreat\", \"retrospection\", \"returned\", \"revel\", \"revere\", \"reverential\", \"revert\", \"reverts\", \"reviewed\", \"rig\", \"rigging\", \"rigid\", \"rigmarole\", \"ringed\", \"ringlets\", \"risus\", \"road\", \"road\", \"road\", \"road\", \"robbing\", \"rodney\", \"romanticism\", \"room\", \"room\", \"room\", \"room\", \"room\", \"room\", \"rooms\", \"roots\", \"ross\", \"ross\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"rounding\", \"roundish\", \"roused\", \"roysterers\", \"rucastle\", \"rulings\", \"rumbling\", \"rumours\", \"rupees\", \"rushed\", \"rushed\", \"rushed\", \"rushed\", \"rusticate\", \"safe\", \"safe\", \"safe\", \"safe\", \"safer\", \"sagged\", \"salvator\", \"san\", \"sapphires\", \"sarah\", \"sarcasms\", \"sat\", \"sat\", \"sat\", \"satisfied\", \"savage\", \"savage\", \"savage\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"save\", \"saves\", \"sawyers\", \"scare\", \"scatheless\", \"scholars\", \"schoolboy\", \"scrutinizing\", \"scum\", \"sear\", \"searched\", \"searches\", \"seasoning\", \"secured\", \"secured\", \"seddar\", \"sedges\", \"send\", \"send\", \"send\", \"send\", \"senegambia\", \"senses\", \"sepoy\", \"set\", \"set\", \"set\", \"set\", \"set\", \"setting\", \"settlers\", \"shadow\", \"shadow\", \"shadowed\", \"shafter\", \"shapely\", \"sheathed\", \"sheepish\", \"sherlock\", \"sherlock\", \"sherlock\", \"sherlock\", \"sherlock\", \"shillings\", \"shin\", \"shire\", \"sholto\", \"sholto\", \"shoulder\", \"shoulder\", \"shoulder\", \"shovelling\", \"showed\", \"showed\", \"showed\", \"showed\", \"showery\", \"showmen\", \"shuman\", \"shutters\", \"shutters\", \"sickening\", \"side\", \"side\", \"side\", \"siding\", \"sight\", \"sight\", \"sight\", \"sight\", \"sign\", \"sign\", \"sign\", \"sign\", \"sign\", \"signs\", \"signs\", \"signs\", \"signs\", \"silvering\", \"simious\", \"simulates\", \"sincere\", \"singly\", \"singular\", \"singular\", \"singular\", \"singular\", \"singular\", \"singular\", \"situated\", \"skeletons\", \"skin\", \"skin\", \"skin\", \"skulks\", \"skulls\", \"slanders\", \"slaney\", \"slaughtered\", \"sledge\", \"slid\", \"slipshod\", \"slowly\", \"slowly\", \"slowly\", \"slowly\", \"slowly\", \"slowness\", \"smarted\", \"smith\", \"smith\", \"smith\", \"smith\", \"smith\", \"snort\", \"soared\", \"society\", \"society\", \"society\", \"softening\", \"soldierly\", \"soldiers\", \"solicitations\", \"son\", \"son\", \"son\", \"soothes\", \"sort\", \"sort\", \"sort\", \"sort\", \"sotheby\", \"sound\", \"sound\", \"sounding\", \"sources\", \"spaces\", \"speaking\", \"speaking\", \"specializes\", \"spent\", \"spent\", \"spent\", \"spent\", \"spent\", \"spindled\", \"spins\", \"spleen\", \"splendor\", \"splugen\", \"spoke\", \"spoke\", \"spoke\", \"spoke\", \"spoor\", \"sprang\", \"sprang\", \"sprang\", \"sprang\", \"spur\", \"squalor\", \"squeals\", \"st\", \"st\", \"st\", \"st\", \"stable\", \"stable\", \"stablemen\", \"stage\", \"stage\", \"stage\", \"staged\", \"standardized\", \"standback\", \"stapleton\", \"stapleton\", \"stare\", \"started\", \"started\", \"started\", \"statement\", \"statement\", \"statement\", \"statement\", \"station\", \"station\", \"station\", \"station\", \"station\", \"stationary\", \"statu\", \"stave\", \"steamed\", \"steeled\", \"step\", \"step\", \"step\", \"stillest\", \"stipulation\", \"stockbroker\", \"stoke\", \"stolen\", \"stoner\", \"stoniest\", \"stood\", \"stood\", \"stood\", \"straight\", \"straight\", \"straight\", \"straight\", \"straight\", \"straight\", \"straight\", \"strait\", \"strange\", \"strange\", \"strange\", \"strange\", \"strange\", \"street\", \"street\", \"street\", \"street\", \"street\", \"street\", \"streets\", \"streets\", \"strikes\", \"strode\", \"struck\", \"struck\", \"struck\", \"struck\", \"stubbly\", \"students\", \"study\", \"study\", \"study\", \"study\", \"sturdily\", \"sturmash\", \"styles\", \"subjective\", \"sublibrarian\", \"substantiate\", \"substitute\", \"suddenly\", \"suddenly\", \"suddenly\", \"suddenly\", \"suddenly\", \"suddenly\", \"suddenly\", \"suddenly\", \"suitcase\", \"sulks\", \"sums\", \"suppose\", \"suppose\", \"suppose\", \"suppose\", \"suppose\", \"suppose\", \"supposing\", \"supposititious\", \"supra\", \"sureties\", \"surges\", \"surprise\", \"surprise\", \"surprise\", \"surprise\", \"surveys\", \"surviving\", \"susceptibilities\", \"suspected\", \"sustenance\", \"swallow\", \"swedish\", \"swerved\", \"swiftest\", \"swiftly\", \"swiftly\", \"swiftly\", \"swiftly\", \"swing\", \"sympathetically\", \"table\", \"table\", \"table\", \"table\", \"tactful\", \"taking\", \"taking\", \"taking\", \"taking\", \"tallies\", \"tapped\", \"teaches\", \"teaspoonful\", \"teeming\", \"tempest\", \"ten\", \"ten\", \"ten\", \"ten\", \"ten\", \"ten\", \"ten\", \"tent\", \"tentacles\", \"tentative\", \"terrible\", \"terrible\", \"terrible\", \"terrible\", \"text\", \"text\", \"text\", \"theophilus\", \"theories\", \"theories\", \"theories\", \"theresa\", \"thickens\", \"thief\", \"thief\", \"thieves\", \"thin\", \"thin\", \"thin\", \"thin\", \"thing\", \"thing\", \"thing\", \"thing\", \"thing\", \"things\", \"things\", \"things\", \"thinkin\", \"thought\", \"thought\", \"thought\", \"thought\", \"thousand\", \"thousand\", \"thousand\", \"threatening\", \"threefold\", \"throw\", \"throw\", \"throw\", \"thumbed\", \"thundering\", \"tightness\", \"time\", \"time\", \"time\", \"time\", \"time\", \"times\", \"times\", \"times\", \"times\", \"times\", \"times\", \"tips\", \"tobaccoes\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"told\", \"torpor\", \"trader\", \"train\", \"train\", \"train\", \"train\", \"train\", \"trained\", \"transcendently\", \"transcribing\", \"translating\", \"transposed\", \"trap\", \"trap\", \"trap\", \"trap\", \"trapper\", \"tripping\", \"trouble\", \"trouble\", \"trouble\", \"trouble\", \"trucks\", \"turned\", \"turned\", \"turned\", \"turned\", \"turning\", \"turning\", \"turning\", \"turning\", \"tussle\", \"twenty\", \"twenty\", \"twenty\", \"type\", \"ululation\", \"unannounced\", \"unappreciative\", \"unbraided\", \"unchecked\", \"uncomfortably\", \"unconcern\", \"unconscious\", \"understand\", \"understand\", \"understand\", \"uneasiness\", \"unextinguishable\", \"unfortunate\", \"unfortunate\", \"unfriendly\", \"uninitiated\", \"universality\", \"unlaced\", \"unlooked\", \"unmanly\", \"unnerving\", \"unprovoked\", \"unscathed\", \"unshaken\", \"untie\", \"upholding\", \"upstair\", \"uselessness\", \"vagabone\", \"vague\", \"vague\", \"vague\", \"valet\", \"valetudinarian\", \"vanderbilt\", \"vaporous\", \"variants\", \"varnished\", \"vastness\", \"vatican\", \"ve\", \"ve\", \"ve\", \"ve\", \"ve\", \"ve\", \"velocity\", \"vented\", \"ventures\", \"verandah\", \"verb\", \"version\", \"verstehen\", \"vicar\", \"vicar\", \"viewing\", \"violate\", \"vipers\", \"virtually\", \"visage\", \"visited\", \"visitors\", \"vivant\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voice\", \"voman\", \"voted\", \"vouch\", \"vulnerable\", \"wade\", \"wailings\", \"wainwright\", \"waistcoat\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"wait\", \"waited\", \"waited\", \"waited\", \"wakes\", \"walked\", \"walked\", \"walked\", \"walked\", \"wallah\", \"wanting\", \"warder\", \"warnin\", \"warning\", \"warning\", \"warning\", \"warranties\", \"warranty\", \"washington\", \"washoe\", \"water\", \"water\", \"water\", \"water\", \"water\", \"watson\", \"watson\", \"watson\", \"watson\", \"watson\", \"watson\", \"wednesday\", \"week\", \"week\", \"week\", \"weep\", \"weren\", \"wesson\", \"west\", \"west\", \"westmoreland\", \"whereat\", \"whips\", \"whitaker\", \"white\", \"white\", \"white\", \"white\", \"whitworth\", \"wielded\", \"wife\", \"wife\", \"wife\", \"wife\", \"wigwams\", \"wills\", \"wimbledon\", \"winch\", \"winchester\", \"window\", \"window\", \"window\", \"window\", \"window\", \"wir\", \"wished\", \"wished\", \"wished\", \"wished\", \"witch\", \"withdrawal\", \"woman\", \"woman\", \"woman\", \"woman\", \"woman\", \"won\", \"won\", \"won\", \"won\", \"word\", \"word\", \"word\", \"word\", \"word\", \"word\", \"words\", \"words\", \"words\", \"words\", \"words\", \"words\", \"work\", \"work\", \"work\", \"work\", \"work\", \"worn\", \"worn\", \"worn\", \"worn\", \"wrist\", \"written\", \"written\", \"written\", \"wrong\", \"wrong\", \"wrong\", \"wrong\", \"wu\", \"yapped\", \"yard\", \"yard\", \"yard\", \"yards\", \"yards\", \"yawls\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"years\", \"years\", \"years\", \"years\", \"yeggman\", \"yews\", \"yoked\", \"youghal\", \"young\", \"young\", \"young\", \"young\", \"yuan\"]}, \"mdsDat\": {\"y\": [-0.064119142361538511, -0.11352411579635446, 0.083996494035685917, 0.0053891235077595315, -0.070776614634756693, 0.11081690716659404, -0.22861475107680401, 0.21073453305789178, -0.010414031558817744, 0.076511597660340464], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [10.493265303084895, 10.235788208520175, 10.140768356566014, 10.12897175644688, 10.06192520385787, 10.028438919286511, 9.9906433558263696, 9.9765741277989068, 9.5831926235791514, 9.3604321450332222], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"x\": [0.09477623718757229, 0.10675205656539594, 0.073387832905042277, 0.031538604732806633, 0.049218717709680386, 0.082671646795546075, -0.0086995205115855505, 0.11006807773948755, -0.25647333444932358, -0.2832403186746219]}, \"R\": 30, \"lambda.step\": 0.01, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"Term\": [\"\\\"\", \"holmes\", \"\\\"i\", \"time\", \"room\", \"face\", \"watson\", \"hand\", \"morning\", \"night\", \"left\", \"\\\"it\", \"thought\", \"case\", \"great\", \"found\", \"side\", \"sherlock\", \"friend\", \"matter\", \"light\", \"house\", \"\\\"you\", \"back\", \"eyes\", \"half\", \"\\\"well\", \"heard\", \"young\", \"open\", \"da\\u00df\", \"sapphires\", \"architect\", \"unmanly\", \"vastness\", \"formation\", \"mettle\", \"slipshod\", \"restoration\", \"unappreciative\", \"meddled\", \"keening\", \"unshaken\", \"disrepute\", \"danite\", \"eased\", \"grisly\", \"hypochondriac\", \"chirrup\", \"skulls\", \"knighthood\", \"decreased\", \"denizens\", \"haughty\", \"linder\", \"withdrawal\", \"unlooked\", \"doomed\", \"chamois\", \"uncomfortably\", \"suitcase\", \"halle\", \"slid\", \"heaping\", \"peaceable\", \"foremen\", \"specializes\", \"statu\", \"sounding\", \"closeted\", \"hymn\", \"lark\", \"hypnotized\", \"verandah\", \"maria\", \"spindled\", \"magnifiques\", \"witch\", \"demonstrative\", \"mace\", \"brilliance\", \"persist\", \"expounding\", \"lady\\\"\", \"hammers\", \"wailings\", \"confidante\", \"charms\", \"diverting\", \"phoned\", \"minutes\", \"dissent\", \"administrator\", \"seasoning\", \"contemporary\", \"transposed\", \"abated\", \"governments\", \"disproves\", \"nettle\", \"\\\"tuxbury\", \"projections\", \"commutation\", \"extirpation\", \"diminishing\", \"vouch\", \"refilling\", \"unchecked\", \"hops\", \"clippings\", \"overgrown\", \"pistoling\", \"large\", \"cross\", \"suspected\", \"waited\", \"wrist\", \"melas\", \"guard\", \"curtain\", \"mission\", \"rigid\", \"easier\", \"occupied\", \"type\", \"cupboard\", \"setting\", \"quiet\", \"vague\", \"besought\", \"bystanders\", \"removes\", \"ululation\", \"ailing\", \"disposes\", \"nauvoo\", \"hostel\", \"punishes\", \"gaming\", \"thin\", \"foul\", \"swiftly\", \"half\", \"condition\", \"largest\", \"native\", \"clues\", \"barred\", \"equal\", \"grove\", \"jagged\", \"floor\", \"hand\", \"discovered\", \"front\", \"left\", \"handed\", \"suddenly\", \"side\", \"locked\", \"society\", \"room\", \"appearance\", \"low\", \"close\", \"men\", \"lit\", \"slowly\", \"books\", \"passed\", \"won\", \"ten\", \"black\", \"caught\", \"money\", \"sprang\", \"house\", \"ran\", \"good\", \"dark\", \"question\", \"time\", \"moment\", \"words\", \"gentleman\", \"\\\"mine\", \"vaporous\", \"dilation\", \"context\", \"galleries\", \"slowness\", \"languorous\", \"fluting\", \"recapture\", \"bat\", \"visage\", \"bleaker\", \"swerved\", \"ninth\", \"\\\"subject\", \"reapers\", \"ban\", \"prospectors\", \"dissolves\", \"diminutive\", \"simious\", \"commiseration\", \"dependant\", \"essex\", \"fiance\", \"bather\", \"coarsened\", \"revere\", \"kerchief\", \"sulks\", \"doggy\", \"\\\"dorak\", \"shovelling\", \"placards\", \"imprecations\", \"essentially\", \"musing\", \"inglenook\", \"jabber\", \"1644\", \"appliances\", \"buys\", \"motley\", \"agin\", \"trucks\", \"scholars\", \"peacefully\", \"reverts\", \"encouragement\", \"sheepish\", \"himalayan\", \"dreads\", \"teeming\", \"bradford\", \"labours\", \"lessened\", \"rein\", \"returned\", \"rooms\", \"foregathering\", \"sedges\", \"spins\", \"fewest\", \"burnin\", \"standback\", \"paradoxes\", \"lull\", \"warder\", \"showmen\", \"mildew\", \"babe\", \"plotted\", \"playful\", \"rusticate\", \"infamy\", \"rafter\", \"shapely\", \"darlin\", \"picnic\", \"basic\", \"rankling\", \"anthropologists\", \"brought\", \"bushes\", \"signs\", \"burglar\", \"ashamed\", \"furiously\", \"visited\", \"drive\", \"reasoner\", \"mixture\", \"beppo\", \"beddoes\", \"consumptive\", \"shuman\", \"skeletons\", \"retrospection\", \"montpensier\", \"\\\"boys\", \"distend\", \"hinging\", \"curtainless\", \"palatable\", \"natures\", \"misers\", \"washington\", \"trouble\", \"knowing\", \"clair\", \"shutters\", \"removed\", \"needed\", \"\\\"precisely\", \"remaining\", \"secured\", \"developed\", \"oil\", \"grange\", \"unconscious\", \"gale\", \"continually\", \"abandoned\", \"beat\", \"savage\", \"started\", \"week\", \"light\", \"dr\", \"east\", \"cleared\", \"remains\", \"companion\", \"stable\", \"shadow\", \"hope\", \"news\", \"brown\", \"straight\", \"interest\", \"struck\", \"night\", \"told\", \"baker\", \"iron\", \"street\", \"lived\", \"dangerous\", \"attention\", \"doctor\", \"station\", \"\\\"well\", \"singular\", \"dark\", \"round\", \"leave\", \"put\", \"doubt\", \"turned\", \"instant\", \"place\", \"mind\", \"left\", \"looked\", \"propositions\", \"unlaced\", \"lo\", \"reproof\", \"precipitation\", \"gardening\", \"coils\", \"\\\"lost\", \"conciliatory\", \"oxen\", \"proverb\", \"flinch\", \"gatherings\", \"transcendently\", \"chilly\", \"ringlets\", \"jeered\", \"\\\"singular\", \"auditors\", \"abetting\", \"lawlessness\", \"weep\", \"obstructed\", \"virtually\", \"adversary\", \"neckcloth\", \"\\\"toby\", \"vipers\", \"alters\", \"hangman\", \"deutsche\", \"yeggman\", \"handles\", \"fronting\", \"\\\"brixton\", \"sarcasms\", \"substitute\", \"rescuing\", \"whitaker\", \"dependents\", \"furrows\", \"democratic\", \"absurdity\", \"modulated\", \"amenities\", \"sincere\", \"\\\"count\", \"indexed\", \"fox\", \"des\", \"rig\", \"1800\", \"presiding\", \"chuckles\", \"insistence\", \"seddar\", \"collision\", \"vagabone\", \"raphael\", \"botanist\", \"cameos\", \"nouveaux\", \"sheathed\", \"carolina\", \"berkshires\", \"looseness\", \"equipped\", \"querulously\", \"homespun\", \"eases\", \"scatheless\", \"marginal\", \"disfavour\", \"garrison\", \"translating\", \"yapped\", \"squalor\", \"feast\", \"\\\"pure\", \"max\", \"nitrates\", \"pennyworth\", \"schoolboy\", \"\\\"absolutely\", \"carruthers\", \"colonel\", \"rate\", \"visitors\", \"waistcoat\", \"stoner\", \"hated\", \"cards\", \"roused\", \"essential\", \"custom\", \"gems\", \"reaching\", \"tapped\", \"crams\", \"nights\\\"\", \"ventures\", \"elp\", \"onyxes\", \"roundish\", \"reflector\", \"persisted\", \"impediment\", \"\\\"move\", \"shillings\", \"cornish\", \"assassin\", \"emsworth\", \"stare\", \"outhouse\", \"sums\", \"stoke\", \"oxshott\", \"malignant\", \"member\", \"shafter\", \"darkened\", \"uneasiness\", \"constabulary\", \"moustached\", \"lestrade\", \"false\", \"lovely\", \"high\", \"work\", \"son\", \"dressing\", \"great\", \"presence\", \"laughing\", \"gave\", \"judge\", \"french\", \"nervous\", \"moran\", \"agent\", \"sat\", \"events\", \"estate\", \"set\", \"thousand\", \"\\\"ah\", \"letter\", \"ground\", \"year\", \"smith\", \"save\", \"death\", \"entered\", \"friend\", \"\\\"well\", \"hall\", \"air\", \"considerable\", \"house\", \"terrible\", \"find\", \"lamp\", \"power\", \"doubt\", \"curious\", \"police\", \"leave\", \"stood\", \"window\", \"eyes\", \"face\", \"end\", \"back\", \"childlike\", \"culminates\", \"leavetaking\", \"renting\", \"subjective\", \"bottleful\", \"notepaper\", \"nestling\", \"oyster\", \"recognizes\", \"blushed\", \"interrupting\", \"officious\", \"philosophical\", \"\\\"hate\", \"commotion\", \"dockmen\", \"admeet\", \"fishermen\", \"conquistadors\", \"candidates\", \"plunger\", \"boccaccio\", \"smarted\", \"deeficulty\", \"clandestine\", \"\\\"starts\", \"charlatanism\", \"increasingly\", \"upholding\", \"discoverer\", \"instructors\", \"partners\", \"lunkah\", \"transcribing\", \"dauntless\", \"jewry\", \"voman\", \"ironhill\", \"washoe\", \"cheerfulness\", \"ringed\", \"external\", \"mogul\", \"elevation\", \"yews\", \"violate\", \"priori\", \"lapped\", \"\\\"1884\", \"palpable\", \"marsh\", \"thickens\", \"pearly\", \"immigrant\", \"jansen\", \"danger\", \"country\", \"impassioned\", \"whips\", \"spleen\", \"malthus\", \"barrelled\", \"emissaries\", \"yards\", \"inn\", \"kindly\", \"carrying\", \"satisfied\", \"baronet\", \"sholto\", \"eleven\", \"endeavoured\", \"lip\", \"senses\", \"heath\", \"dozen\", \"ross\", \"retreat\", \"searched\", \"branches\", \"hut\", \"strikes\", \"beneath\", \"san\", \"mate\", \"cat\", \"chart\", \"vulnerable\", \"thinkin\", \"evasively\", \"hill\", \"moat\", \"countess\", \"inspection\", \"\\\"is\", \"mortimer\", \"wished\", \"darkness\", \"city\", \"evidence\", \"headed\", \"london\", \"trap\", \"office\", \"card\", \"people\", \"age\", \"heard\", \"game\", \"double\", \"put\", \"minute\", \"sign\", \"written\", \"pocket\", \"big\", \"hat\", \"fact\", \"red\", \"murder\", \"looked\", \"spoke\", \"head\", \"back\", \"investigation\", \"night\", \"safe\", \"mrs\", \"showed\", \"found\", \"\\\"\", \"hour\", \"case\", \"blood\", \"white\", \"water\", \"called\", \"great\", \"walked\", \"stockbroker\", \"stave\", \"sturmash\", \"splugen\", \"lukewarmness\", \"frocks\", \"\\\"porlock\", \"westmoreland\", \"unbraided\", \"plotter\", \"teaches\", \"hairiness\", \"guliolmi\", \"\\\"guess\", \"roysterers\", \"picker\", \"tempest\", \"faltered\", \"settlers\", \"thumbed\", \"rounding\", \"pacify\", \"nomad\", \"fraternal\", \"tripping\", \"luxe\", \"outlawed\", \"electro\", \"editing\", \"sickening\", \"railroads\", \"supra\", \"broadmoor\", \"paddled\", \"bender\", \"proudly\", \"persecuting\", \"\\\"down\", \"lifts\", \"ironical\", \"soldierly\", \"\\\"either\", \"explanatory\", \"steamed\", \"rupees\", \"outnumbered\", \"menschen\", \"domain\", \"clarendon\", \"commonplaceness\", \"magnetism\", \"irresolution\", \"incited\", \"fall\", \"teaspoonful\", \"97163\", \"aspirations\", \"preoccupation\", \"susceptibilities\", \"universality\", \"pulsed\", \"clipper\", \"overthrow\", \"patriarchs\", \"decoration\", \"cumbered\", \"greyminster\", \"meted\", \"\\\"apart\", \"percentages\", \"northwest\", \"unannounced\", \"gradients\", \"grappling\", \"edgar\", \"lander\", \"ma\\u00eetres\", \"chaos\", \"controlling\", \"nineteen\", \"prisoner\", \"es\", \"adventure\", \"gennaro\", \"hiding\", \"hair\", \"initials\", \"feeling\", \"capital\", \"html\", \"gloomy\", \"hearing\", \"morris\", \"prime\", \"halted\", \"theresa\", \"combination\", \"allusion\", \"fitted\", \"parkhurst\", \"reined\", \"swallow\", \"compilation\", \"supposing\", \"conditions\", \"dull\", \"den\", \"lashed\", \"hampshire\", \"warning\", \"soldiers\", \"cell\", \"characteristics\", \"district\", \"wrong\", \"clean\", \"loud\", \"dying\", \"surprise\", \"send\", \"loose\", \"eager\", \"life\", \"thing\", \"observed\", \"sound\", \"rushed\", \"broke\", \"study\", \"manner\", \"worn\", \"act\", \"clear\", \"gray\", \"fresh\", \"things\", \"eye\", \"reach\", \"appeared\", \"head\", \"turned\", \"cold\", \"key\", \"taking\", \"room\", \"hands\", \"days\", \"cut\", \"moment\", \"idea\", \"met\", \"point\", \"\\\"we\", \"good\", \"road\", \"papers\", \"give\", \"watson\", \"bain\", \"refurnished\", \"arkansas\", \"impish\", \"gravelled\", \"pseudo\", \"angelic\", \"microcosm\", \"sympathetically\", \"wade\", \"choleric\", \"repairer\", \"genially\", \"coursed\", \"fabricated\", \"blase\", \"circumspectly\", \"accredited\", \"emboldened\", \"purchasable\", \"queerness\", \"blossomed\", \"imaginings\", \"doses\", \"gorilla\", \"growled\", \"risus\", \"alton\", \"larch\", \"tentative\", \"carouse\", \"endowment\", \"era\", \"rigging\", \"enthusiastically\", \"quarries\", \"legacy\", \"modeller\", \"diaphanous\", \"drum\", \"apocrypha\", \"astonish\", \"crave\", \"upstair\", \"oiled\", \"unfriendly\", \"senegambia\", \"epileptic\", \"contribution\", \"needing\", \"retentive\", \"leeds\", \"unprovoked\", \"phantom\", \"levity\", \"delhi\", \"misconception\", \"resounding\", \"wu\", \"conic\", \"read\", \"reverential\", \"postcards\", \"gratify\", \"tactful\", \"cormorant\", \"avowed\", \"approbation\", \"nigger\", \"puffs\", \"wainwright\", \"bulletins\", \"untie\", \"adults\", \"legends\", \"pickpockets\", \"applauded\", \"howard\", \"surviving\", \"sotheby\", \"irritates\", \"intimidation\", \"nostrums\", \"miasmatic\", \"baskerville\", \"motive\", \"st\", \"assure\", \"entering\", \"personal\", \"position\", \"frame\", \"rain\", \"absolutely\", \"rattled\", \"readily\", \"maiden\", \"favorite\", \"henry\", \"\\\"when\", \"overtaken\", \"giving\", \"limits\", \"softening\", \"rampant\", \"huguenots\", \"pardoned\", \"youghal\", \"sear\", \"aggressor\", \"james\", \"barclay\", \"sort\", \"croydon\", \"hugo\", \"purely\", \"situated\", \"tips\", \"parents\", \"eat\", \"making\", \"theories\", \"remarkable\", \"post\", \"painful\", \"describe\", \"account\", \"event\", \"\\\"what\", \"bear\", \"watson\", \"dear\", \"case\", \"brother\", \"word\", \"twenty\", \"matter\", \"lord\", \"young\", \"place\", \"friend\", \"easy\", \"clock\", \"reached\", \"good\", \"hands\", \"business\", \"strange\", \"evening\", \"father\", \"found\", \"house\", \"\\\"there\", \"\\\"the\", \"years\", \"registering\", \"panted\", \"distasteful\", \"eloquent\", \"flanks\", \"impossibilities\", \"warnin\", \"bunny\", \"livin\", \"merged\", \"coud\", \"slanders\", \"reclined\", \"esmeralda\", \"peajacket\", \"derisively\", \"lesser\", \"hammerford\", \"comprises\", \"wimbledon\", \"jessamine\", \"scrutinizing\", \"lagged\", \"eagles\", \"forebears\", \"appreciating\", \"hopelessness\", \"lecoq\", \"badger\", \"infirm\", \"reciprocate\", \"oasis\", \"sublibrarian\", \"psychology\", \"intervening\", \"emanates\", \"domineering\", \"entanglement\", \"tobaccoes\", \"stablemen\", \"brilliancy\", \"surveys\", \"\\\"vermissa\", \"vivant\", \"curable\", \"graciously\", \"varnished\", \"dreamless\", \"creosote\", \"overseer\", \"moaned\", \"unnerving\", \"middleton\", \"rigmarole\", \"allay\", \"stationary\", \"displaces\", \"kennel\", \"caused\", \"playmate\", \"spur\", \"\\\"need\", \"canopy\", \"rodney\", \"\\\"facts\", \"uninitiated\", \"tent\", \"substantiate\", \"circulates\", \"beginnin\", \"libels\", \"crests\", \"version\", \"elixir\", \"tussle\", \"perpendicular\", \"cautionary\", \"strait\", \"contrasted\", \"unscathed\", \"reigned\", \"body\", \"late\", \"horses\", \"presented\", \"bar\", \"thought\", \"charles\", \"frances\", \"board\", \"trained\", \"commissionaire\", \"streets\", \"portion\", \"stolen\", \"bitterly\", \"\\\"where\", \"sherlock\", \"bitterns\", \"blobs\", \"knox\", \"praised\", \"beaune\", \"brusque\", \"digression\", \"scum\", \"inability\", \"gratifying\", \"wakes\", \"final\", \"arrival\", \"poison\", \"cool\", \"history\", \"christmas\", \"\\\"witness\", \"valet\", \"exchanged\", \"rumours\", \"ceremony\", \"pathway\", \"open\", \"hung\", \"time\", \"stage\", \"unfortunate\", \"proceedings\", \"west\", \"cigarette\", \"blind\", \"picture\", \"speaking\", \"crime\", \"afraid\", \"mccarthy\", \"official\", \"wait\", \"english\", \"throw\", \"holmes\", \"lady\", \"found\", \"road\", \"nature\", \"ve\", \"end\", \"force\", \"opened\", \"hard\", \"asked\", \"police\", \"character\", \"mind\", \"\\\"but\", \"understand\", \"hear\", \"held\", \"information\", \"cried\", \"life\", \"yuan\", \"lastly\", \"criticizing\", \"exposition\", \"romanticism\", \"judges\", \"quitting\", \"depressions\", \"injection\", \"snort\", \"steeled\", \"pulsated\", \"wallah\", \"blasting\", \"standardized\", \"purloined\", \"1893\", \"antipathy\", \"possessor\", \"deprive\", \"repaired\", \"messrs\", \"mound\", \"silvering\", \"revert\", \"valetudinarian\", \"grunt\", \"appendix\", \"solicitations\", \"asprawl\", \"crumb\", \"rulings\", \"pushes\", \"births\", \"tentacles\", \"floats\", \"offender\", \"sawyers\", \"pervert\", \"theophilus\", \"darkling\", \"sturdily\", \"sources\", \"yoked\", \"sustenance\", \"grottos\", \"stubbly\", \"splendor\", \"cruellest\", \"\\\"bartholomew\", \"singly\", \"dure\", \"grudging\", \"soared\", \"searches\", \"punctual\", \"mornin\", \"trapper\", \"stoniest\", \"warranties\", \"domicile\", \"sagged\", \"sureties\", \"pairs\", \"beastly\", \"incarceration\", \"callosities\", \"hyam\", \"\\\"heap\", \"criminology\", \"naively\", \"dispelled\", \"eel\", \"showery\", \"flakey\", \"velocity\", \"torpor\", \"revel\", \"stipulation\", \"corot\", \"\\\"rubbish\", \"weren\", \"excess\", \"effacement\", \"tightness\", \"attenuated\", \"morning\", \"limited\", \"expressed\", \"including\", \"medium\", \"implied\", \"merchantablity\", \"warranty\", \"fitness\", \"hoped\", \"provided\", \"\\\"as\", \"cunningham\", \"sarah\", \"intense\", \"purpose\", \"cord\", \"knock\", \"endeavouring\", \"wednesday\", \"merryweather\", \"dealing\", \"unconcern\", \"styles\", \"noiselessness\", \"thundering\", \"saves\", \"knife\", \"passage\", \"flowers\", \"determine\", \"profound\", \"furtive\", \"park\", \"porter\", \"hopkins\", \"excitement\", \"ashes\", \"early\", \"gate\", \"\\\"that\", \"kind\", \"narrative\", \"carriage\", \"dead\", \"yard\", \"inspector\", \"bed\", \"green\", \"window\", \"lay\", \"book\", \"fellow\", \"\\\"yes\", \"round\", \"back\", \"spent\", \"young\", \"criminal\", \"points\", \"held\", \"papers\", \"matter\", \"hour\", \"person\", \"knew\", \"case\", \"table\", \"side\", \"\\\"granting\", \"folkestone\", \"horsewhip\", \"flattering\", \"neurotic\", \"winch\", \"rumbling\", \"hisself\", \"viewing\", \"\\\"syracusan\", \"commune\", \"chokey\", \"medieval\", \"approaches\", \"jiffy\", \"1730\", \"skulks\", \"advocated\", \"blount\", \"newmarket\", \"nicholson\", \"accepts\", \"fatiguing\", \"begat\", \"orbital\", \"reap\", \"folio\", \"fuse\", \"stillest\", \"foes\", \"cater\", \"\\\"tonga\", \"lunatics\", \"unextinguishable\", \"onerous\", \"swiftest\", \"dwellers\", \"trader\", \"siding\", \"cheat\", \"bedding\", \"elevate\", \"verstehen\", \"tallies\", \"spaces\", \"obscuring\", \"boisterously\", \"vatican\", \"whitworth\", \"\\\"plain\", \"plundered\", \"salvator\", \"patrician\", \"cops\", \"clerkship\", \"disastrous\", \"broderick\", \"representatives\", \"carboys\", \"colleges\", \"wir\", \"\\\"rochester\", \"contemporaries\", \"capturing\", \"ferreted\", \"piqued\", \"lumbago\", \"fibres\", \"illusions\", \"murger\", \"boldest\", \"implements\", \"ditching\", \"soothes\", \"uselessness\", \"clearings\", \"detaching\", \"complexity\", \"finger\", \"rucastle\", \"pray\", \"cushing\", \"coronet\", \"knees\", \"pair\", \"join\", \"strode\", \"students\", \"fat\", \"browner\", \"packet\", \"shoulder\", \"hunts\", \"penned\", \"whereat\", \"wills\", \"shin\", \"permits\", \"boilers\", \"ornamentation\", \"informant\", \"portalis\", \"particle\", \"blushing\", \"congratulating\", \"staged\", \"frank\", \"peterson\", \"diary\", \"couch\", \"extended\", \"vicar\", \"draw\", \"odds\", \"parcel\", \"shadowed\", \"threatening\", \"confession\", \"swing\", \"safer\", \"magistrate\", \"replace\", \"kindness\", \"jacket\", \"neat\", \"highly\", \"face\", \"\\\"you\", \"pycroft\", \"fire\", \"quick\", \"holmes\", \"skin\", \"table\", \"woman\", \"turning\", \"stapleton\", \"miss\", \"friends\", \"sight\", \"eyes\", \"\\\"\", \"stood\", \"busy\", \"\\\"i\", \"step\", \"arm\", \"don\", \"lady\", \"feet\", \"text\", \"hear\", \"watson\", \"understand\", \"drew\", \"knew\", \"chance\", \"family\", \"give\", \"dear\", \"good\", \"find\", \"black\", \"caribou\", \"17th\", \"wigwams\", \"in\\\"\", \"persecutor\", \"malign\", \"danton\", \"abnormality\", \"darwin\", \"curmudgeon\", \"quests\", \"resides\", \"canting\", \"\\\"tropical\", \"ploughs\", \"ead\", \"mummy\", \"shire\", \"\\\"smith\", \"vented\", \"maned\", \"bewildering\", \"chills\", \"slaughtered\", \"supposititious\", \"aloofness\", \"proportions\", \"agonizing\", \"assessment\", \"wesson\", \"prodded\", \"gilding\", \"celibate\", \"partridge\", \"disbelieve\", \"bart\", \"\\\"adelbert\", \"frankest\", \"forceful\", \"frog\", \"knifing\", \"vanderbilt\", \"advancement\", \"sledge\", \"reviewed\", \"discusses\", \"mangrove\", \"104\", \"sepoy\", \"verb\", \"goodnight\", \"plaudits\", \"swedish\", \"eastbourne\", \"conference\", \"bunker\", \"\\\"any\", \"disremember\", \"threefold\", \"pard\", \"protestations\", \"excludes\", \"robbing\", \"fairest\", \"grooves\", \"jean\", \"refugees\", \"blessings\", \"annuity\", \"voted\", \"yawls\", \"noises\", \"surges\", \"crusade\", \"simulates\", \"27\", \"outfitter\", \"enteric\", \"aggrieved\", \"functions\", \"defying\", \"hoop\", \"abusing\", \"wielded\", \"part\", \"\\\"a\", \"answered\", \"variants\", \"property\", \"eccles\", \"ink\", \"odd\", \"winchester\", \"wanting\", \"\\\"easier\", \"dumbfounded\", \"buffelsspruit\", \"equestrian\", \"creep\", \"estimation\", \"facer\", \"hippocratic\", \"\\\"mean\", \"flickerin\", \"dullest\", \"favours\", \"blossoming\", \"spoor\", \"hotelward\", \"squeals\", \"\\\"it\", \"purse\", \"chances\", \"prefer\", \"invariably\", \"thieves\", \"slaney\", \"guns\", \"decent\", \"roots\", \"marvin\", \"meets\", \"clattered\", \"partington\", \"gentlemen\", \"apologize\", \"loyal\", \"scare\", \"legible\", \"\\\"miss\", \"curb\", \"prefers\", \"statement\", \"dinner\", \"indications\", \"anxious\", \"\\\"who\", \"\\\"\", \"overton\", \"\\\"certainly\", \"forms\", \"wife\", \"thief\", \"\\\"i\", \"remarked\", \"\\\"in\", \"married\", \"godfrey\", \"class\", \"facts\", \"\\\"the\", \"point\", \"suppose\", \"bell\", \"present\", \"voice\", \"happened\", \"paper\", \"times\", \"box\", \"england\", \"began\", \"find\", \"don\", \"knew\", \"pipe\", \"importance\", \"years\", \"text\", \"business\", \"\\\"and\", \"hand\", \"train\", \"clear\"], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.2067000000000001, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.1583999999999999, 2.1812999999999998, 2.2088000000000001, 2.1697000000000002, 2.2092000000000001, 2.2092999999999998, 2.1827000000000001, 2.1913, 2.2094999999999998, 2.2094999999999998, 2.2098, 2.2098, 2.2101999999999999, 2.2101999999999999, 2.2103999999999999, 2.1573000000000002, 2.1789000000000001, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.3016999999999999, 2.1421000000000001, 2.1861999999999999, 2.1594000000000002, 2.0777000000000001, 2.1829999999999998, 2.2115999999999998, 2.2115999999999998, 2.2115999999999998, 2.2119, 2.2122999999999999, 2.2122999999999999, 2.2126000000000001, 2.0825999999999998, 1.9924999999999999, 2.1265999999999998, 2.0299, 1.9656, 2.0895000000000001, 2.0003000000000002, 1.9387000000000001, 2.0962999999999998, 2.1051000000000002, 1.7547999999999999, 2.0209999999999999, 2.0261999999999998, 1.9734, 1.8574999999999999, 2.0388000000000002, 2.0099, 2.0575000000000001, 1.7392000000000001, 1.9261999999999999, 1.7975000000000001, 1.6291, 1.8589, 1.6988000000000001, 1.8017000000000001, 0.97709999999999997, 1.6445000000000001, 0.98340000000000005, 1.3485, 1.6131, 0.56720000000000004, 1.278, 1.4245000000000001, 1.5898000000000001, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.2301000000000002, 2.2302, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.1956000000000002, 2.2320000000000002, 2.2035, 2.2324999999999999, 2.2332000000000001, 2.2332999999999998, 2.2332999999999998, 2.1776, 2.2336, 2.2338, 2.2339000000000002, 2.2343000000000002, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.3250999999999999, 2.1659999999999999, 2.1844000000000001, 2.21, 2.2086000000000001, 2.1951999999999998, 2.1861999999999999, 2.2347999999999999, 2.2347999999999999, 2.2063999999999999, 2.2349999999999999, 2.2349999999999999, 2.2349999999999999, 2.2353000000000001, 2.2353000000000001, 2.2050999999999998, 2.2360000000000002, 2.1901999999999999, 2.1716000000000002, 2.1280000000000001, 2.1082999999999998, 2.0434000000000001, 2.0707, 2.1775000000000002, 2.1356999999999999, 2.1185, 2.0015999999999998, 2.1459999999999999, 2.0779999999999998, 1.9829000000000001, 2.0468999999999999, 2.0737999999999999, 2.0545, 1.9469000000000001, 1.9730000000000001, 1.6849000000000001, 1.8191999999999999, 1.9478, 2.0621, 1.6645000000000001, 1.9570000000000001, 1.9407000000000001, 1.7886, 1.7403999999999999, 1.7394000000000001, 1.3875999999999999, 1.7569999999999999, 1.5032000000000001, 1.2196, 1.3985000000000001, 1.1085, 1.2134, 1.1073, 1.2171000000000001, 0.98160000000000003, 1.0638000000000001, 0.58440000000000003, 1.0365, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.2374999999999998, 2.1821999999999999, 2.2385000000000002, 2.2385000000000002, 2.2389000000000001, 2.2389000000000001, 2.2393999999999998, 2.2181000000000002, 2.2395999999999998, 2.2081, 2.2397999999999998, 2.2399, 2.2399, 2.2401, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.3309000000000002, 2.2406000000000001, 2.2406000000000001, 2.2408999999999999, 2.2410999999999999, 2.2414999999999998, 2.2414999999999998, 2.2418, 2.2418, 2.2418, 2.2418, 2.2096, 2.2422, 2.2422, 2.2422, 2.2422, 2.2422, 2.1198000000000001, 2.1861000000000002, 2.2061000000000002, 2.0931999999999999, 2.0497000000000001, 2.0909, 2.1335000000000002, 2.0028999999999999, 2.0958999999999999, 2.1501000000000001, 2.0329000000000002, 2.1518999999999999, 2.1518999999999999, 2.1074999999999999, 2.1606000000000001, 2.1065, 1.9337, 2.0527000000000002, 2.1560999999999999, 1.9399, 2.0718000000000001, 2.0388000000000002, 1.8964000000000001, 1.9601, 1.9094, 2.0426000000000002, 1.8242, 1.7776000000000001, 1.8342000000000001, 1.6178999999999999, 1.6531, 1.7956000000000001, 1.833, 1.9610000000000001, 1.2891999999999999, 1.8415999999999999, 1.3854, 1.8283, 1.9321999999999999, 1.3564000000000001, 1.8519000000000001, 1.3337000000000001, 1.3924000000000001, 1.3520000000000001, 1.0101, 0.7964, 0.30359999999999998, 0.82799999999999996, 0.14419999999999999, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.2341000000000002, 2.2187000000000001, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.2296999999999998, 2.2406999999999999, 2.2284000000000002, 2.2410999999999999, 2.2410999999999999, 2.2414000000000001, 2.2265999999999999, 2.2263999999999999, 2.2416999999999998, 2.2416999999999998, 2.2418999999999998, 2.2423999999999999, 2.2233000000000001, 2.2227000000000001, 2.2427000000000001, 2.2427000000000001, 2.2433000000000001, 2.2210000000000001, 2.2435, 2.2107999999999999, 2.2437, 2.2437, 2.2437, 2.2439, 2.3344999999999998, 2.3344999999999998, 2.3344999999999998, 2.1949000000000001, 2.2444000000000002, 2.2446999999999999, 2.2450000000000001, 2.2004000000000001, 2.1585999999999999, 2.1661999999999999, 2.1556000000000002, 2.1482999999999999, 2.097, 2.1686999999999999, 2.0081000000000002, 2.1501000000000001, 2.0847000000000002, 2.1273, 2.0438000000000001, 2.1078000000000001, 1.8624000000000001, 2.0832999999999999, 2.1392000000000002, 1.8349, 2.1040000000000001, 2.0261, 2.0594000000000001, 1.9134, 2.1053000000000002, 1.9502999999999999, 1.7818000000000001, 1.7552000000000001, 1.827, 1.4945999999999999, 1.7290000000000001, 1.3691, 1.0824, 1.9023000000000001, 1.0718000000000001, 1.861, 1.4208000000000001, 1.5467, 0.80500000000000005, -0.89490000000000003, 1.2073, 0.42620000000000002, 1.6108, 1.3783000000000001, 1.7417, 1.5426, 0.59399999999999997, 1.4483999999999999, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.2496999999999998, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.2404000000000002, 2.2503000000000002, 2.2302, 2.2509999999999999, 2.2362000000000002, 2.2067999999999999, 2.2511999999999999, 2.2204999999999999, 2.2515000000000001, 2.2345999999999999, 2.2517, 2.2522000000000002, 2.2524000000000002, 2.2528000000000001, 2.2528999999999999, 2.2532999999999999, 2.2532999999999999, 2.2534999999999998, 2.2534999999999998, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.3441000000000001, 2.2538, 2.2538, 2.2044999999999999, 2.2248000000000001, 2.2543000000000002, 2.2547000000000001, 2.2017000000000002, 2.2549999999999999, 2.2549999999999999, 2.2549999999999999, 2.2056, 2.1375000000000002, 2.1536, 2.1878000000000002, 2.1766000000000001, 2.0905999999999998, 2.1211000000000002, 2.1354000000000002, 2.1404999999999998, 1.944, 1.9676, 2.0575000000000001, 2.0377999999999998, 2.0238999999999998, 2.0746000000000002, 2.0036999999999998, 1.9815, 2.1206, 2.0676999999999999, 1.7493000000000001, 2.0615999999999999, 1.9245000000000001, 1.827, 1.9576, 2.0095000000000001, 1.7896000000000001, 1.4776, 1.5104, 1.8468, 1.8826000000000001, 1.8859999999999999, 0.91639999999999999, 1.3069999999999999, 1.4303999999999999, 1.7025999999999999, 1.2608999999999999, 1.7039, 1.5128999999999999, 1.2099, 1.4715, 0.42299999999999999, 1.2511000000000001, 1.151, 0.74019999999999997, -0.10979999999999999, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.2330000000000001, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.2393000000000001, 2.2507999999999999, 2.2178, 2.2235999999999998, 2.2517999999999998, 2.2343000000000002, 2.1960999999999999, 2.2523, 2.2334999999999998, 2.202, 2.2528000000000001, 2.2530000000000001, 2.2530999999999999, 2.2530999999999999, 2.1972, 2.2147999999999999, 2.2536999999999998, 2.2199, 2.2538999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.3443999999999998, 2.1825000000000001, 2.2189999999999999, 2.1678000000000002, 2.2541000000000002, 2.2541000000000002, 2.2544, 2.2547000000000001, 2.2549999999999999, 2.2553999999999998, 2.2553999999999998, 2.1532, 2.2050000000000001, 2.1172, 2.1583000000000001, 2.1819000000000002, 2.2197, 2.0554999999999999, 2.1854, 2.0876000000000001, 2.1221000000000001, 1.7897000000000001, 1.9137999999999999, 1.7836000000000001, 1.9550000000000001, 1.8691, 1.9844999999999999, 1.6829000000000001, 1.9280999999999999, 1.6099000000000001, 1.5740000000000001, 1.3625, 1.8603000000000001, 1.6551, 1.6839999999999999, 1.1259999999999999, 1.3119000000000001, 1.4357, 1.4829000000000001, 1.3849, 1.4932000000000001, 0.91930000000000001, 0.58689999999999998, 1.5737000000000001, 1.1021000000000001, 1.1047, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.2587000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.2360000000000002, 2.2227000000000001, 2.2595000000000001, 2.2595999999999998, 2.2595999999999998, 2.1821000000000002, 2.2324999999999999, 2.2601, 2.2604000000000002, 2.2610999999999999, 2.2612000000000001, 2.2414000000000001, 2.2616999999999998, 2.2616999999999998, 2.2622, 2.2625999999999999, 2.1634000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.3532000000000002, 2.2381000000000002, 2.2366999999999999, 2.2362000000000002, 2.2627999999999999, 2.222, 2.2633999999999999, 2.2633999999999999, 2.2633999999999999, 2.2637, 2.2637, 2.2637, 2.2637, 2.1227999999999998, 2.1859000000000002, 1.9685999999999999, 2.2092000000000001, 2.1465999999999998, 2.2273999999999998, 2.1084999999999998, 2.1680000000000001, 2.1579000000000002, 2.1749000000000001, 2.1627000000000001, 1.9016, 2.0768, 2.1303999999999998, 2.0405000000000002, 1.98, 2.0162, 2.0459999999999998, 1.3071999999999999, 1.6725000000000001, 1.5230999999999999, 1.7219, 1.8778999999999999, 1.7584, 1.5117, 1.9713000000000001, 1.6192, 1.6951000000000001, 1.7091000000000001, 1.4125000000000001, 1.8951, 1.2527999999999999, 1.6087, 1.2758, 1.3689, 1.3204, 1.6329, 1.0219, 0.495, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.2608999999999999, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.2185000000000001, 2.2610999999999999, 2.2612999999999999, 2.2616000000000001, 2.2616999999999998, 2.2616999999999998, 2.2616999999999998, 2.2616999999999998, 2.2616999999999998, 2.2618, 2.2494000000000001, 2.2363, 2.2621000000000002, 2.2622, 2.2622, 2.2235, 2.2631000000000001, 2.2631999999999999, 2.2633999999999999, 2.2643, 2.2646999999999999, 2.2648999999999999, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.3555000000000001, 2.1999, 2.1838000000000002, 2.2650999999999999, 2.2654000000000001, 2.266, 2.266, 2.2130000000000001, 2.2189000000000001, 2.1758000000000002, 2.1926000000000001, 2.2332999999999998, 2.1347999999999998, 2.1600999999999999, 2.0625, 2.0979000000000001, 2.1699000000000002, 2.0872000000000002, 2.0084, 2.0825, 1.9487000000000001, 1.9392, 2.1355, 1.6679999999999999, 1.7797000000000001, 1.9592000000000001, 1.6785000000000001, 1.8213999999999999, 1.5073000000000001, 1.2914000000000001, 2.0596000000000001, 1.4112, 1.8914, 1.8611, 1.6355, 1.5219, 1.1202000000000001, 1.4207000000000001, 1.6095999999999999, 1.1456999999999999, 0.56079999999999997, 1.1077999999999999, 0.80620000000000003, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.2696999999999998, 2.2700999999999998, 2.2705000000000002, 2.2707999999999999, 2.2709000000000001, 2.2555000000000001, 2.2448999999999999, 2.2719999999999998, 2.2728000000000002, 2.2730000000000001, 2.2730000000000001, 2.2732000000000001, 2.2496999999999998, 2.2239, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.3639999999999999, 2.2275999999999998, 2.2736999999999998, 2.2738999999999998, 2.2738999999999998, 2.2742, 2.2433999999999998, 2.2078000000000002, 2.2753000000000001, 2.2757999999999998, 2.2757999999999998, 2.2757999999999998, 2.2401, 2.2770000000000001, 2.2770000000000001, 2.2770000000000001, 2.2770000000000001, 2.2382, 2.2382, 2.2370999999999999, 2.2360000000000002, 1.9758, 2.0158, 2.2145000000000001, 2.0794000000000001, 2.1053999999999999, 1.6989000000000001, 2.1442999999999999, 1.8834, 1.8402000000000001, 2.0548999999999999, 2.0945999999999998, 1.9099999999999999, 2.0337000000000001, 1.9529000000000001, 1.6778999999999999, 1.1721999999999999, 1.728, 2.1110000000000002, 1.3323, 1.8122, 1.8835999999999999, 1.5507, 1.4436, 1.6751, 1.6455, 1.6182000000000001, 0.72570000000000001, 1.3709, 1.7629999999999999, 1.0317000000000001, 1.4779, 1.5672999999999999, 0.87250000000000005, 1.0145, 0.2777, 0.4929, 0.89019999999999999, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.2519999999999998, 2.2696000000000001, 2.2385999999999999, 2.2827999999999999, 2.2831999999999999, 2.2841999999999998, 2.2845, 2.2846000000000002, 2.2850999999999999, 2.286, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.3765000000000001, 2.1659999999999999, 2.2867999999999999, 2.2871000000000001, 2.2559, 2.2875000000000001, 2.2875000000000001, 2.2875000000000001, 2.2884000000000002, 2.2888999999999999, 2.2888999999999999, 2.2894999999999999, 2.2894999999999999, 2.2894999999999999, 2.2902999999999998, 2.1890000000000001, 2.2911999999999999, 2.2911999999999999, 2.2911999999999999, 2.2923, 2.2923, 2.2923, 2.2923, 2.1798999999999999, 2.173, 2.218, 2.1490999999999998, 2.2395, 1.7882, 2.2395, 2.2395, 2.2134999999999998, 1.9750000000000001, 2.1758999999999999, 1.7309000000000001, 1.9646999999999999, 2.0543, 2.0036999999999998, 2.0440999999999998, 2.1326999999999998, 1.8813, 1.7950999999999999, 1.8083, 1.9200999999999999, 1.962, 1.8835, 1.8413999999999999, 1.9031, 1.6892, 1.9011, 1.7588999999999999, 1.8080000000000001, 1.7987, 1.3604000000000001, 1.4464999999999999, 1.2906, 1.8339000000000001, 1.7401, 1.2461, 1.4981, 1.1564000000000001, 1.456, 0.17810000000000001, 1.5398000000000001, 0.78190000000000004], \"Freq\": [11394.0, 3382.0, 2046.0, 1381.0, 1505.0, 1264.0, 1376.0, 1033.0, 782.0, 1109.0, 892.0, 734.0, 725.0, 1018.0, 669.0, 953.0, 651.0, 565.0, 755.0, 807.0, 611.0, 1255.0, 644.0, 1199.0, 865.0, 554.0, 634.0, 761.0, 671.0, 498.0, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 279.11036852412252, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 322.19546491219444, 113.19517977957645, 41.127830325336454, 130.15265761360695, 34.131839821764167, 33.132543556291694, 78.215948230427202, 58.140452538810656, 31.134079005056901, 31.134079005056901, 28.136766132726724, 28.136766132726724, 25.140050559472396, 25.140050559472396, 24.141313847276066, 136.17808189422607, 67.171113499036167, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 0.55199108432873012, 163.16268043454335, 43.149448908402491, 80.163480846688671, 488.11950223438345, 37.156585977092504, 18.151547435758946, 18.151547435758946, 18.151547435758946, 17.153881650761814, 16.156475326976256, 16.156475326976256, 15.15937427111362, 203.22168294781224, 837.31925671949261, 82.195524344515547, 304.59797964557652, 700.91464628915799, 111.56940240947151, 265.30530414824165, 496.96178492582408, 84.212002087524084, 73.222979558486813, 958.16720517482543, 131.41849902140754, 125.50504951784512, 170.96330902988254, 310.42924937253082, 97.257890189377434, 109.70824490447099, 73.332777893022694, 254.63571329581831, 114.00207095465942, 163.60095425231754, 244.33165053176108, 119.81761898336303, 158.26641585070132, 131.47839406887448, 368.14120133616905, 146.67074389960794, 277.11580375281187, 177.83664116879046, 142.71643784949848, 267.46065900830496, 172.88857181735597, 156.06843555458124, 137.42107742696132, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 267.10905949661958, 209.10996808054068, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 399.1330718313211, 43.125514527697156, 108.1771770098762, 35.129787690977388, 28.135396576437071, 27.136418929477262, 27.136418929477262, 165.25863736586373, 25.138690873550939, 24.139957948299489, 23.141326412364588, 21.144420420455983, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 0.55158697671460477, 134.41938438897964, 83.19070871458257, 44.170843870348541, 41.151820590397172, 53.150062418773302, 64.157940185128282, 19.148103924950156, 19.148103924950156, 37.148444204648776, 18.150222019541051, 18.150222019541051, 18.150222019541051, 17.152563139001217, 17.152563139001217, 35.189795787186299, 15.158071935154149, 46.160768651256255, 64.212448729529797, 138.20784144770988, 184.98076920680043, 506.78844576983391, 300.13830231843048, 53.241848857025857, 79.19427816808971, 83.178294976244558, 217.13494437943467, 55.205100315512951, 102.23167346654652, 220.18525835712884, 123.28674520045988, 99.614767428412961, 108.27266771306401, 229.94671362978502, 183.34429902348143, 642.40981272931845, 304.58618761179855, 149.3684381721728, 86.286660430476388, 359.39499522882022, 107.32325225795908, 110.30599563425281, 143.94012615698304, 154.41020547030476, 152.2618249781591, 273.20500463192786, 146.36714246570298, 202.50187851192817, 268.78137727685805, 168.03337103774965, 184.94440618164492, 170.67534810954132, 174.59319820972618, 162.72441418247823, 177.84833470850455, 165.51686577726187, 171.80185292424585, 149.7319953049323, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 53.118565358953298, 234.09207084439831, 34.127161108705295, 33.127872052216098, 29.131174533224875, 29.131174533224875, 25.135455609751482, 50.130203250293292, 24.136731717918163, 67.136391809354819, 23.138109924048322, 22.139602991599752, 22.139602991599752, 21.141225903982097, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 0.55062763471820808, 19.144935468341274, 19.144935468341274, 18.14706849956411, 17.149426087332664, 16.152045662478525, 16.152045662478525, 15.154973464268977, 15.154973464268977, 15.154973464268977, 15.154973464268977, 33.176268884499258, 14.158267293435422, 14.158267293435422, 14.158267293435422, 14.158267293435422, 14.158267293435422, 274.98894639706504, 56.158379952167678, 29.156923418437295, 240.03243733646013, 386.81619490160307, 191.74232205759429, 91.337515566107498, 529.05134033244065, 145.14661979518712, 65.206613304430533, 310.65476507166176, 55.168312655072583, 55.218760979908851, 86.169257899961977, 49.180222125819682, 78.173179864004524, 304.7267912945324, 113.20940109096956, 46.253399893814411, 266.35485568943449, 77.202593789463222, 95.190497665691566, 224.1674020417648, 131.75912739043315, 163.05424945156491, 83.222235068700471, 205.14099237064036, 239.02666453344364, 187.27534800895708, 406.11198302464555, 353.00253752262677, 205.1703644896011, 149.09777123320916, 103.9883884357502, 486.06564410070109, 125.226074563949, 313.10154732062239, 126.59941887633313, 103.22239174904422, 195.07818360244363, 112.4044316392872, 188.27644418542093, 165.4559575098996, 173.01384431324954, 215.0556316216389, 207.27707780216073, 186.25773144000405, 145.45054903476219, 147.26637709340753, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 177.10942617123561, 333.33673246561574, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 93.126660801292175, 64.118180644023525, 82.126890531618471, 50.121816392133603, 50.121816392133603, 44.124052107384479, 70.128066568530684, 69.133188018468658, 39.126415115497153, 38.126958615654033, 36.128131442346934, 30.132529671043955, 55.132506805660462, 53.13594269012637, 27.135411379257054, 27.135411379257054, 23.140329695940839, 48.138861728871291, 22.141815487375382, 65.135446260280361, 21.143430504346501, 21.143430504346501, 21.143430504346501, 20.14519237226579, 0.55128936978099019, 0.55128936978099019, 0.55128936978099019, 63.187138205384215, 18.149244792130141, 17.151591008729483, 16.154197983825274, 47.1504158177489, 105.20491084265792, 90.349007616498881, 101.21001179007776, 103.20128806307609, 175.20148566486807, 66.273394806765765, 438.92411492888527, 73.324747350568956, 154.48453338356904, 91.199762287616778, 207.30789268331347, 105.1930815156782, 521.06291925541939, 105.23349754383284, 74.217568800396236, 378.37163018241711, 88.426389725373127, 129.25992606384605, 105.33477750117027, 183.42050407853748, 82.210080559767036, 124.21839784421317, 167.1664206277861, 161.19126682925398, 132.36503673252332, 234.26993069992241, 149.52192296256999, 249.87053298150968, 375.88078648849466, 111.6456060946205, 344.35676245951714, 114.37673823237169, 163.50894146046207, 138.75321699961756, 226.40334411620896, 511.71404909259473, 145.31901979772971, 165.74590985127077, 121.22960403522002, 128.02043373486981, 112.32607992391425, 116.8225873098426, 129.15502481435695, 115.52091335771435, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 112.11539886802382, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 106.15256806861828, 63.12125588203385, 100.14098047091663, 45.126475048198294, 70.17088538327468, 183.15812725609445, 40.128718881616585, 100.22406634483308, 36.130939181121029, 62.139378984177355, 33.132937523297244, 28.137156471621442, 27.138174884403899, 24.141700297403187, 23.143063514500128, 21.146145688562431, 21.146145688562431, 20.147896996736211, 20.147896996736211, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 0.55210636076899511, 19.149815156789646, 19.149815156789646, 63.237443096821146, 36.15754148280319, 17.154257455006952, 16.156848952625921, 59.164481636002471, 15.159745469222278, 15.159745469222278, 15.159745469222278, 42.173498919623036, 117.26853505995112, 88.453987858326158, 45.222487756045496, 51.190213860005052, 150.36395166832426, 101.26677888392852, 73.284098526459104, 68.239425716900584, 443.57039073231942, 327.22945434411804, 136.47931303834568, 156.31290582736861, 169.34182993833932, 114.23653800568054, 157.24572745033032, 165.62006830154095, 64.282338873139182, 79.295703637631917, 312.3987642892007, 81.369582014466829, 137.49751325258481, 196.00156766903785, 114.35885314556043, 91.608729222292936, 149.73116915008868, 276.667410833654, 256.84049544626288, 122.33657164076206, 115.09732235379708, 113.81250750469424, 397.29436233220201, 221.70413004413609, 163.77545046809399, 124.67624910410613, 162.96908720261325, 124.45401395509549, 134.96338518157896, 154.51072003126535, 125.41039811823444, 151.71950073700179, 125.63636010968565, 126.64914214136365, 126.90401130370273, 131.05840401237484, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 301.11832634953475, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 90.120137188186405, 59.119269509499567, 153.20993673763169, 111.1306871827059, 36.128113762144878, 59.129514832936344, 199.12694700663153, 31.131668007064938, 56.138905656933829, 142.15272723744161, 26.136491836423151, 25.137671031712159, 24.138940950588257, 24.138940950588257, 147.23073982717781, 82.149398531214956, 21.143413407828447, 63.215913235916076, 20.145175342338632, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 0.55128423248517422, 157.21998408634695, 61.153486057885239, 163.12455475710181, 19.147105117372579, 19.147105117372579, 18.149227914875063, 17.151574219553215, 16.154181292247454, 15.157095163458184, 15.157095163458184, 127.24009286837, 41.170871824862523, 218.13238835183427, 82.217952576162773, 55.315711196116808, 29.161275365391464, 195.26946070267005, 43.177072013910923, 101.25981591111682, 72.223711089074101, 872.88889314308881, 327.77112191045501, 637.72566008832644, 177.31290686080996, 282.76181733876291, 124.21546945345689, 457.82038596984387, 136.47291983978474, 352.85554181490596, 315.08016578733782, 311.08684761056139, 106.44754811039273, 143.26386084691416, 137.26889747001607, 305.43639651306592, 222.0420552748665, 184.71128577354921, 169.6113791163221, 187.01286881106239, 167.31308067874699, 251.32195865017619, 238.16236176297551, 148.14926748965198, 162.57771921750614, 148.5478023952941, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 114.1171761411879, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 266.28907437207602, 220.27006774577481, 59.12403259809745, 56.124780636951854, 55.125047484160767, 671.37840204559859, 112.16353358090034, 44.128748698315476, 38.131625756525679, 30.13714062030941, 29.138030176289867, 53.142179215734934, 25.142247564275458, 25.142247564275458, 22.146333527013091, 20.149676856935763, 513.40982888632516, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 0.55264520881060253, 44.153026693634708, 41.156736560619301, 40.152304598080825, 19.151587440088726, 51.165274888231181, 17.156012252582162, 17.156012252582162, 17.156012252582162, 16.158593589410874, 16.158593589410874, 16.158593589410874, 16.158593589410874, 434.94482939303168, 78.215878958138347, 1034.0908153371474, 37.185246829246829, 83.266398243769757, 28.174477563643624, 110.31217179846163, 51.254422598496362, 55.340606251500461, 44.282549587346395, 48.349091905643817, 237.74185659934813, 79.307893699310739, 57.378857673374462, 85.431779705820617, 111.45309707776573, 90.406127712969678, 79.681631877363714, 1329.9610735592009, 322.84128022526113, 457.95010750356624, 199.75531373507945, 122.57462972514642, 169.37417186100558, 283.91694246149689, 89.464372982055338, 180.79307147756271, 147.91219862250034, 141.69275258618828, 200.69559406858895, 97.561582668648512, 195.16463872934111, 130.134785762447, 126.42341086355232, 116.56693806507919, 116.67660727090801, 99.744030149198792, 117.1106672581373, 103.41188552677994, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 127.11726662938051, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 749.32340670593669, 108.1184454910529, 86.120446529102438, 69.122842262971417, 65.123582675343044, 64.123781803616552, 63.123987059898333, 63.123987059898333, 63.123987059898333, 60.124642562458057, 82.133291240085555, 119.15818290147365, 51.12705440490069, 47.128410785078863, 47.128410785078863, 157.19522993218467, 33.135600328660118, 32.13634049833891, 30.13795966481355, 23.145667749742262, 21.148732224625871, 20.150473502758068, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 0.5528867012616665, 110.22910684125004, 149.24382256276024, 19.152380699406145, 18.154478715348535, 16.159374491821609, 16.159374491821609, 59.184670938782141, 44.176012087651394, 100.314727660123, 69.377767073042548, 32.169223806103567, 133.48486011648166, 93.287119351304455, 273.576336579128, 181.10757651287213, 72.257933964237793, 152.57545198020529, 257.64078386576153, 127.39570904252986, 224.37251270572634, 182.94796734471311, 81.451202857636673, 408.47439067639152, 277.17531516094385, 142.07695763565738, 252.27432549068476, 174.78535372475761, 349.33502327647597, 456.27120137130566, 93.42038186104574, 287.76299815889308, 122.59454887441568, 120.66542355216949, 159.66128353387984, 181.9696786253721, 259.44994394128133, 177.1794842583445, 146.50470190221063, 186.35468362999882, 186.78508261589286, 152.0089140238442, 152.24747538810581, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 94.088373236828261, 71.091460537508823, 54.095371773295945, 47.097770757999633, 44.099022298123273, 67.117045302313215, 78.059217377963208, 29.108910992435458, 23.116186225958604, 22.117752106582206, 22.117752106582206, 21.119454023996582, 46.088327104150046, 105.98903383963226, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 0.54416964038242976, 68.060138110402946, 19.123343625266191, 18.125579833775063, 18.125579833775063, 17.128051167475846, 34.109690692798438, 77.002973152509469, 14.137316189819957, 13.141226913316494, 13.141226913316494, 13.141226913316494, 30.114103679395125, 11.150850339335545, 11.150850339335545, 11.150850339335545, 11.150850339335545, 28.151726648358537, 28.117057231708134, 27.120268068335644, 26.11721342723671, 937.09702588982566, 496.07277744063435, 33.101580387115654, 175.49281349365282, 127.68429411006791, 1887.4799447666512, 51.057276931307861, 317.15012732516402, 406.51926026434688, 94.83403774696032, 71.951779177222193, 247.38716503961297, 107.76424164263088, 135.34753393131095, 472.94717721082714, 3825.5519854749227, 238.13811239688107, 51.045297301697083, 806.10237977413271, 135.90518827672895, 102.46912214105099, 254.21979551560815, 246.33352876206555, 151.73033288593339, 142.23397111832003, 143.47123605355185, 287.83801758884039, 133.36530965085848, 95.507391344890991, 159.71991799614429, 113.96439695677375, 106.95468464011083, 137.95988672577056, 127.4268942560926, 124.96592906738931, 121.20206783121571, 106.57789190398164, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 298.77485439002243, 161.06416468756646, 271.64247470139605, 63.082128673637122, 51.085479917257445, 33.094811504886032, 30.097386370563395, 29.098355289401287, 25.102947814709371, 20.111033438254697, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 0.54115701294033169, 650.17566146810702, 17.117923988650766, 16.120730324872568, 34.111064328557177, 15.12386618480158, 15.12386618480158, 15.12386618480158, 13.131389521626566, 12.135955384519836, 12.135955384519836, 11.141221926363233, 11.141221926363233, 11.141221926363233, 10.147363841118496, 121.82518246726866, 9.1546191596459447, 9.1546191596459447, 9.1546191596459447, 8.1633208149927974, 8.1633208149927974, 8.1633208149927974, 8.1633208149927974, 72.919856855109501, 67.928067691690941, 28.085444967090393, 68.861439335735298, 20.120200880773659, 6918.4440329979088, 20.123355781288858, 20.115910534340074, 26.098466943378188, 299.3524158941031, 34.049414790493373, 1173.0646260088795, 148.63207147333361, 76.677115525261854, 107.35453115811164, 76.653541898037417, 42.045542174536401, 195.87704157635241, 303.45052946979672, 261.48454198990373, 130.86174408040603, 104.16643064118237, 140.7353977481815, 156.84391179662242, 113.09581315991264, 235.38003265038111, 95.368510424821409, 145.51692982267036, 119.81407582311799, 108.91931189114884, 281.88307430611883, 223.74158163857129, 202.10775191600069, 97.132044734983069, 104.60574477774092, 159.72301293894907, 119.89328015722637, 130.38847902142604, 113.28021198432735, 121.6962206769149, 106.77373432087623, 110.45876392229728], \"Total\": [11394, 3382, 2046, 1381, 1505, 1264, 1376, 1033, 782, 1109, 892, 734, 725, 1018, 669, 953, 651, 565, 755, 807, 611, 1255, 644, 1199, 865, 554, 634, 761, 671, 498, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 280, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 339, 117, 42, 136, 35, 34, 81, 60, 32, 32, 29, 29, 26, 26, 25, 144, 70, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 175, 45, 85, 554, 39, 19, 19, 19, 18, 17, 17, 16, 231, 1033, 90, 364, 892, 126, 327, 651, 95, 82, 1505, 159, 151, 216, 441, 116, 134, 86, 407, 151, 247, 433, 170, 261, 198, 1255, 258, 938, 420, 259, 1381, 438, 339, 254, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 268, 210, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 414, 44, 112, 36, 29, 28, 28, 175, 26, 25, 24, 22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 144, 88, 46, 43, 56, 68, 20, 20, 39, 19, 19, 19, 18, 18, 37, 16, 49, 69, 154, 209, 611, 353, 57, 88, 94, 274, 61, 120, 283, 149, 117, 130, 304, 238, 1109, 460, 199, 103, 632, 142, 147, 224, 253, 250, 634, 236, 420, 739, 386, 568, 470, 538, 449, 617, 533, 892, 495, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 54, 248, 35, 34, 30, 30, 26, 52, 25, 70, 24, 23, 23, 22, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 20, 20, 19, 18, 17, 17, 16, 16, 16, 16, 35, 15, 15, 15, 15, 15, 310, 60, 31, 278, 465, 222, 102, 669, 168, 72, 381, 61, 61, 99, 54, 90, 414, 137, 51, 359, 92, 117, 316, 174, 226, 102, 311, 378, 281, 755, 634, 320, 223, 137, 1255, 187, 721, 191, 141, 470, 166, 466, 386, 414, 736, 865, 1264, 597, 1199, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 179, 341, 1, 1, 1, 1, 1, 1, 95, 65, 84, 51, 51, 45, 72, 71, 40, 39, 37, 31, 57, 55, 28, 28, 24, 50, 23, 68, 22, 22, 22, 21, 1, 1, 1, 67, 19, 18, 17, 50, 115, 98, 111, 114, 203, 72, 552, 81, 181, 103, 253, 121, 761, 124, 83, 568, 102, 161, 127, 255, 95, 167, 264, 263, 201, 495, 250, 597, 1199, 157, 1109, 168, 372, 278, 953, 11394, 410, 1018, 227, 303, 186, 233, 669, 256, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 113, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 108, 64, 103, 46, 72, 192, 41, 104, 37, 64, 34, 29, 28, 25, 24, 22, 22, 21, 21, 1, 1, 1, 1, 20, 20, 67, 38, 18, 17, 63, 16, 16, 16, 45, 132, 98, 49, 56, 177, 116, 83, 77, 601, 432, 166, 194, 213, 137, 202, 217, 74, 96, 511, 99, 191, 299, 154, 117, 236, 597, 538, 184, 166, 164, 1505, 567, 372, 216, 438, 214, 282, 427, 270, 938, 342, 381, 570, 1376, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 307, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 92, 60, 159, 115, 37, 61, 211, 32, 58, 150, 27, 26, 25, 25, 156, 86, 22, 66, 21, 1, 1, 1, 1, 1, 1, 1, 169, 64, 178, 20, 20, 19, 18, 17, 16, 16, 141, 44, 250, 91, 60, 31, 238, 47, 120, 83, 1376, 456, 1018, 239, 414, 163, 807, 189, 671, 617, 755, 158, 261, 243, 938, 567, 413, 366, 444, 358, 953, 1255, 291, 501, 462, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 115, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 273, 229, 60, 57, 56, 725, 116, 45, 39, 31, 30, 55, 26, 26, 23, 21, 565, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 46, 43, 42, 20, 54, 18, 18, 18, 17, 17, 17, 17, 498, 85, 1381, 40, 94, 30, 129, 57, 62, 49, 54, 340, 96, 66, 107, 148, 116, 99, 3382, 573, 953, 342, 180, 277, 597, 120, 343, 260, 246, 466, 141, 533, 249, 334, 280, 299, 187, 403, 601, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 128, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 782, 109, 87, 70, 66, 65, 64, 64, 64, 61, 84, 123, 52, 48, 48, 164, 34, 33, 31, 24, 22, 21, 1, 1, 1, 1, 1, 118, 162, 20, 19, 17, 17, 63, 47, 110, 75, 34, 152, 104, 334, 213, 80, 182, 332, 153, 306, 251, 93, 736, 448, 192, 451, 270, 739, 1199, 115, 671, 178, 179, 299, 381, 807, 410, 280, 555, 1018, 472, 651, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 95, 72, 55, 48, 45, 69, 81, 30, 24, 23, 23, 22, 48, 112, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 72, 20, 19, 19, 18, 36, 83, 15, 14, 14, 14, 32, 12, 12, 12, 12, 30, 30, 29, 28, 1264, 644, 36, 214, 152, 3382, 59, 472, 631, 119, 87, 358, 138, 188, 865, 11394, 414, 61, 2046, 218, 153, 521, 573, 279, 265, 280, 1376, 334, 161, 555, 256, 220, 570, 456, 938, 721, 433, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 309, 164, 285, 64, 52, 34, 31, 30, 26, 21, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 734, 18, 17, 36, 16, 16, 16, 14, 13, 13, 12, 12, 12, 11, 135, 10, 10, 10, 9, 9, 9, 9, 82, 77, 31, 80, 22, 11394, 22, 22, 29, 409, 39, 2046, 206, 98, 144, 99, 50, 297, 501, 427, 191, 146, 213, 247, 168, 434, 142, 248, 196, 180, 721, 521, 555, 155, 184, 462, 265, 413, 265, 1033, 229, 511], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -4.8672000000000004, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -4.7244000000000002, -5.7652999999999999, -6.7622999999999998, -5.6264000000000003, -6.9442000000000004, -6.9730999999999996, -6.1317000000000004, -6.4231999999999996, -7.0335000000000001, -7.0335000000000001, -7.1317000000000004, -7.1317000000000004, -7.2404999999999999, -7.2404999999999999, -7.2794999999999996, -5.5816999999999997, -6.2813999999999997, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -10.4071, -5.4019000000000004, -6.7159000000000004, -6.1067, -4.3139000000000003, -6.8621999999999996, -7.5526999999999997, -7.5526999999999997, -7.5526999999999997, -7.6064999999999996, -7.6632999999999996, -7.6632999999999996, -7.7236000000000002, -5.1837999999999997, -3.7759999999999998, -6.0823, -4.7817999999999996, -3.9497, -5.7830000000000004, -4.9184999999999999, -4.2915999999999999, -6.0586000000000002, -6.1970000000000001, -3.6374, -5.6188000000000002, -5.6653000000000002, -5.3601000000000001, -4.7622999999999998, -5.9164000000000003, -5.8010000000000002, -6.1970000000000001, -4.9607000000000001, -5.7652999999999999, -5.4019000000000004, -5.0088999999999997, -5.7141000000000002, -5.4455, -5.6188000000000002, -4.5968, -5.5113000000000003, -4.8815999999999997, -5.3200000000000003, -5.5388000000000002, -4.9109999999999996, -5.3484999999999996, -5.4583000000000004, -5.5816999999999997, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -4.8876999999999997, -5.1314000000000002, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -4.4873000000000003, -6.6925999999999997, -5.7868000000000004, -6.8926999999999996, -7.1082999999999998, -7.1432000000000002, -7.1432000000000002, -5.3663999999999996, -7.2171000000000003, -7.2561, -7.2968000000000002, -7.3834, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -10.383699999999999, -5.5730000000000004, -6.0469999999999997, -6.6700999999999997, -6.7389999999999999, -6.4882, -6.3030999999999997, -7.4782999999999999, -7.4782999999999999, -6.8388, -7.5293000000000001, -7.5293000000000001, -7.5293000000000001, -7.5831, -7.5831, -6.8926999999999996, -7.7001999999999997, -6.6266999999999996, -6.3030999999999997, -5.5438000000000001, -5.2580999999999998, -4.2503000000000002, -4.7716000000000003, -6.4882, -6.0956999999999999, -6.0469999999999997, -5.0941000000000001, -6.4519000000000002, -5.8433000000000002, -5.0804, -5.6578999999999997, -5.8727999999999998, -5.7868000000000004, -5.0449000000000002, -5.2634999999999996, -4.0126999999999997, -4.7584, -5.4676999999999998, -6.0119999999999996, -4.5953999999999997, -5.7960000000000003, -5.7775999999999996, -5.5084999999999997, -5.4348999999999998, -5.4478999999999997, -4.8692000000000002, -5.4878999999999998, -5.1653000000000002, -4.8838999999999997, -5.3544999999999998, -5.2580999999999998, -5.3426, -5.3136999999999999, -5.3846999999999996, -5.3022999999999998, -5.3663999999999996, -5.3308999999999997, -5.4676999999999998, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -6.4823000000000004, -5.0132000000000003, -6.915, -6.9439000000000002, -7.0686999999999998, -7.0686999999999998, -7.2111999999999998, -6.5393999999999997, -7.2503000000000002, -6.2522000000000002, -7.2910000000000004, -7.3333000000000004, -7.3333000000000004, -7.3776000000000002, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -10.3779, -7.4725000000000001, -7.4725000000000001, -7.5235000000000003, -7.5773000000000001, -7.6341000000000001, -7.6341000000000001, -7.6943999999999999, -7.6943999999999999, -7.6943999999999999, -7.6943999999999999, -6.9439000000000002, -7.7584999999999997, -7.7584999999999997, -7.7584999999999997, -7.7584999999999997, -7.7584999999999997, -4.8524000000000003, -6.4283999999999999, -7.0686999999999998, -4.9880000000000004, -4.5171000000000001, -5.2152000000000003, -5.9503000000000004, -4.2000999999999999, -5.4889000000000001, -6.282, -4.7331000000000003, -6.4459999999999997, -6.4459999999999997, -6.0061, -6.5591999999999997, -6.1025, -4.7492999999999999, -5.7361000000000004, -6.6208999999999998, -4.8856000000000002, -6.1151999999999997, -5.9077999999999999, -5.0566000000000004, -5.5895999999999999, -5.3788, -6.0411999999999999, -5.1448, -4.9962999999999997, -5.2362000000000002, -4.4641000000000002, -4.6036000000000001, -5.1448, -5.4686000000000003, -5.8277999999999999, -4.2847, -5.6360999999999999, -4.7427999999999999, -5.6281999999999996, -5.8277999999999999, -5.1997, -5.7449000000000003, -5.2309000000000001, -5.3605999999999998, -5.3308999999999997, -5.0974000000000004, -5.1497000000000002, -5.2630999999999997, -5.4889000000000001, -5.4752999999999998, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -5.2873000000000001, -4.6581999999999999, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -10.3743, -5.9252000000000002, -6.2937000000000003, -6.0495999999999999, -6.5358000000000001, -6.5358000000000001, -6.6607000000000003, -6.2054999999999998, -6.2196999999999996, -6.7782, -6.8034999999999997, -6.8559999999999999, -7.0324, -6.4424999999999999, -6.4787999999999997, -7.1337999999999999, -7.1337999999999999, -7.2873999999999999, -6.5757000000000003, -7.3297999999999996, -6.2784000000000004, -7.3739999999999997, -7.3739999999999997, -7.3739999999999997, -7.4203000000000001, -10.3743, -10.3743, -10.3743, -6.3091999999999997, -7.5198999999999998, -7.5736999999999997, -7.6304999999999996, -6.5963000000000003, -5.8052000000000001, -5.9577, -5.8437000000000001, -5.8243, -5.2986000000000004, -6.2633999999999999, -4.3872, -6.1642000000000001, -5.4255000000000004, -5.9466999999999999, -5.1315999999999997, -5.8052000000000001, -4.2117000000000004, -5.8052000000000001, -6.1508000000000003, -4.5317999999999996, -5.9798, -5.6013000000000002, -5.8052000000000001, -5.2541000000000002, -6.0495999999999999, -5.6405000000000003, -5.351, -5.3814000000000002, -5.5785, -5.0095999999999998, -5.4583000000000004, -4.9477000000000002, -4.5370999999999997, -5.7502000000000004, -4.6257999999999999, -5.7237999999999998, -5.3691000000000004, -5.5343999999999998, -5.0442, -4.2628000000000004, -5.4852999999999996, -5.3570000000000002, -5.673, -5.6167999999999996, -5.7412999999999998, -5.7152000000000003, -5.609, -5.7152000000000003, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -5.7317, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -5.7862, -6.2995999999999999, -5.8438999999999997, -6.6292, -6.1959, -5.2445000000000004, -6.7439999999999998, -5.8438999999999997, -6.8464, -6.3152999999999997, -6.9306999999999999, -7.0892999999999997, -7.1242000000000001, -7.2370999999999999, -7.2778, -7.3643999999999998, -7.3643999999999998, -7.4107000000000003, -7.4107000000000003, -10.364699999999999, -10.364699999999999, -10.364699999999999, -10.364699999999999, -7.4592999999999998, -7.4592999999999998, -6.2995999999999999, -6.8464, -7.5640999999999998, -7.6208999999999998, -6.3639999999999999, -7.6811999999999996, -7.6811999999999996, -7.6811999999999996, -6.6965000000000003, -5.6885000000000003, -5.9702000000000002, -6.6292, -6.5068000000000001, -5.4420999999999999, -5.8339999999999996, -6.1546000000000003, -6.2244000000000002, -4.3662000000000001, -4.6727999999999996, -5.5392999999999999, -5.4031000000000002, -5.3235999999999999, -5.7141999999999999, -5.3967999999999998, -5.3474000000000004, -6.2840999999999996, -6.0766999999999998, -4.7230999999999996, -6.0521000000000003, -5.532, -5.1814, -5.7141999999999999, -5.9371, -5.4554, -4.8391999999999999, -4.9104999999999999, -5.6470000000000002, -5.7141999999999999, -5.7229000000000001, -4.4757999999999996, -5.0613999999999999, -5.3594999999999997, -5.6308999999999996, -5.3655999999999997, -5.6388999999999996, -5.5540000000000003, -5.4420999999999999, -5.6388999999999996, -5.4420999999999999, -5.6228999999999996, -5.6150000000000002, -5.6228999999999996, -5.5917000000000003, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -4.7488999999999999, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -5.9477000000000002, -6.3635999999999999, -5.4219999999999997, -5.7403000000000004, -6.8460000000000001, -6.3635999999999999, -5.1608000000000001, -6.9908000000000001, -6.4147999999999996, -5.4961000000000002, -7.1600999999999999, -7.1977000000000002, -7.2367999999999997, -7.2367999999999997, -5.4618000000000002, -6.0396000000000001, -7.3640999999999996, -6.2991999999999999, -7.4104000000000001, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -10.3643, -5.3963999999999999, -6.3308999999999997, -5.3592000000000004, -7.4588999999999999, -7.4588999999999999, -7.5099999999999998, -7.5636999999999999, -7.6205999999999996, -7.6807999999999996, -7.6807999999999996, -5.6067999999999998, -6.7195999999999998, -5.0701000000000001, -6.0396000000000001, -6.4325000000000001, -7.0551000000000004, -5.181, -6.6731999999999996, -5.8337000000000003, -6.1677999999999997, -3.6922000000000001, -4.6723999999999997, -3.9996, -5.2773000000000003, -4.8139000000000003, -5.6304999999999996, -4.3326000000000002, -5.5388999999999999, -4.5900999999999996, -4.7099000000000002, -4.7195, -5.7858999999999998, -5.4890999999999996, -5.5316999999999998, -4.7389999999999999, -5.0564999999999998, -5.2496, -5.3232999999999997, -5.2279999999999998, -5.3350999999999997, -4.9298000000000002, -4.9870000000000001, -5.4618000000000002, -5.3901000000000003, -5.4684999999999997, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -5.7050999999999998, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -4.8632999999999997, -5.0522999999999998, -6.3548999999999998, -6.4061000000000003, -6.4238, -3.9405000000000001, -5.7226999999999997, -6.6420000000000003, -6.7847999999999997, -7.0137, -7.0464000000000002, -6.4600999999999997, -7.1890000000000001, -7.1890000000000001, -7.3110999999999997, -7.4016999999999999, -4.2084999999999999, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -10.355600000000001, -6.6420000000000003, -6.7108999999999996, -6.7348999999999997, -7.4501999999999997, -6.4977999999999998, -7.5549999999999997, -7.5549999999999997, -7.5549999999999997, -7.6117999999999997, -7.6117999999999997, -7.6117999999999997, -7.6117999999999997, -4.3754, -6.0801999999999996, -3.5095999999999998, -6.8106999999999998, -6.0189000000000004, -7.0801999999999996, -5.7404999999999999, -6.4977999999999998, -6.4238, -6.6420000000000003, -6.5571000000000002, -4.9782000000000002, -6.0677000000000003, -6.3887999999999998, -5.9954000000000001, -5.7314999999999996, -5.9390000000000001, -6.0677000000000003, -3.2753999999999999, -4.6853999999999996, -4.3259999999999996, -5.1520999999999999, -5.6379000000000001, -5.3263999999999996, -4.8051000000000004, -5.9500000000000002, -5.2519, -5.4530000000000003, -5.4943999999999997, -5.1520999999999999, -5.8648999999999996, -5.1773999999999996, -5.5826000000000002, -5.6218000000000004, -5.7050999999999998, -5.6879, -5.8448000000000002, -5.6879, -5.8151999999999999, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -5.5957999999999997, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -3.8283999999999998, -5.7564000000000002, -5.9816000000000003, -6.1986999999999997, -6.2575000000000003, -6.2727000000000004, -6.2881999999999998, -6.2881999999999998, -6.2881999999999998, -6.3361000000000001, -6.0286, -5.6603000000000003, -6.4954999999999998, -6.5754000000000001, -6.5754000000000001, -5.3853999999999997, -6.9192999999999998, -6.9490999999999996, -7.0114000000000001, -7.2664, -7.3531000000000004, -7.3994, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -10.353300000000001, -5.7382, -5.4372999999999996, -7.4478999999999997, -7.4989999999999997, -7.6096000000000004, -7.6096000000000004, -6.3525999999999998, -6.6398000000000001, -5.8324999999999996, -6.1986999999999997, -6.9490999999999996, -5.5500999999999996, -5.9043000000000001, -4.8350999999999997, -5.2496, -6.1567999999999996, -5.4175000000000004, -4.8952999999999998, -5.5957999999999997, -5.0365000000000002, -5.2441000000000004, -6.0407000000000002, -4.4396000000000004, -4.8243, -5.4920999999999998, -4.9188000000000001, -5.2889999999999997, -4.5960999999999999, -4.3281000000000001, -5.9043000000000001, -4.7888000000000002, -5.6356000000000002, -5.6603000000000003, -5.3727999999999998, -5.2441000000000004, -4.8952999999999998, -5.2718999999999996, -5.4642999999999997, -5.2441000000000004, -5.2224000000000004, -5.444, -5.4241000000000001, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -5.8852000000000002, -6.1620999999999997, -6.431, -6.5667999999999997, -6.6311999999999998, -6.2191000000000001, -6.0693999999999999, -7.0355999999999996, -7.2579000000000002, -7.3003, -7.3003, -7.3445, -6.5877999999999997, -5.7663000000000002, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -10.344799999999999, -6.2046000000000001, -7.4394, -7.4904000000000002, -7.4904000000000002, -7.5442, -6.8818999999999999, -6.0820999999999996, -7.7253999999999996, -7.7938999999999998, -7.7938999999999998, -7.7938999999999998, -7.0029000000000003, -7.9469000000000003, -7.9469000000000003, -7.9469000000000003, -7.9469000000000003, -7.0693999999999999, -7.0693999999999999, -7.1043000000000003, -7.1406000000000001, -3.5909, -4.2252999999999998, -6.9108000000000001, -5.2633999999999999, -5.5795000000000003, -2.8835999999999999, -6.4869000000000003, -4.6684000000000001, -4.4212999999999996, -5.8746999999999998, -6.1482999999999999, -4.9181999999999997, -5.7477999999999998, -5.5194000000000001, -4.2682000000000002, -2.1957, -4.9549000000000003, -6.4869000000000003, -3.7528999999999999, -5.5121000000000002, -5.7948000000000004, -4.9024000000000001, -4.9142999999999999, -5.4024999999999999, -5.4836, -5.4558, -4.7561999999999998, -5.5266999999999999, -5.8643999999999998, -5.3581000000000003, -5.6856999999999998, -5.7477999999999998, -5.4905999999999997, -5.5717999999999996, -5.5872999999999999, -5.6353, -5.7477999999999998, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -4.7233999999999998, -5.3392999999999997, -4.8177000000000003, -6.2671000000000001, -6.4744000000000002, -6.8982000000000001, -6.9903000000000004, -7.0229999999999997, -7.1656000000000004, -7.3783000000000003, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -10.3322, -3.9443000000000001, -7.5316000000000001, -7.5884999999999998, -6.8693, -7.6486999999999998, -7.6486999999999998, -7.6486999999999998, -7.7813999999999997, -7.8548999999999998, -7.8548999999999998, -7.9343000000000004, -7.9343000000000004, -7.9343000000000004, -8.0206, -5.6146000000000003, -8.1150000000000002, -8.1150000000000002, -8.1150000000000002, -8.2193000000000005, -8.2193000000000005, -8.2193000000000005, -8.2193000000000005, -6.1220999999999997, -6.1920000000000002, -7.0568, -6.1776, -7.3783000000000003, -1.5797000000000001, -7.3783000000000003, -7.3783000000000003, -7.1280000000000001, -4.7201000000000004, -6.8693, -3.3542000000000001, -5.4161999999999999, -6.0696000000000003, -5.7352999999999996, -6.0696000000000003, -6.6639999999999997, -5.1337000000000002, -4.6970999999999998, -4.8437000000000001, -5.5364000000000004, -5.7632000000000003, -5.4640000000000004, -5.3579999999999997, -5.6817000000000002, -4.9465000000000003, -5.8517999999999999, -5.4363999999999999, -5.6227, -5.7171000000000003, -4.7676999999999996, -5.0065999999999997, -5.0991999999999997, -5.8314000000000004, -5.7538, -5.3270999999999997, -5.6308999999999996, -5.5289999999999999, -5.6730999999999998, -5.5904999999999996, -5.7352999999999996, -5.6905000000000001]}};\n",
         "\n",
         "function LDAvis_load_lib(url, callback){\n",
         "  var s = document.createElement('script');\n",
         "  s.src = url;\n",
         "  s.async = true;\n",
         "  s.onreadystatechange = s.onload = callback;\n",
         "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
         "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
         "}\n",
         "\n",
         "if(typeof(LDAvis) !== \"undefined\"){\n",
         "   // already loaded: just create the visualization\n",
         "   !function(LDAvis){\n",
         "       new LDAvis(\"#\" + \"ldavis_el109345929019688114613504\", ldavis_el109345929019688114613504_data);\n",
         "   }(LDAvis);\n",
         "}else if(typeof define === \"function\" && define.amd){\n",
         "   // require.js is available: use it to load d3/LDAvis\n",
         "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
         "   require([\"d3\"], function(d3){\n",
         "      window.d3 = d3;\n",
         "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
         "        new LDAvis(\"#\" + \"ldavis_el109345929019688114613504\", ldavis_el109345929019688114613504_data);\n",
         "      });\n",
         "    });\n",
         "}else{\n",
         "    // require.js not available: dynamically load d3 & LDAvis\n",
         "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
         "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
         "                 new LDAvis(\"#\" + \"ldavis_el109345929019688114613504\", ldavis_el109345929019688114613504_data);\n",
         "            })\n",
         "         });\n",
         "}\n",
         "</script>"
        ],
        "text/plain": [
         "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
         "topic                                                \n",
         "3      10.493265        1       1  0.094776 -0.064119\n",
         "5      10.235788        1       2  0.106752 -0.113524\n",
         "9      10.140768        1       3  0.073388  0.083996\n",
         "1      10.128972        1       4  0.031539  0.005389\n",
         "2      10.061925        1       5  0.049219 -0.070777\n",
         "0      10.028439        1       6  0.082672  0.110817\n",
         "6       9.990643        1       7 -0.008700 -0.228615\n",
         "4       9.976574        1       8  0.110068  0.210735\n",
         "8       9.583193        1       9 -0.256473 -0.010414\n",
         "7       9.360432        1      10 -0.283240  0.076512, topic_info=      Category          Freq        Term  Total  loglift  logprob\n",
         "7193   Default  11394.000000           \"  11394  30.0000  30.0000\n",
         "3650   Default   3382.000000      holmes   3382  29.0000  29.0000\n",
         "14816  Default   2046.000000          \"i   2046  28.0000  28.0000\n",
         "6944   Default   1381.000000        time   1381  27.0000  27.0000\n",
         "1230   Default   1505.000000        room   1505  26.0000  26.0000\n",
         "4093   Default   1264.000000        face   1264  25.0000  25.0000\n",
         "18371  Default   1376.000000      watson   1376  24.0000  24.0000\n",
         "5516   Default   1033.000000        hand   1033  23.0000  23.0000\n",
         "11116  Default    782.000000     morning    782  22.0000  22.0000\n",
         "5611   Default   1109.000000       night   1109  21.0000  21.0000\n",
         "10170  Default    892.000000        left    892  20.0000  20.0000\n",
         "16698  Default    734.000000         \"it    734  19.0000  19.0000\n",
         "2623   Default    725.000000     thought    725  18.0000  18.0000\n",
         "16002  Default   1018.000000        case   1018  17.0000  17.0000\n",
         "3634   Default    669.000000       great    669  16.0000  16.0000\n",
         "2706   Default    953.000000       found    953  15.0000  15.0000\n",
         "720    Default    651.000000        side    651  14.0000  14.0000\n",
         "3919   Default    565.000000    sherlock    565  13.0000  13.0000\n",
         "6297   Default    755.000000      friend    755  12.0000  12.0000\n",
         "9780   Default    807.000000      matter    807  11.0000  11.0000\n",
         "17851  Default    611.000000       light    611  10.0000  10.0000\n",
         "12029  Default   1255.000000       house   1255   9.0000   9.0000\n",
         "14762  Default    644.000000        \"you    644   8.0000   8.0000\n",
         "12617  Default   1199.000000        back   1199   7.0000   7.0000\n",
         "10939  Default    865.000000        eyes    865   6.0000   6.0000\n",
         "18451  Default    554.000000        half    554   5.0000   5.0000\n",
         "10368  Default    634.000000       \"well    634   4.0000   4.0000\n",
         "6049   Default    761.000000       heard    761   3.0000   3.0000\n",
         "3528   Default    671.000000       young    671   2.0000   2.0000\n",
         "1634   Default    498.000000        open    498   1.0000   1.0000\n",
         "...        ...           ...         ...    ...      ...      ...\n",
         "16370  Topic10    148.632071    remarked    206   1.9647  -5.4162\n",
         "16685  Topic10     76.677116         \"in     98   2.0543  -6.0696\n",
         "18221  Topic10    107.354531     married    144   2.0037  -5.7353\n",
         "3936   Topic10     76.653542     godfrey     99   2.0441  -6.0696\n",
         "18140  Topic10     42.045542       class     50   2.1327  -6.6640\n",
         "10184  Topic10    195.877042       facts    297   1.8813  -5.1337\n",
         "18718  Topic10    303.450529        \"the    501   1.7951  -4.6971\n",
         "12386  Topic10    261.484542       point    427   1.8083  -4.8437\n",
         "17874  Topic10    130.861744     suppose    191   1.9201  -5.5364\n",
         "3067   Topic10    104.166431        bell    146   1.9620  -5.7632\n",
         "11191  Topic10    140.735398     present    213   1.8835  -5.4640\n",
         "8374   Topic10    156.843912       voice    247   1.8414  -5.3580\n",
         "18369  Topic10    113.095813    happened    168   1.9031  -5.6817\n",
         "15857  Topic10    235.380033       paper    434   1.6892  -4.9465\n",
         "1446   Topic10     95.368510       times    142   1.9011  -5.8518\n",
         "10582  Topic10    145.516930         box    248   1.7589  -5.4364\n",
         "16269  Topic10    119.814076     england    196   1.8080  -5.6227\n",
         "8966   Topic10    108.919312       began    180   1.7987  -5.7171\n",
         "12261  Topic10    281.883074        find    721   1.3604  -4.7677\n",
         "9835   Topic10    223.741582         don    521   1.4465  -5.0066\n",
         "1736   Topic10    202.107752        knew    555   1.2906  -5.0992\n",
         "15082  Topic10     97.132045        pipe    155   1.8339  -5.8314\n",
         "8858   Topic10    104.605745  importance    184   1.7401  -5.7538\n",
         "4398   Topic10    159.723013       years    462   1.2461  -5.3271\n",
         "10456  Topic10    119.893280        text    265   1.4981  -5.6309\n",
         "3600   Topic10    130.388479    business    413   1.1564  -5.5290\n",
         "14745  Topic10    113.280212        \"and    265   1.4560  -5.6731\n",
         "5516   Topic10    121.696221        hand   1033   0.1781  -5.5905\n",
         "5027   Topic10    106.773734       train    229   1.5398  -5.7353\n",
         "7518   Topic10    110.458764       clear    511   0.7819  -5.6905\n",
         "\n",
         "[1627 rows x 6 columns], token_table=       Topic      Freq          Term\n",
         "term                                \n",
         "7193       1  0.005178             \"\n",
         "7193       4  0.044936             \"\n",
         "7193       5  0.006846             \"\n",
         "7193       6  0.000088             \"\n",
         "7193       9  0.335791             \"\n",
         "7193      10  0.607162             \"\n",
         "12555      4  1.000000         \"1884\n",
         "14812      9  0.012195            \"a\n",
         "14812     10  0.981707            \"a\n",
         "12462      3  1.000000   \"absolutely\n",
         "18087     10  1.000000     \"adelbert\n",
         "14033      3  0.811966           \"ah\n",
         "14033      6  0.068376           \"ah\n",
         "14033      7  0.059829           \"ah\n",
         "14033      8  0.051282           \"ah\n",
         "14745      1  0.075472          \"and\n",
         "14745      6  0.483019          \"and\n",
         "14745      9  0.011321          \"and\n",
         "14745     10  0.426415          \"and\n",
         "14741     10  1.000000          \"any\n",
         "12488      5  1.000000        \"apart\n",
         "14018      4  0.024390           \"as\n",
         "14018      8  0.967480           \"as\n",
         "5341       8  1.000000  \"bartholomew\n",
         "1216       2  1.000000         \"boys\n",
         "15320      3  1.000000      \"brixton\n",
         "18493      1  0.200803          \"but\n",
         "18493      5  0.036145          \"but\n",
         "18493      6  0.068273          \"but\n",
         "18493      7  0.522088          \"but\n",
         "...      ...       ...           ...\n",
         "1608       3  0.007576         wrong\n",
         "1608       5  0.886364         wrong\n",
         "1608       9  0.007576         wrong\n",
         "5367       6  1.000000            wu\n",
         "10753      3  1.000000        yapped\n",
         "6402       2  0.006536          yard\n",
         "6402       4  0.156863          yard\n",
         "6402       8  0.830065          yard\n",
         "7682       3  0.010526         yards\n",
         "7682       4  0.978947         yards\n",
         "8601      10  1.000000         yawls\n",
         "6199       1  0.035398          year\n",
         "6199       3  0.721239          year\n",
         "6199       4  0.030973          year\n",
         "6199       7  0.013274          year\n",
         "6199       8  0.004425          year\n",
         "6199       9  0.190265          year\n",
         "4398       1  0.106061         years\n",
         "4398       5  0.225108         years\n",
         "4398       6  0.322511         years\n",
         "4398      10  0.346320         years\n",
         "2308       3  1.000000       yeggman\n",
         "12222      4  1.000000          yews\n",
         "8942       8  1.000000         yoked\n",
         "265        6  1.000000       youghal\n",
         "3528       4  0.026826         young\n",
         "3528       6  0.526080         young\n",
         "3528       8  0.429210         young\n",
         "3528       9  0.017884         young\n",
         "9744       8  1.000000          yuan\n",
         "\n",
         "[2760 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[4, 6, 10, 2, 3, 1, 7, 5, 9, 8])"
        ]
       },
       "execution_count": 31,
       "metadata": {},
       "output_type": "execute_result"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "collapsed": true
     },
     "source": [
      "From the above visualization, we can observe that the algorithm returned pretty interesting results. For example, one identified topic is related to Watson, locations (room, street, house, etc.), and time (days, hours, etc.). While, another topic is related to Holmes, men, and murder. For me these are pretty interesting results. I recommend the reader to try investigate the results by themselves. Moreover, I think that running the topic model algorithm on other text corpus can help to better understand this algorithms advantages."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <a id=\"word2vec\"></a> 5. Finding SImilar Paragraphs using Word2Vec "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "\"I have notes of several similar cases, though none, as I remarked before, which were quite as prompt. My whole examination served to \n",
      "turn my conjecture into a certainty. Circumstantial evidence is occasionally very convincing, as when you find a trout in the milk, to \n",
      "quote Thoreau's example.\"\n",
      "                                                                                                 -The Adventure of the Noble Bachelor\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These days, no NLP related post can be complete without including the words \"deep learning.\" Therefore, in this section I will demonstrate how to use Word2Vec deep learning inspired algorithm to search for paragraphs that have similar text or writing style. \n",
      "\n",
      "First, let's build a Word2Vec model using Sherlock's stories. We will construct the Word2Vec model using the Gensim package and a similar method to the one presented in [Word2vec Tutorial](http://rare-technologies.com/word2vec-tutorial/) and in my previous [post](https://dato.com/learn/gallery/notebooks/deep_text_learning.html)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import graphlab as gl\n",
      "import urllib2\n",
      "import gensim\n",
      "import nltk\n",
      "import re\n",
      "\n",
      "txt = urllib2.urlopen(\"https://sherlock-holm.es/stories/plain-text/cnus.txt\").read()\n",
      "\n",
      "re_words_split = re.compile(\"(\\w+)\")\n",
      "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
      "def txt2words(s):\n",
      "    s = re.sub(\"[^a-zA-Z]\", \" \", s).lower()\n",
      "    return re_words_split.findall(s)\n",
      "\n",
      "class MySentences(object):\n",
      "        def __init__(self, txt):\n",
      "            self._txt = txt.decode(\"utf8\") \n",
      "            \n",
      "        def __iter__(self):\n",
      "            \"\"\"\n",
      "            Split the English text into sentences and then to words using NLTK\n",
      "            :param txt: input text.    \n",
      "            :param remove_none_english_chars: if True then remove none English chars from text\n",
      "            :return: list of words in which each list consists of single sentence's words from the original input text.\n",
      "            :rtype: str\n",
      "            \"\"\"                \n",
      "            # split text into sentences using NLTK package\n",
      "            for s in tokenizer.tokenize(self._txt):                                    \n",
      "                yield txt2words(s)\n",
      "\n",
      "sentences = MySentences(txt)\n",
      "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=3, workers=4)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/pablo/anaconda/envs/dato-env/lib/python2.7/site-packages/numpy/lib/utils.py:99: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
        "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
        "  warnings.warn(depdoc, DeprecationWarning)\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We now have a trained Word2Vec model, let's see if it gives reasonable results:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print model.most_similar(\"watson\")\n",
      "print model.most_similar(\"holmes\")"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'mortimer', 0.7463816404342651), (u'dear', 0.6983240842819214), (u'colleague', 0.6832994222640991), (u'oh', 0.6541128158569336), (u'tut', 0.6502598524093628), (u'trevelyan', 0.6456252336502075), (u'ah', 0.6448663473129272), (u'jack', 0.6376811861991882), (u'god', 0.6342098712921143), (u'sake', 0.6111361980438232)]\n",
        "[(u'lestrade', 0.7618856430053711), (u'mac', 0.7348191738128662), (u'mcmurdo', 0.7301948070526123), (u'phelps', 0.7230634689331055), (u'melas', 0.7184404134750366), (u'baynes', 0.7174237966537476), (u'douglas', 0.7089315056800842), (u'cornelius', 0.6709353923797607), (u'sternly', 0.6695888042449951), (u'barker', 0.6632921695709229)]\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We got that the most similar word to Watson is Mortimer and the most similar word to Holmes is Lestrade. These results sound logical enough. Let us calculate the average vector of each paragraph."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import graphlab as gl\n",
      "import re\n",
      "import numpy as np\n",
      "\n",
      "BASE_DIR = r\"/Users/pablo/fromHomeMac/sherlock/data\" # NOTE: Update BASE_DIR to your own directory path\n",
      "sf =  gl.load_sframe(\"%s/books.sframe\" % BASE_DIR)\n",
      "sf_paragraphs = sf.flat_map(['title', 'text'], lambda t: [[t['title'],p.strip()] for p in t['text'].split(\"\\n\\n\")])\n",
      "sf_paragraphs = sf_paragraphs.rename({'text': 'paragraph'})\n",
      "sf_paragraphs['paragraph_words_number'] = sf_paragraphs['paragraph'].apply(lambda p: len(re_words_split.findall(p)) )\n",
      "sf_paragraphs = sf_paragraphs[sf_paragraphs['paragraph_words_number'] >=25]\n",
      "\n",
      "def txt2avg_vector(txt, w2v_model):\n",
      "    words = [w for w in txt2words(txt.lower()) if w in w2v_model]\n",
      "    v = np.mean([w2v_model[w] for w in words],axis=0)    \n",
      "    return v\n",
      "\n",
      "sf_paragraphs['mean_vector'] = sf_paragraphs['paragraph'].apply(lambda p: txt2avg_vector(p, model))"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have the mean vector value of each paragraph. Let's utilize [GraphLab Create nearest neighbors toolkit](https://dato.com/products/create/docs/graphlab.toolkits.nearest_neighbors.html) to identify paragraphs that have similar text or  writing style. We will acheive that by calaculating the nearest neighbor to each the mean vector of each paragraph. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#construncting nearest neighbors model\n",
      "nn_model = gl.nearest_neighbors.create(sf_paragraphs, features=['mean_vector'])\n",
      "\n",
      "#calaculating the two nearest neighbors of each paragraph from all the paragraphs \n",
      "r = nn_model.query(sf_paragraphs, k=2)\n",
      "r.head(10)"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false,
      "scrolled": true
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "PROGRESS: Starting ball tree nearest neighbors model training.\n",
        "PROGRESS: +------------+--------------+\n",
        "PROGRESS: | Tree level | Elapsed Time |\n",
        "PROGRESS: +------------+--------------+\n",
        "PROGRESS: | 0          | 166.646ms    |\n",
        "PROGRESS: | 1          | 317.073ms    |\n",
        "PROGRESS: | 2          | 470.327ms    |\n",
        "PROGRESS: | 3          | 621.256ms    |\n",
        "PROGRESS: | 4          | 694.293ms    |\n",
        "PROGRESS: +------------+--------------+\n",
        "PROGRESS: +--------------+-------------+--------------+\n",
        "PROGRESS: | Query points | % Complete. | Elapsed Time |\n",
        "PROGRESS: +--------------+-------------+--------------+\n",
        "PROGRESS: | 1            | 0           | 81.39ms      |\n",
        "PROGRESS: | 28           | 0           | 1.10s        |\n",
        "PROGRESS: | 51           | 0.25        | 2.09s        |\n",
        "PROGRESS: | 76           | 0.5         | 3.09s        |\n",
        "PROGRESS: | 103          | 0.75        | 4.09s        |\n",
        "PROGRESS: | 130          | 1           | 5.10s        |\n",
        "PROGRESS: | 158          | 1.25        | 6.15s        |\n",
        "PROGRESS: | 186          | 1.5         | 7.11s        |\n",
        "PROGRESS: | 214          | 1.75        | 8.11s        |\n",
        "PROGRESS: | 244          | 2           | 9.10s        |\n",
        "PROGRESS: | 273          | 2.25        | 10.22s       |\n",
        "PROGRESS: | 297          | 2.5         | 11.14s       |\n",
        "PROGRESS: | 325          | 2.75        | 12.10s       |\n",
        "PROGRESS: | 346          | 2.75        | 13.11s       |\n",
        "PROGRESS: | 372          | 3           | 14.12s       |\n",
        "PROGRESS: | 401          | 3.25        | 15.15s       |\n",
        "PROGRESS: | 432          | 3.5         | 16.12s       |\n",
        "PROGRESS: | 462          | 3.75        | 17.13s       |\n",
        "PROGRESS: | 490          | 4           | 18.13s       |\n",
        "PROGRESS: | 520          | 4.25        | 19.16s       |\n",
        "PROGRESS: | 548          | 4.5         | 20.21s       |\n",
        "PROGRESS: | 568          | 4.75        | 21.23s       |\n",
        "PROGRESS: | 586          | 5           | 22.13s       |\n",
        "PROGRESS: | 610          | 5           | 23.13s       |\n",
        "PROGRESS: | 635          | 5.25        | 24.14s       |\n",
        "PROGRESS: | 663          | 5.5         | 25.21s       |\n",
        "PROGRESS: | 690          | 5.75        | 26.15s       |\n",
        "PROGRESS: | 717          | 6           | 27.16s       |\n",
        "PROGRESS: | 740          | 6.25        | 28.15s       |\n",
        "PROGRESS: | 773          | 6.5         | 29.14s       |\n",
        "PROGRESS: | 799          | 6.75        | 30.16s       |\n",
        "PROGRESS: | 825          | 7           | 31.16s       |\n",
        "PROGRESS: | 851          | 7.25        | 32.26s       |\n",
        "PROGRESS: | 871          | 7.25        | 33.16s       |\n",
        "PROGRESS: | 898          | 7.5         | 34.18s       |\n",
        "PROGRESS: | 923          | 7.75        | 35.15s       |\n",
        "PROGRESS: | 955          | 8           | 36.18s       |\n",
        "PROGRESS: | 981          | 8.25        | 37.18s       |\n",
        "PROGRESS: | 1012         | 8.5         | 38.16s       |\n",
        "PROGRESS: | 1040         | 8.75        | 39.21s       |\n",
        "PROGRESS: | 1070         | 9           | 40.20s       |\n",
        "PROGRESS: | 1099         | 9.25        | 41.27s       |\n",
        "PROGRESS: | 1125         | 9.5         | 42.25s       |\n",
        "PROGRESS: | 1150         | 9.75        | 43.21s       |\n",
        "PROGRESS: | 1176         | 10          | 44.20s       |\n",
        "PROGRESS: | 1202         | 10.25       | 45.22s       |\n",
        "PROGRESS: | 1229         | 10.5        | 46.21s       |\n",
        "PROGRESS: | 1258         | 10.75       | 47.19s       |\n",
        "PROGRESS: | 1287         | 11          | 48.26s       |\n",
        "PROGRESS: | 1311         | 11          | 49.21s       |\n",
        "PROGRESS: | 1338         | 11.25       | 50.23s       |\n",
        "PROGRESS: | 1360         | 11.5        | 51.21s       |\n",
        "PROGRESS: | 1387         | 11.75       | 52.23s       |\n",
        "PROGRESS: | 1414         | 12          | 53.22s       |\n",
        "PROGRESS: | 1446         | 12.25       | 54.21s       |\n",
        "PROGRESS: | 1474         | 12.5        | 55.21s       |\n",
        "PROGRESS: | 1505         | 12.75       | 56.25s       |\n",
        "PROGRESS: | 1538         | 13          | 57.26s       |\n",
        "PROGRESS: | 1564         | 13.25       | 58.22s       |\n",
        "PROGRESS: | 1596         | 13.5        | 59.23s       |\n",
        "PROGRESS: | 1625         | 13.75       | 1m 0s        |\n",
        "PROGRESS: | 1654         | 14          | 1m 1s        |\n",
        "PROGRESS: | 1682         | 14.25       | 1m 2s        |\n",
        "PROGRESS: | 1713         | 14.5        | 1m 3s        |\n",
        "PROGRESS: | 1745         | 14.75       | 1m 4s        |\n",
        "PROGRESS: | 1775         | 15          | 1m 5s        |\n",
        "PROGRESS: | 1802         | 15.25       | 1m 6s        |\n",
        "PROGRESS: | 1832         | 15.5        | 1m 7s        |\n",
        "PROGRESS: | 1858         | 15.75       | 1m 8s        |\n",
        "PROGRESS: | 1883         | 16          | 1m 9s        |\n",
        "PROGRESS: | 1909         | 16.25       | 1m 10s       |\n",
        "PROGRESS: | 1940         | 16.5        | 1m 11s       |\n",
        "PROGRESS: | 1974         | 16.75       | 1m 12s       |\n",
        "PROGRESS: | 2006         | 17          | 1m 13s       |\n",
        "PROGRESS: | 2034         | 17.25       | 1m 14s       |\n",
        "PROGRESS: | 2059         | 17.5        | 1m 15s       |\n",
        "PROGRESS: | 2091         | 17.75       | 1m 16s       |\n",
        "PROGRESS: | 2114         | 18          | 1m 17s       |\n",
        "PROGRESS: | 2143         | 18.25       | 1m 18s       |\n",
        "PROGRESS: | 2172         | 18.5        | 1m 19s       |\n",
        "PROGRESS: | 2196         | 18.75       | 1m 20s       |\n",
        "PROGRESS: | 2220         | 19          | 1m 21s       |\n",
        "PROGRESS: | 2243         | 19          | 1m 22s       |\n",
        "PROGRESS: | 2269         | 19.25       | 1m 23s       |\n",
        "PROGRESS: | 2297         | 19.5        | 1m 24s       |\n",
        "PROGRESS: | 2321         | 19.75       | 1m 25s       |\n",
        "PROGRESS: | 2349         | 20          | 1m 26s       |\n",
        "PROGRESS: | 2378         | 20.25       | 1m 27s       |\n",
        "PROGRESS: | 2402         | 20.5        | 1m 28s       |\n",
        "PROGRESS: | 2426         | 20.75       | 1m 29s       |\n",
        "PROGRESS: | 2457         | 21          | 1m 30s       |\n",
        "PROGRESS: | 2484         | 21.25       | 1m 31s       |\n",
        "PROGRESS: | 2509         | 21.5        | 1m 32s       |\n",
        "PROGRESS: | 2537         | 21.75       | 1m 33s       |\n",
        "PROGRESS: | 2562         | 21.75       | 1m 34s       |\n",
        "PROGRESS: | 2596         | 22.25       | 1m 35s       |\n",
        "PROGRESS: | 2623         | 22.5        | 1m 36s       |\n",
        "PROGRESS: | 2649         | 22.5        | 1m 37s       |\n",
        "PROGRESS: | 2675         | 22.75       | 1m 38s       |\n",
        "PROGRESS: | 2705         | 23          | 1m 39s       |\n",
        "PROGRESS: | 2740         | 23.5        | 1m 40s       |\n",
        "PROGRESS: | 2771         | 23.75       | 1m 41s       |\n",
        "PROGRESS: | 2800         | 24          | 1m 42s       |\n",
        "PROGRESS: | 2836         | 24.25       | 1m 43s       |\n",
        "PROGRESS: | 2867         | 24.5        | 1m 44s       |\n",
        "PROGRESS: | 2893         | 24.75       | 1m 45s       |\n",
        "PROGRESS: | 2928         | 25          | 1m 46s       |\n",
        "PROGRESS: | 2952         | 25.25       | 1m 47s       |\n",
        "PROGRESS: | 2986         | 25.5        | 1m 48s       |\n",
        "PROGRESS: | 3017         | 25.75       | 1m 49s       |\n",
        "PROGRESS: | 3039         | 26          | 1m 50s       |\n",
        "PROGRESS: | 3068         | 26.25       | 1m 51s       |\n",
        "PROGRESS: | 3096         | 26.5        | 1m 52s       |\n",
        "PROGRESS: | 3123         | 26.75       | 1m 53s       |\n",
        "PROGRESS: | 3153         | 27          | 1m 54s       |\n",
        "PROGRESS: | 3180         | 27.25       | 1m 55s       |\n",
        "PROGRESS: | 3202         | 27.25       | 1m 56s       |\n",
        "PROGRESS: | 3231         | 27.5        | 1m 57s       |\n",
        "PROGRESS: | 3263         | 27.75       | 1m 58s       |\n",
        "PROGRESS: | 3292         | 28          | 1m 59s       |\n",
        "PROGRESS: | 3321         | 28.25       | 2m 0s        |\n",
        "PROGRESS: | 3350         | 28.5        | 2m 1s        |\n",
        "PROGRESS: | 3380         | 28.75       | 2m 2s        |\n",
        "PROGRESS: | 3411         | 29.25       | 2m 3s        |\n",
        "PROGRESS: | 3438         | 29.25       | 2m 4s        |\n",
        "PROGRESS: | 3467         | 29.5        | 2m 5s        |\n",
        "PROGRESS: | 3501         | 30          | 2m 6s        |\n",
        "PROGRESS: | 3528         | 30.25       | 2m 7s        |\n",
        "PROGRESS: | 3555         | 30.25       | 2m 8s        |\n",
        "PROGRESS: | 3585         | 30.75       | 2m 9s        |\n",
        "PROGRESS: | 3612         | 30.75       | 2m 10s       |\n",
        "PROGRESS: | 3635         | 31          | 2m 11s       |\n",
        "PROGRESS: | 3665         | 31.25       | 2m 12s       |\n",
        "PROGRESS: | 3698         | 31.5        | 2m 13s       |\n",
        "PROGRESS: | 3733         | 32          | 2m 14s       |\n",
        "PROGRESS: | 3763         | 32.25       | 2m 15s       |\n",
        "PROGRESS: | 3794         | 32.5        | 2m 16s       |\n",
        "PROGRESS: | 3822         | 32.75       | 2m 17s       |\n",
        "PROGRESS: | 3850         | 33          | 2m 18s       |\n",
        "PROGRESS: | 3885         | 33.25       | 2m 19s       |\n",
        "PROGRESS: | 3917         | 33.5        | 2m 20s       |\n",
        "PROGRESS: | 3945         | 33.75       | 2m 21s       |\n",
        "PROGRESS: | 3976         | 34          | 2m 22s       |\n",
        "PROGRESS: | 4005         | 34.25       | 2m 23s       |\n",
        "PROGRESS: | 4033         | 34.5        | 2m 24s       |\n",
        "PROGRESS: | 4064         | 34.75       | 2m 25s       |\n",
        "PROGRESS: | 4094         | 35          | 2m 26s       |\n",
        "PROGRESS: | 4122         | 35.25       | 2m 27s       |\n",
        "PROGRESS: | 4149         | 35.5        | 2m 28s       |\n",
        "PROGRESS: | 4181         | 35.75       | 2m 29s       |\n",
        "PROGRESS: | 4216         | 36          | 2m 30s       |\n",
        "PROGRESS: | 4244         | 36.25       | 2m 31s       |\n",
        "PROGRESS: | 4270         | 36.5        | 2m 32s       |\n",
        "PROGRESS: | 4298         | 36.75       | 2m 33s       |\n",
        "PROGRESS: | 4326         | 37          | 2m 34s       |\n",
        "PROGRESS: | 4357         | 37.25       | 2m 35s       |\n",
        "PROGRESS: | 4390         | 37.5        | 2m 36s       |\n",
        "PROGRESS: | 4414         | 37.75       | 2m 37s       |\n",
        "PROGRESS: | 4449         | 38          | 2m 38s       |\n",
        "PROGRESS: | 4476         | 38.25       | 2m 39s       |\n",
        "PROGRESS: | 4504         | 38.5        | 2m 40s       |\n",
        "PROGRESS: | 4533         | 38.75       | 2m 41s       |\n",
        "PROGRESS: | 4558         | 39          | 2m 42s       |\n",
        "PROGRESS: | 4590         | 39.25       | 2m 43s       |\n",
        "PROGRESS: | 4625         | 39.5        | 2m 44s       |\n",
        "PROGRESS: | 4654         | 39.75       | 2m 45s       |\n",
        "PROGRESS: | 4681         | 40          | 2m 46s       |\n",
        "PROGRESS: | 4707         | 40.25       | 2m 47s       |\n",
        "PROGRESS: | 4736         | 40.5        | 2m 48s       |\n",
        "PROGRESS: | 4761         | 40.75       | 2m 49s       |\n",
        "PROGRESS: | 4792         | 41          | 2m 50s       |\n",
        "PROGRESS: | 4821         | 41.25       | 2m 51s       |\n",
        "PROGRESS: | 4851         | 41.5        | 2m 52s       |\n",
        "PROGRESS: | 4883         | 41.75       | 2m 53s       |\n",
        "PROGRESS: | 4911         | 42          | 2m 54s       |\n",
        "PROGRESS: | 4939         | 42.25       | 2m 55s       |\n",
        "PROGRESS: | 4964         | 42.5        | 2m 56s       |\n",
        "PROGRESS: | 4997         | 42.75       | 2m 57s       |\n",
        "PROGRESS: | 5027         | 43          | 2m 58s       |\n",
        "PROGRESS: | 5059         | 43.25       | 2m 59s       |\n",
        "PROGRESS: | 5084         | 43.5        | 3m 0s        |\n",
        "PROGRESS: | 5109         | 43.75       | 3m 1s        |\n",
        "PROGRESS: | 5131         | 44          | 3m 2s        |\n",
        "PROGRESS: | 5160         | 44.25       | 3m 3s        |\n",
        "PROGRESS: | 5186         | 44.25       | 3m 4s        |\n",
        "PROGRESS: | 5214         | 44.5        | 3m 5s        |\n",
        "PROGRESS: | 5246         | 45          | 3m 6s        |\n",
        "PROGRESS: | 5270         | 45          | 3m 7s        |\n",
        "PROGRESS: | 5299         | 45.25       | 3m 8s        |\n",
        "PROGRESS: | 5328         | 45.5        | 3m 9s        |\n",
        "PROGRESS: | 5358         | 45.75       | 3m 10s       |\n",
        "PROGRESS: | 5386         | 46          | 3m 11s       |\n",
        "PROGRESS: | 5413         | 46.25       | 3m 12s       |\n",
        "PROGRESS: | 5442         | 46.5        | 3m 13s       |\n",
        "PROGRESS: | 5469         | 46.75       | 3m 14s       |\n",
        "PROGRESS: | 5502         | 47          | 3m 15s       |\n",
        "PROGRESS: | 5528         | 47.25       | 3m 16s       |\n",
        "PROGRESS: | 5560         | 47.5        | 3m 17s       |\n",
        "PROGRESS: | 5584         | 47.75       | 3m 18s       |\n",
        "PROGRESS: | 5615         | 48          | 3m 19s       |\n",
        "PROGRESS: | 5651         | 48.25       | 3m 20s       |\n",
        "PROGRESS: | 5676         | 48.5        | 3m 21s       |\n",
        "PROGRESS: | 5709         | 48.75       | 3m 22s       |\n",
        "PROGRESS: | 5735         | 49          | 3m 23s       |\n",
        "PROGRESS: | 5761         | 49.25       | 3m 24s       |\n",
        "PROGRESS: | 5793         | 49.5        | 3m 25s       |\n",
        "PROGRESS: | 5820         | 49.75       | 3m 26s       |\n",
        "PROGRESS: | 5850         | 50          | 3m 27s       |\n",
        "PROGRESS: | 5877         | 50.25       | 3m 28s       |\n",
        "PROGRESS: | 5905         | 50.5        | 3m 29s       |\n",
        "PROGRESS: | 5936         | 50.75       | 3m 30s       |\n",
        "PROGRESS: | 5961         | 51          | 3m 31s       |\n",
        "PROGRESS: | 5990         | 51.25       | 3m 32s       |\n",
        "PROGRESS: | 6025         | 51.5        | 3m 33s       |\n",
        "PROGRESS: | 6052         | 51.75       | 3m 34s       |\n",
        "PROGRESS: | 6071         | 52          | 3m 35s       |\n",
        "PROGRESS: | 6097         | 52.25       | 3m 36s       |\n",
        "PROGRESS: | 6129         | 52.5        | 3m 37s       |\n",
        "PROGRESS: | 6162         | 52.75       | 3m 38s       |\n",
        "PROGRESS: | 6193         | 53          | 3m 39s       |\n",
        "PROGRESS: | 6223         | 53.25       | 3m 40s       |\n",
        "PROGRESS: | 6252         | 53.5        | 3m 41s       |\n",
        "PROGRESS: | 6284         | 53.75       | 3m 42s       |\n",
        "PROGRESS: | 6314         | 54          | 3m 43s       |\n",
        "PROGRESS: | 6349         | 54.25       | 3m 44s       |\n",
        "PROGRESS: | 6378         | 54.5        | 3m 45s       |\n",
        "PROGRESS: | 6406         | 54.75       | 3m 46s       |\n",
        "PROGRESS: | 6438         | 55          | 3m 47s       |\n",
        "PROGRESS: | 6459         | 55.25       | 3m 48s       |\n",
        "PROGRESS: | 6486         | 55.5        | 3m 49s       |\n",
        "PROGRESS: | 6517         | 55.75       | 3m 50s       |\n",
        "PROGRESS: | 6548         | 56          | 3m 51s       |\n",
        "PROGRESS: | 6577         | 56.25       | 3m 52s       |\n",
        "PROGRESS: | 6614         | 56.5        | 3m 53s       |\n",
        "PROGRESS: | 6642         | 56.75       | 3m 54s       |\n",
        "PROGRESS: | 6669         | 57          | 3m 55s       |\n",
        "PROGRESS: | 6695         | 57.25       | 3m 56s       |\n",
        "PROGRESS: | 6725         | 57.5        | 3m 57s       |\n",
        "PROGRESS: | 6749         | 57.75       | 3m 58s       |\n",
        "PROGRESS: | 6779         | 58          | 3m 59s       |\n",
        "PROGRESS: | 6808         | 58.25       | 4m 0s        |\n",
        "PROGRESS: | 6841         | 58.5        | 4m 1s        |\n",
        "PROGRESS: | 6867         | 58.75       | 4m 2s        |\n",
        "PROGRESS: | 6896         | 59          | 4m 3s        |\n",
        "PROGRESS: | 6924         | 59.25       | 4m 4s        |\n",
        "PROGRESS: | 6954         | 59.5        | 4m 5s        |\n",
        "PROGRESS: | 6977         | 59.75       | 4m 6s        |\n",
        "PROGRESS: | 7007         | 60          | 4m 7s        |\n",
        "PROGRESS: | 7035         | 60.25       | 4m 8s        |\n",
        "PROGRESS: | 7061         | 60.5        | 4m 9s        |\n",
        "PROGRESS: | 7085         | 60.75       | 4m 10s       |\n",
        "PROGRESS: | 7114         | 61          | 4m 11s       |\n",
        "PROGRESS: | 7135         | 61          | 4m 12s       |\n",
        "PROGRESS: | 7155         | 61.25       | 4m 13s       |\n",
        "PROGRESS: | 7179         | 61.5        | 4m 14s       |\n",
        "PROGRESS: | 7203         | 61.75       | 4m 15s       |\n",
        "PROGRESS: | 7224         | 61.75       | 4m 16s       |\n",
        "PROGRESS: | 7251         | 62          | 4m 17s       |\n",
        "PROGRESS: | 7273         | 62.25       | 4m 18s       |\n",
        "PROGRESS: | 7299         | 62.5        | 4m 19s       |\n",
        "PROGRESS: | 7321         | 62.75       | 4m 20s       |\n",
        "PROGRESS: | 7346         | 63          | 4m 21s       |\n",
        "PROGRESS: | 7368         | 63          | 4m 22s       |\n",
        "PROGRESS: | 7391         | 63.25       | 4m 23s       |\n",
        "PROGRESS: | 7415         | 63.5        | 4m 24s       |\n",
        "PROGRESS: | 7442         | 63.75       | 4m 25s       |\n",
        "PROGRESS: | 7471         | 64          | 4m 26s       |\n",
        "PROGRESS: | 7497         | 64.25       | 4m 27s       |\n",
        "PROGRESS: | 7524         | 64.5        | 4m 28s       |\n",
        "PROGRESS: | 7548         | 64.75       | 4m 29s       |\n",
        "PROGRESS: | 7579         | 65          | 4m 30s       |\n",
        "PROGRESS: | 7611         | 65.25       | 4m 31s       |\n",
        "PROGRESS: | 7638         | 65.5        | 4m 32s       |\n",
        "PROGRESS: | 7668         | 65.75       | 4m 33s       |\n",
        "PROGRESS: | 7698         | 66          | 4m 34s       |\n",
        "PROGRESS: | 7724         | 66.25       | 4m 35s       |\n",
        "PROGRESS: | 7745         | 66.25       | 4m 36s       |\n",
        "PROGRESS: | 7769         | 66.5        | 4m 37s       |\n",
        "PROGRESS: | 7797         | 66.75       | 4m 38s       |\n",
        "PROGRESS: | 7828         | 67          | 4m 39s       |\n",
        "PROGRESS: | 7853         | 67.25       | 4m 40s       |\n",
        "PROGRESS: | 7878         | 67.5        | 4m 41s       |\n",
        "PROGRESS: | 7905         | 67.75       | 4m 42s       |\n",
        "PROGRESS: | 7931         | 68          | 4m 43s       |\n",
        "PROGRESS: | 7955         | 68          | 4m 44s       |\n",
        "PROGRESS: | 7977         | 68.25       | 4m 45s       |\n",
        "PROGRESS: | 8006         | 68.5        | 4m 46s       |\n",
        "PROGRESS: | 8028         | 68.75       | 4m 47s       |\n",
        "PROGRESS: | 8050         | 69          | 4m 48s       |\n",
        "PROGRESS: | 8075         | 69.25       | 4m 49s       |\n",
        "PROGRESS: | 8097         | 69.25       | 4m 50s       |\n",
        "PROGRESS: | 8123         | 69.5        | 4m 51s       |\n",
        "PROGRESS: | 8149         | 69.75       | 4m 52s       |\n",
        "PROGRESS: | 8172         | 70          | 4m 53s       |\n",
        "PROGRESS: | 8201         | 70.25       | 4m 54s       |\n",
        "PROGRESS: | 8231         | 70.5        | 4m 55s       |\n",
        "PROGRESS: | 8256         | 70.75       | 4m 56s       |\n",
        "PROGRESS: | 8282         | 71          | 4m 57s       |\n",
        "PROGRESS: | 8307         | 71.25       | 4m 58s       |\n",
        "PROGRESS: | 8337         | 71.5        | 4m 59s       |\n",
        "PROGRESS: | 8363         | 71.5        | 5m 0s        |\n",
        "PROGRESS: | 8386         | 71.75       | 5m 1s        |\n",
        "PROGRESS: | 8403         | 72          | 5m 2s        |\n",
        "PROGRESS: | 8424         | 72.25       | 5m 3s        |\n",
        "PROGRESS: | 8445         | 72.25       | 5m 4s        |\n",
        "PROGRESS: | 8467         | 72.5        | 5m 5s        |\n",
        "PROGRESS: | 8483         | 72.75       | 5m 6s        |\n",
        "PROGRESS: | 8503         | 72.75       | 5m 7s        |\n",
        "PROGRESS: | 8522         | 73          | 5m 8s        |\n",
        "PROGRESS: | 8538         | 73          | 5m 9s        |\n",
        "PROGRESS: | 8557         | 73.25       | 5m 10s       |\n",
        "PROGRESS: | 8572         | 73.5        | 5m 11s       |\n",
        "PROGRESS: | 8592         | 73.5        | 5m 12s       |\n",
        "PROGRESS: | 8611         | 73.75       | 5m 13s       |\n",
        "PROGRESS: | 8634         | 74          | 5m 14s       |\n",
        "PROGRESS: | 8649         | 74          | 5m 15s       |\n",
        "PROGRESS: | 8675         | 74.25       | 5m 16s       |\n",
        "PROGRESS: | 8697         | 74.5        | 5m 17s       |\n",
        "PROGRESS: | 8722         | 74.75       | 5m 18s       |\n",
        "PROGRESS: | 8743         | 75          | 5m 19s       |\n",
        "PROGRESS: | 8761         | 75          | 5m 20s       |\n",
        "PROGRESS: | 8784         | 75.25       | 5m 21s       |\n",
        "PROGRESS: | 8808         | 75.5        | 5m 22s       |\n",
        "PROGRESS: | 8832         | 75.75       | 5m 23s       |\n",
        "PROGRESS: | 8858         | 75.75       | 5m 24s       |\n",
        "PROGRESS: | 8878         | 76          | 5m 25s       |\n",
        "PROGRESS: | 8903         | 76.25       | 5m 26s       |\n",
        "PROGRESS: | 8927         | 76.5        | 5m 27s       |\n",
        "PROGRESS: | 8955         | 76.75       | 5m 28s       |\n",
        "PROGRESS: | 8979         | 77          | 5m 29s       |\n",
        "PROGRESS: | 9008         | 77.25       | 5m 30s       |\n",
        "PROGRESS: | 9030         | 77.25       | 5m 31s       |\n",
        "PROGRESS: | 9052         | 77.5        | 5m 32s       |\n",
        "PROGRESS: | 9069         | 77.75       | 5m 33s       |\n",
        "PROGRESS: | 9089         | 77.75       | 5m 34s       |\n",
        "PROGRESS: | 9112         | 78          | 5m 35s       |\n",
        "PROGRESS: | 9135         | 78.25       | 5m 36s       |\n",
        "PROGRESS: | 9161         | 78.5        | 5m 37s       |\n",
        "PROGRESS: | 9185         | 78.75       | 5m 38s       |\n",
        "PROGRESS: | 9208         | 78.75       | 5m 39s       |\n",
        "PROGRESS: | 9227         | 79          | 5m 40s       |\n",
        "PROGRESS: | 9248         | 79.25       | 5m 41s       |\n",
        "PROGRESS: | 9269         | 79.5        | 5m 42s       |\n",
        "PROGRESS: | 9292         | 79.5        | 5m 43s       |\n",
        "PROGRESS: | 9315         | 79.75       | 5m 44s       |\n",
        "PROGRESS: | 9332         | 80          | 5m 45s       |\n",
        "PROGRESS: | 9356         | 80.25       | 5m 46s       |\n",
        "PROGRESS: | 9376         | 80.25       | 5m 47s       |\n",
        "PROGRESS: | 9394         | 80.5        | 5m 48s       |\n",
        "PROGRESS: | 9408         | 80.5        | 5m 49s       |\n",
        "PROGRESS: | 9426         | 80.75       | 5m 50s       |\n",
        "PROGRESS: | 9448         | 81          | 5m 51s       |\n",
        "PROGRESS: | 9469         | 81          | 5m 52s       |\n",
        "PROGRESS: | 9491         | 81.25       | 5m 53s       |\n",
        "PROGRESS: | 9510         | 81.5        | 5m 54s       |\n",
        "PROGRESS: | 9534         | 81.75       | 5m 55s       |\n",
        "PROGRESS: | 9562         | 82          | 5m 56s       |\n",
        "PROGRESS: | 9583         | 82          | 5m 57s       |\n",
        "PROGRESS: | 9605         | 82.25       | 5m 58s       |\n",
        "PROGRESS: | 9628         | 82.5        | 5m 59s       |\n",
        "PROGRESS: | 9643         | 82.5        | 6m 0s        |\n",
        "PROGRESS: | 9669         | 82.75       | 6m 1s        |\n",
        "PROGRESS: | 9691         | 83          | 6m 2s        |\n",
        "PROGRESS: | 9712         | 83.25       | 6m 3s        |\n",
        "PROGRESS: | 9737         | 83.5        | 6m 4s        |\n",
        "PROGRESS: | 9755         | 83.5        | 6m 5s        |\n",
        "PROGRESS: | 9771         | 83.75       | 6m 6s        |\n",
        "PROGRESS: | 9794         | 84          | 6m 7s        |\n",
        "PROGRESS: | 9814         | 84          | 6m 8s        |\n",
        "PROGRESS: | 9837         | 84.25       | 6m 9s        |\n",
        "PROGRESS: | 9858         | 84.5        | 6m 11s       |\n",
        "PROGRESS: | 9875         | 84.5        | 6m 11s       |\n",
        "PROGRESS: | 9897         | 84.75       | 6m 12s       |\n",
        "PROGRESS: | 9919         | 85          | 6m 13s       |\n",
        "PROGRESS: | 9945         | 85.25       | 6m 14s       |\n",
        "PROGRESS: | 9969         | 85.5        | 6m 15s       |\n",
        "PROGRESS: | 9994         | 85.5        | 6m 16s       |\n",
        "PROGRESS: | 10016        | 85.75       | 6m 17s       |\n",
        "PROGRESS: | 10040        | 86          | 6m 18s       |\n",
        "PROGRESS: | 10062        | 86.25       | 6m 19s       |\n",
        "PROGRESS: | 10089        | 86.5        | 6m 20s       |\n",
        "PROGRESS: | 10108        | 86.5        | 6m 21s       |\n",
        "PROGRESS: | 10132        | 86.75       | 6m 22s       |\n",
        "PROGRESS: | 10153        | 87          | 6m 23s       |\n",
        "PROGRESS: | 10176        | 87.25       | 6m 24s       |\n",
        "PROGRESS: | 10198        | 87.25       | 6m 25s       |\n",
        "PROGRESS: | 10224        | 87.5        | 6m 27s       |\n",
        "PROGRESS: | 10247        | 87.75       | 6m 27s       |\n",
        "PROGRESS: | 10269        | 88          | 6m 28s       |\n",
        "PROGRESS: | 10294        | 88.25       | 6m 29s       |\n",
        "PROGRESS: | 10318        | 88.5        | 6m 30s       |\n",
        "PROGRESS: | 10339        | 88.5        | 6m 32s       |\n",
        "PROGRESS: | 10360        | 88.75       | 6m 33s       |\n",
        "PROGRESS: | 10384        | 89          | 6m 34s       |\n",
        "PROGRESS: | 10403        | 89          | 6m 35s       |\n",
        "PROGRESS: | 10422        | 89.25       | 6m 36s       |\n",
        "PROGRESS: | 10439        | 89.5        | 6m 36s       |\n",
        "PROGRESS: | 10462        | 89.5        | 6m 37s       |\n",
        "PROGRESS: | 10482        | 89.75       | 6m 39s       |\n",
        "PROGRESS: | 10495        | 90          | 6m 40s       |\n",
        "PROGRESS: | 10514        | 90          | 6m 41s       |\n",
        "PROGRESS: | 10535        | 90.25       | 6m 41s       |\n",
        "PROGRESS: | 10554        | 90.5        | 6m 42s       |\n",
        "PROGRESS: | 10572        | 90.5        | 6m 43s       |\n",
        "PROGRESS: | 10594        | 90.75       | 6m 44s       |\n",
        "PROGRESS: | 10614        | 91          | 6m 46s       |\n",
        "PROGRESS: | 10640        | 91.25       | 6m 46s       |\n",
        "PROGRESS: | 10666        | 91.25       | 6m 47s       |\n",
        "PROGRESS: | 10685        | 91.5        | 6m 49s       |\n",
        "PROGRESS: | 10704        | 91.75       | 6m 50s       |\n",
        "PROGRESS: | 10728        | 92          | 6m 51s       |\n",
        "PROGRESS: | 10748        | 92          | 6m 52s       |\n",
        "PROGRESS: | 10770        | 92.25       | 6m 53s       |\n",
        "PROGRESS: | 10793        | 92.5        | 6m 54s       |\n",
        "PROGRESS: | 10813        | 92.75       | 6m 55s       |\n",
        "PROGRESS: | 10832        | 92.75       | 6m 56s       |\n",
        "PROGRESS: | 10852        | 93          | 6m 57s       |\n",
        "PROGRESS: | 10874        | 93.25       | 6m 58s       |\n",
        "PROGRESS: | 10897        | 93.25       | 6m 59s       |\n",
        "PROGRESS: | 10916        | 93.5        | 7m 0s        |\n",
        "PROGRESS: | 10934        | 93.75       | 7m 1s        |\n",
        "PROGRESS: | 10958        | 94          | 7m 2s        |\n",
        "PROGRESS: | 10975        | 94          | 7m 3s        |\n",
        "PROGRESS: | 10996        | 94.25       | 7m 4s        |\n",
        "PROGRESS: | 11014        | 94.25       | 7m 5s        |\n",
        "PROGRESS: | 11030        | 94.5        | 7m 6s        |\n",
        "PROGRESS: | 11051        | 94.75       | 7m 7s        |\n",
        "PROGRESS: | 11063        | 94.75       | 7m 8s        |\n",
        "PROGRESS: | 11080        | 95          | 7m 9s        |\n",
        "PROGRESS: | 11100        | 95          | 7m 10s       |\n",
        "PROGRESS: | 11116        | 95.25       | 7m 11s       |\n",
        "PROGRESS: | 11133        | 95.5        | 7m 12s       |\n",
        "PROGRESS: | 11151        | 95.5        | 7m 13s       |\n",
        "PROGRESS: | 11170        | 95.75       | 7m 14s       |\n",
        "PROGRESS: | 11189        | 95.75       | 7m 15s       |\n",
        "PROGRESS: | 11207        | 96          | 7m 16s       |\n",
        "PROGRESS: | 11228        | 96.25       | 7m 17s       |\n",
        "PROGRESS: | 11248        | 96.25       | 7m 18s       |\n",
        "PROGRESS: | 11264        | 96.5        | 7m 19s       |\n",
        "PROGRESS: | 11285        | 96.75       | 7m 20s       |\n",
        "PROGRESS: | 11309        | 97          | 7m 21s       |\n",
        "PROGRESS: | 11328        | 97          | 7m 22s       |\n",
        "PROGRESS: | 11346        | 97.25       | 7m 23s       |\n",
        "PROGRESS: | 11364        | 97.25       | 7m 24s       |\n",
        "PROGRESS: | 11379        | 97.5        | 7m 25s       |\n",
        "PROGRESS: | 11394        | 97.5        | 7m 26s       |\n",
        "PROGRESS: | 11410        | 97.75       | 7m 27s       |\n",
        "PROGRESS: | 11427        | 98          | 7m 28s       |\n",
        "PROGRESS: | 11438        | 98          | 7m 29s       |\n",
        "PROGRESS: | 11449        | 98          | 7m 30s       |\n",
        "PROGRESS: | 11463        | 98.25       | 7m 31s       |\n",
        "PROGRESS: | 11480        | 98.25       | 7m 32s       |\n",
        "PROGRESS: | 11493        | 98.5        | 7m 33s       |\n",
        "PROGRESS: | 11506        | 98.5        | 7m 34s       |\n",
        "PROGRESS: | 11515        | 98.75       | 7m 35s       |\n",
        "PROGRESS: | 11527        | 98.75       | 7m 36s       |\n",
        "PROGRESS: | 11539        | 98.75       | 7m 37s       |\n",
        "PROGRESS: | 11552        | 99          | 7m 38s       |\n",
        "PROGRESS: | 11564        | 99          | 7m 39s       |\n",
        "PROGRESS: | 11575        | 99.25       | 7m 40s       |\n",
        "PROGRESS: | 11585        | 99.25       | 7m 41s       |\n",
        "PROGRESS: | 11597        | 99.25       | 7m 42s       |\n",
        "PROGRESS: | 11615        | 99.5        | 7m 43s       |\n",
        "PROGRESS: | 11626        | 99.5        | 7m 44s       |\n",
        "PROGRESS: | 11636        | 99.75       | 7m 45s       |\n",
        "PROGRESS: | 11648        | 99.75       | 7m 46s       |\n",
        "PROGRESS: | Done         |             | 7m 47s       |\n",
        "PROGRESS: +--------------+-------------+--------------+\n"
       ]
      },
      {
       "data": {
        "text/html": [
         "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
         "    <tr>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">query_label</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">reference_label</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">distance</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">rank</th>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3326</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.106417322776</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6453</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6454</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6455</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">6456</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "</table>\n",
         "[10 rows x 4 columns]<br/>\n",
         "</div>"
        ],
        "text/plain": [
         "Columns:\n",
         "\tquery_label\tint\n",
         "\treference_label\tint\n",
         "\tdistance\tfloat\n",
         "\trank\tint\n",
         "\n",
         "Rows: 10\n",
         "\n",
         "Data:\n",
         "+-------------+-----------------+----------------+------+\n",
         "| query_label | reference_label |    distance    | rank |\n",
         "+-------------+-----------------+----------------+------+\n",
         "|      0      |        0        |      0.0       |  1   |\n",
         "|      0      |       3326      | 0.106417322776 |  2   |\n",
         "|      1      |        1        |      0.0       |  1   |\n",
         "|      1      |       6453      |      0.0       |  2   |\n",
         "|      2      |        2        |      0.0       |  1   |\n",
         "|      2      |       6454      |      0.0       |  2   |\n",
         "|      3      |       6455      |      0.0       |  1   |\n",
         "|      3      |        3        |      0.0       |  2   |\n",
         "|      4      |       6456      |      0.0       |  1   |\n",
         "|      4      |        4        |      0.0       |  2   |\n",
         "+-------------+-----------------+----------------+------+\n",
         "[10 rows x 4 columns]"
        ]
       },
       "execution_count": 35,
       "metadata": {},
       "output_type": "execute_result"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Of course the nearest neighbors to each paragraph is the paragraph itself. Therefore, let us filter out paragraph that are with a distance of zero from each other. Additionally, let's look only on two near paragraphs that have small distance from each other (distance < 0.08)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#filter out paragraphs that are exactly exactly the same\n",
      "r = r[r['distance'] != 0]\n",
      "\n",
      "#filter out paragraphs that are with distance >= 0.1\n",
      "r = r[r['distance'] < 0.08]\n",
      "r"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "text/html": [
         "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
         "    <tr>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">query_label</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">reference_label</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">distance</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">rank</th>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4358</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4402</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0756935211504</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4402</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4358</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0756935211504</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4664</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8648</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0771454554256</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5202</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">3456</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0750878786393</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5569</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5684</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0762642005346</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5684</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5569</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0762642005346</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9146</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9562</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0377801868587</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9562</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9146</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0377801868587</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">9652</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">8917</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0687209701245</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2</td>\n",
         "    </tr>\n",
         "</table>\n",
         "[? rows x 4 columns]<br/>Note: Only the head of the SFrame is printed. This SFrame is lazily evaluated.<br/>You can use len(sf) to force materialization.\n",
         "</div>"
        ],
        "text/plain": [
         "Columns:\n",
         "\tquery_label\tint\n",
         "\treference_label\tint\n",
         "\tdistance\tfloat\n",
         "\trank\tint\n",
         "\n",
         "Rows: Unknown\n",
         "\n",
         "Data:\n",
         "+-------------+-----------------+-----------------+------+\n",
         "| query_label | reference_label |     distance    | rank |\n",
         "+-------------+-----------------+-----------------+------+\n",
         "|     4358    |       4402      | 0.0756935211504 |  2   |\n",
         "|     4402    |       4358      | 0.0756935211504 |  2   |\n",
         "|     4664    |       8648      | 0.0771454554256 |  2   |\n",
         "|     5202    |       3456      | 0.0750878786393 |  2   |\n",
         "|     5569    |       5684      | 0.0762642005346 |  2   |\n",
         "|     5684    |       5569      | 0.0762642005346 |  2   |\n",
         "|     9146    |       9562      | 0.0377801868587 |  2   |\n",
         "|     9562    |       9146      | 0.0377801868587 |  2   |\n",
         "|     9652    |       8917      | 0.0687209701245 |  2   |\n",
         "+-------------+-----------------+-----------------+------+\n",
         "[? rows x 4 columns]\n",
         "Note: Only the head of the SFrame is printed. This SFrame is lazily evaluated.\n",
         "You can use len(sf) to force materialization."
        ]
       },
       "execution_count": 36,
       "metadata": {},
       "output_type": "execute_result"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, let's use join to match between each query_label and reference_label values and their actual paragraphs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf_paragraphs = sf_paragraphs.add_row_number('query_label')\n",
      "sf_paragraphs = sf_paragraphs.add_row_number('reference_label')\n",
      "sf_similar = r.join(sf_paragraphs, on=\"query_label\").join(sf_paragraphs, on=\"reference_label\")"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sf_similar[['paragraph','title', 'title.1', 'paragraph.1', 'distance']]"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "data": {
        "text/html": [
         "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
         "    <tr>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">paragraph</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">title</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">title.1</th>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">paragraph.1</th>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The German was sent for<br>but professed to know ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Hound of the<br>Baskervilles ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">His Last Bow</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">It was a blazing hot day<br>in August. Baker Street ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">At first this vague and<br>terrible power was ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A Study In Scarlet</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A Study In Scarlet</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Had the wanderer remained<br>awake for another half ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Had the wanderer remained<br>awake for another half ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A Study In Scarlet</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">A Study In Scarlet</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">At first this vague and<br>terrible power was ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"It only remains to<br>indicate the part which ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Hound of the<br>Baskervilles ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Hound of the<br>Baskervilles ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Sir Henry was more<br>pleased than surprise ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Sir Henry was more<br>pleased than surprise ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Hound of the<br>Baskervilles ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Hound of the<br>Baskervilles ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">\"It only remains to<br>indicate the part which ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">At the mention of this<br>gigantic sum we all ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Sign of the Four</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Empty House</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">All day I turned these<br>facts over in my mind, ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Pictures for \"The<br>Adventure of the Missing ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Missing Three-Quarter</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Dancing Men</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Pictures for \"The<br>Adventure of the Dancing ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Pictures for \"The<br>Adventure of the Golden ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Golden Pince-Nez</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Priory School</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Pictures for \"The<br>Adventure of the Priory ...</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Pictures for \"The<br>Adventure of the Priory ...</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Priory School</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">The Golden Pince-Nez</td>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Pictures for \"The<br>Adventure of the Golden ...</td>\n",
         "    </tr>\n",
         "</table>\n",
         "<table frame=\"box\" rules=\"cols\">\n",
         "    <tr>\n",
         "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">distance</th>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0750878786393</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0756935211504</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0756935211504</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0762642005346</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0762642005346</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0771454554256</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0687209701245</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0377801868587</td>\n",
         "    </tr>\n",
         "    <tr>\n",
         "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.0377801868587</td>\n",
         "    </tr>\n",
         "</table>\n",
         "[9 rows x 5 columns]<br/>\n",
         "</div>"
        ],
        "text/plain": [
         "Columns:\n",
         "\tparagraph\tstr\n",
         "\ttitle\tstr\n",
         "\ttitle.1\tstr\n",
         "\tparagraph.1\tstr\n",
         "\tdistance\tfloat\n",
         "\n",
         "Rows: 9\n",
         "\n",
         "Data:\n",
         "+-------------------------------+-------------------------------+\n",
         "|           paragraph           |             title             |\n",
         "+-------------------------------+-------------------------------+\n",
         "| The German was sent for bu... | The Hound of the Baskervilles |\n",
         "| At first this vague and te... |       A Study In Scarlet      |\n",
         "| Had the wanderer remained ... |       A Study In Scarlet      |\n",
         "| \"It only remains to indica... | The Hound of the Baskervilles |\n",
         "| Sir Henry was more pleased... | The Hound of the Baskervilles |\n",
         "| At the mention of this gig... |      The Sign of the Four     |\n",
         "| Pictures for \"The Adventur... |   The Missing Three-Quarter   |\n",
         "| Pictures for \"The Adventur... |      The Golden Pince-Nez     |\n",
         "| Pictures for \"The Adventur... |       The Priory School       |\n",
         "+-------------------------------+-------------------------------+\n",
         "+-------------------------------+-------------------------------+\n",
         "|            title.1            |          paragraph.1          |\n",
         "+-------------------------------+-------------------------------+\n",
         "|          His Last Bow         | It was a blazing hot day i... |\n",
         "|       A Study In Scarlet      | Had the wanderer remained ... |\n",
         "|       A Study In Scarlet      | At first this vague and te... |\n",
         "| The Hound of the Baskervilles | Sir Henry was more pleased... |\n",
         "| The Hound of the Baskervilles | \"It only remains to indica... |\n",
         "|        The Empty House        | All day I turned these fac... |\n",
         "|        The Dancing Men        | Pictures for \"The Adventur... |\n",
         "|       The Priory School       | Pictures for \"The Adventur... |\n",
         "|      The Golden Pince-Nez     | Pictures for \"The Adventur... |\n",
         "+-------------------------------+-------------------------------+\n",
         "+-----------------+\n",
         "|     distance    |\n",
         "+-----------------+\n",
         "| 0.0750878786393 |\n",
         "| 0.0756935211504 |\n",
         "| 0.0756935211504 |\n",
         "| 0.0762642005346 |\n",
         "| 0.0762642005346 |\n",
         "| 0.0771454554256 |\n",
         "| 0.0687209701245 |\n",
         "| 0.0377801868587 |\n",
         "| 0.0377801868587 |\n",
         "+-----------------+\n",
         "[9 rows x 5 columns]"
        ]
       },
       "execution_count": 38,
       "metadata": {},
       "output_type": "execute_result"
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's look at some of the similar paragraphs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print sf_similar[1]['paragraph']\n",
      "print \"-\"*100\n",
      "print sf_similar[1]['paragraph.1']"
     ],
     "language": "python",
     "metadata": {
      "collapsed": false
     },
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "At first this vague and terrible power was exercised only upon the\n",
        "     recalcitrants who, having embraced the Mormon faith, wished\n",
        "     afterwards to pervert or to abandon it. Soon, however, it took a\n",
        "     wider range. The supply of adult women was running short, and\n",
        "     polygamy without a female population on which to draw was a barren\n",
        "     doctrine indeed. Strange rumours began to be bandied about--rumours\n",
        "     of murdered immigrants and rifled camps in regions where Indians had\n",
        "     never been seen. Fresh women appeared in the harems of the\n",
        "     Elders--women who pined and wept, and bore upon their faces the\n",
        "     traces of an unextinguishable horror. Belated wanderers upon the\n",
        "     mountains spoke of gangs of armed men, masked, stealthy, and\n",
        "     noiseless, who flitted by them in the darkness. These tales and\n",
        "     rumours took substance and shape, and were corroborated and\n",
        "     re-corroborated, until they resolved themselves into a definite name.\n",
        "     To this day, in the lonely ranches of the West, the name of the\n",
        "     Danite Band, or the Avenging Angels, is a sinister and an ill-omened\n",
        "     one.\n",
        "----------------------------------------------------------------------------------------------------\n",
        "Had the wanderer remained awake for another half hour a strange sight\n",
        "     would have met his eyes. Far away on the extreme verge of the alkali\n",
        "     plain there rose up a little spray of dust, very slight at first, and\n",
        "     hardly to be distinguished from the mists of the distance, but\n",
        "     gradually growing higher and broader until it formed a solid,\n",
        "     well-defined cloud. This cloud continued to increase in size until it\n",
        "     became evident that it could only be raised by a great multitude of\n",
        "     moving creatures. In more fertile spots the observer would have come\n",
        "     to the conclusion that one of those great herds of bisons which graze\n",
        "     upon the prairie land was approaching him. This was obviously\n",
        "     impossible in these arid wilds. As the whirl of dust drew nearer to\n",
        "     the solitary bluff upon which the two castaways were reposing, the\n",
        "     canvas-covered tilts of waggons and the figures of armed horsemen\n",
        "     began to show up through the haze, and the apparition revealed itself\n",
        "     as being a great caravan upon its journey for the West. But what a\n",
        "     caravan! When the head of it had reached the base of the mountains,\n",
        "     the rear was not yet visible on the horizon. Right across the\n",
        "     enormous plain stretched the straggling array, waggons and carts, men\n",
        "     on horseback, and men on foot. Innumerable women who staggered along\n",
        "     under burdens, and children who toddled beside the waggons or peeped\n",
        "     out from under the white coverings. This was evidently no ordinary\n",
        "     party of immigrants, but rather some nomad people who had been\n",
        "     compelled from stress of circumstances to seek themselves a new\n",
        "     country. There rose through the clear air a confused clattering and\n",
        "     rumbling from this great mass of humanity, with the creaking of\n",
        "     wheels and the neighing of horses. Loud as it was, it was not\n",
        "     sufficient to rouse the two tired wayfarers above them.\n"
       ]
      }
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Although that in the paragraphs match the text is completely different, it still has the several similar motifs. In the first paragraph dog is leading his master. \n",
      "While in the second paragraph the boots are replacing the dog part. In both paragraphs the author use somewhat similar motifs \"dreadful sight to see that huge black creature\" \n",
      "and \"saw something that made me feel sickish.\" I personally find these results quite interesting."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <a id=\"go\"></a> 6. Where to Go From Here"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<pre>\n",
      "\"Thank you,\" said Holmes, \"I only wished to ask you how you would go from here to the Strand.\"\n",
      "                                                                              -The Red-Headed League\n",
      "</pre>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook, we presented a short and practical tutorial for NLP, which covered several common NLP topics, such as NER, Topic Model, and Word2Vec. If you want to continue to explore this dataset yourself, there are a lot more that can be done. You can rerun this code using different texts (Harry Potter, Lord of the Rings, and etc.). In addition, you can try to modify the above code to create social networks between persons and locations, or to use [GloVe]( http://nlp.stanford.edu/projects/glove/) instead of Word2Vec. Furthermore, you can also try to run other graph theory algorithms, such as community detection algorithms, on the constructed social networks to uncover additional interesting insights. We hope that the methods and code presented in this notebook can assist you to solve other text analysis tasks."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## <a id=\"reading\"></a> 7. Further Reading"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Further reading material:\n",
      "- [Analysis of communities in a mythological social network, Miranda et al.](http://arxiv.org/abs/1306.2537)\n",
      "- [A survey of named entity recognition and classification, David Nadeau and Satoshi Sekine](http://nlp.cs.nyu.edu/sekine/papers/li07.pdf)\n",
      "- [Probabilistic topic models, David M. Blei](https://www.cs.princeton.edu/~blei/papers/Blei2012.pdf)\n",
      "- [LDAvis: A method for visualizing and interpreting topic models](http://stat-graphics.org/movies/ldavis.html)\n",
      "- [Practical deep text learning blog post](https://dato.com/learn/gallery/notebooks/deep_text_learning.html)\n",
      "- [Deep learning with word2vec and gensim](http://rare-technologies.com/deep-learning-with-word2vec-and-gensim/)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}